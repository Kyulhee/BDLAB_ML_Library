{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model, Sequential \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide raw data into train / test & x_val / y_val\n",
    "def data_split(raw_data, index_col, test_index):\n",
    "    \n",
    "    train_data = raw_data.iloc[list(raw_data.iloc[:,index_col]!=7)]\n",
    "    train_data = raw_data.iloc[list(train_data.iloc[:,index_col]!=test_index)]\n",
    "    test_data = raw_data.iloc[list(raw_data.iloc[:,index_col]==test_index)]\n",
    "    test_data = test_data.append(raw_data.iloc[list(raw_data.iloc[:,index_col]==7)])\n",
    "    \n",
    "    y_val = train_data.Platinum_Status\n",
    "    x_val = train_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val = test_data.Platinum_Status\n",
    "    test_x_val = test_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    return train_data, test_data, y_val, x_val, test_y_val, test_x_val\n",
    "\n",
    "    # raw_data: have gene_expressions(maybe multiple columns), index column, Platinum_Status column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all of model performance \n",
    "# - predictions(probability) / labeled predictions(0/1) / Loss / Accuracy / Sensitivity / Specificity / AUC values of Train / Test dataset.\n",
    "# using trained models, or you can put predictions(probability) passively(in this case, Loss & Accuracy do not provided.)\n",
    "def model_performance(information=False, Input_Prediction_Passively=False, using_model=None, tr_predictions=None, ts_predictions=None, tr_x_val=None, tr_y_val=None, ts_x_val=None, ts_y_val=None, output_list=None):\n",
    "    \n",
    "    if information == True:            \n",
    "        print(\"options model_performance:\\n1) using_model: keras models that you want to check performance. \\\"Input_Prediction_Passive\\\" option for input prediction list instead using models.\\n3) tr_predictions & ts_predictions: prediction input passively. put this data only when not using keras model.\\n4) tr_x_val & ts_x_val: input samples of train/test samples.\\n4) tr_y_val & ts_y_val: results of train/test samples.\\n5) output_list: return values that you want to recieve.\\n CAUTION: Essential variable.\\n\\t tr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\\n\\n* CAUTION: if 'None' value is returned, please check your input tr inputs(None value for tr outputs) or ts inputs(None value for ts outputs).\") \n",
    "        return 0\n",
    "    elif information != False:\n",
    "        print(\"for using information options, please set 'information' variable for 'True'\")\n",
    "        return -1\n",
    "    \n",
    "    if using_model is None:\n",
    "        if Input_Prediction_Passively == False:\n",
    "            print(\"ERROR: There are no models for using.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\") \n",
    "            return -1\n",
    "        elif (tr_predictions is None) and (ts_predictions is None): # No model/prediction input. no performance should be calculated.\n",
    "                print(\"ERROR: Input prediction list instead using saved model.\")\n",
    "                return -1\n",
    "        else: # No model input, but Input_Prediction_Passively is True & input prediction is valid.\n",
    "            tr_loss,tr_accuracy= None, None\n",
    "            ts_loss,ts_accuracy= None, None\n",
    "            \n",
    "    elif Input_Prediction_Passively == True: # both of model/prediction putted, could cause confusing.\n",
    "        ch = input(\"You put both model and prediction. Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "        while 1:\n",
    "            if ch == 'p':\n",
    "                using_model = None\n",
    "                break\n",
    "            elif ch == 'm':\n",
    "                tr_predictions = None\n",
    "                ts_predictions = None\n",
    "                break\n",
    "            elif ch == 'e':\n",
    "                return 0\n",
    "            else:\n",
    "                print(\"you put worng option: \"+str(ch))\n",
    "            ch = input(\"Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "                \n",
    "    if output_list is None:\n",
    "        print(\"ERROR: There are no output_list for return.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\")\n",
    "        return -1\n",
    "    \n",
    "    if not(tr_x_val is None) and not(tr_y_val is None):\n",
    "        # predict tr result only when no tr_prediction input\n",
    "        if tr_predictions is None:\n",
    "            tr_loss,tr_accuracy= using_model.evaluate(tr_x_val,tr_y_val)\n",
    "            tr_predictions = using_model.predict(tr_x_val)\n",
    "        # tr sensitivity / specificity\n",
    "        labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "        tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, tr_y_val)\n",
    "        tr_predictions_flat = tr_predictions[:,0]   \n",
    "        # roc(tr)\n",
    "        fpr_tr, tpr_tr, threshold_tr = metrics.roc_curve(tr_y_val, tr_predictions)\n",
    "        roc_auc_tr = metrics.auc(fpr_tr, tpr_tr)\n",
    "    \n",
    "    if not(ts_x_val is None) and not(ts_y_val is None):\n",
    "        # predict ts result only when no ts_prediction input\n",
    "        if ts_predictions is None:\n",
    "            ts_loss,ts_accuracy= using_model.evaluate(ts_x_val,ts_y_val)\n",
    "            ts_predictions = using_model.predict(ts_x_val)\n",
    "        labeled_ts_predictions = np.where(ts_predictions > 0.5, 1, 0).flatten()\n",
    "        ts_sensitivity, ts_specificity = check_correct(labeled_ts_predictions, ts_y_val)\n",
    "        ts_predictions_flat = ts_predictions[:,0]   \n",
    "        # roc(ts)\n",
    "        fpr_ts, tpr_ts, threshold_ts = metrics.roc_curve(ts_y_val, ts_predictions)\n",
    "        roc_auc_ts = metrics.auc(fpr_ts, tpr_ts)    \n",
    "    \n",
    "    if (not(tr_x_val is None) and not(tr_y_val is None)) and (not(ts_x_val is None) and not(ts_y_val is None)):\n",
    "        y_true = np.append(tr_y_val, ts_y_val)\n",
    "        y_pred = np.append(tr_predictions, ts_predictions)\n",
    "        fpr_total, tpr_total, threshold_total = metrics.roc_curve(y_true, y_pred)\n",
    "        roc_auc_total = metrics.auc(fpr_total, tpr_total)\n",
    "        \n",
    "    return_list = []\n",
    "    \n",
    "    for output in output_list:\n",
    "        \n",
    "        if(output == \"tr_loss\"):\n",
    "            return_list.append(tr_loss)\n",
    "                               \n",
    "        elif(output == \"tr_accuracy\"):\n",
    "            return_list.append(tr_accuracy)\n",
    "                               \n",
    "        elif(output == \"tr_sensitivity\"):\n",
    "            return_list.append(tr_sensitivity)\n",
    "                               \n",
    "        elif(output == \"tr_specificity\"):\n",
    "            return_list.append(tr_specificity)\n",
    "                               \n",
    "        elif(output == \"tr_predictions\"):\n",
    "            return_list.append(tr_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_tr_predictions\"):\n",
    "            return_list.append(labeled_tr_predictions)\n",
    "                               \n",
    "        elif(output == \"tr_predictions_flat\"):\n",
    "            return_list.append(tr_predictions_flat)\n",
    "            \n",
    "        elif(output == \"roc_auc_tr\"):\n",
    "            return_list.append(roc_auc_tr)\n",
    "\n",
    "        elif(output == \"ts_loss\"):\n",
    "            return_list.append(ts_loss)\n",
    "                               \n",
    "        elif(output == \"ts_accuracy\"):\n",
    "            return_list.append(ts_accuracy)\n",
    "                               \n",
    "        elif(output == \"ts_sensitivity\"):\n",
    "            return_list.append(ts_sensitivity)\n",
    "                               \n",
    "        elif(output == \"ts_specificity\"):\n",
    "            return_list.append(ts_specificity)\n",
    "                               \n",
    "        elif(output == \"ts_predictions\"):\n",
    "            return_list.append(ts_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_ts_predictions\"):\n",
    "            return_list.append(labeled_ts_predictions)\n",
    "                               \n",
    "        elif(output == \"ts_predictions_flat\"):\n",
    "            return_list.append(ts_predictions_flat)\n",
    "        \n",
    "        elif(output == \"roc_auc_ts\"):\n",
    "            return_list.append(roc_auc_ts)\n",
    "            \n",
    "        elif(output == \"roc_auc_total\"):\n",
    "            return_list.append(roc_auc_total)\n",
    "                               \n",
    "        else:\n",
    "            print(\"There are no options <\"+str(output)+\">. Please refer these output options:\\ntr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\")\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage algorithm\n",
    "def ensemble_coverage(inputModels,x,y):\n",
    "    \n",
    "    outputModels = []\n",
    "    modelInfo = []\n",
    "    coverageTotal= [False]*len(y)\n",
    "    \n",
    "    for i in range(len(inputModels)):\n",
    "        m = inputModels[i]\n",
    "        yHat = m.predict(x[i])\n",
    "        yHat = [round(i) for [i] in yHat]\n",
    "        \n",
    "        loss, acc = m.evaluate(x[i],y)\n",
    "        modelInfo.append((m,yHat,acc))\n",
    "        #print(yHat[0:10])\n",
    "        #print(acc)\n",
    "    \n",
    "    modelInfo.sort(key=lambda x : x[2],reverse=True)\n",
    "    #print(modelInfo)\n",
    "    \n",
    "    for m,yHat,acc in modelInfo:\n",
    "        beforeCoverage = sum(coverageTotal)\n",
    "        coverage = [a == b for a,b in zip(y,yHat)]\n",
    "        coverageTotal = [a or b for a,b in zip(coverageTotal,coverage)]\n",
    "        afterCoverage = sum(coverageTotal)\n",
    "        \n",
    "        print(afterCoverage/len(y))\n",
    "        \n",
    "        if afterCoverage > beforeCoverage:\n",
    "            outputModels.append(m)\n",
    "            print(\"Increased Coverage : model added!\")\n",
    "        else:\n",
    "            print(\"Same Coverage : model not added\")\n",
    "        if afterCoverage == len(y):\n",
    "            print(\"Fully Covered!\")\n",
    "            break\n",
    "                \n",
    "    return outputModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation: import & preprocessing data + import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable for each cycle. 1 cycle = five index\n",
    "cycle_index = 0\n",
    "\n",
    "em_output_list = {\"em_type\":[],\"test_index\":[],\"ensemble_comb\":[], \"ensemble_names\":[], \"tr_accuracy\":[], \"tr_sensitivity\":[], \"tr_specificity\":[], \"roc_auc_tr\":[], \n",
    "                 \"ts_accuracy\":[], \"ts_sensitivity\":[], \"ts_specificity\":[], \"roc_auc_ts\":[], \n",
    "                 \"roc_auc_total\":[]}\n",
    "\n",
    "em_type = [\"em\", \"mean\", \"t-em\", \"mo-em\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input path & name of models / raw data for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test index(1~5): 3\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: '../best_models/test_3/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-83925d28d240>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mmodel_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmodel_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: '../best_models/test_3/'"
     ]
    }
   ],
   "source": [
    "# change model_path & each model_name.\n",
    "\n",
    "types = [\"OV_six_fold_Annotation3000_400\", \n",
    "         \"OV_six_fold_CV_400\", \n",
    "         \"OV_six_fold_Var_400\", \n",
    "         \"OV_six_fold_new_Diff_400\",\n",
    "         \"OV_six_fold_Clin\", \n",
    "         \"OV_six_fold_SNV\" \n",
    "         ]\n",
    "\n",
    "ch = input(\"test index(1~5): \")\n",
    "ts_i = int(ch)\n",
    "\n",
    "# input model path & ensemble data(Transcriptome, Cinical Information, Somatic Mutation data)\n",
    "# data path(server): /home/tjahn/TCGA_Ovary/01.Data/DNN/TC_intersect_subsamples_by_names \n",
    "input_model_path = \"../best_models/test_\"+str(ts_i)+\"/\"\n",
    "path = \"../TC_six_fold_subsamples/\"\n",
    "save_model_path = \"../models/\"\n",
    "save_prediction_path = \"../predictions/\"\n",
    "save_result_path = \"../result/\"\n",
    "\n",
    "model_names = []\n",
    "model_index = []\n",
    "files = os.listdir(input_model_path)\n",
    "\n",
    "for f in files:\n",
    "    ext= os.path.splitext(f)[-1]\n",
    "    if ext == \".h5\":\n",
    "        model_names.append(os.path.splitext(f)[0])\n",
    "        ind = int(f.split(\"_\")[1].split(\"-\")[0])\n",
    "        model_index.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_0_0\n",
      "OV_six_fold_Annotation3000_400\n",
      "\n",
      "m_1_0\n",
      "OV_six_fold_CV_400\n",
      "\n",
      "m_3_0\n",
      "OV_survival_wo_new_Diff_400\n",
      "\n",
      "m_2_0\n",
      "OV_six_fold_Var_400\n",
      "\n",
      "m_2_1\n",
      "OV_six_fold_Var_400\n",
      "\n",
      "m_3_2\n",
      "OV_survival_wo_new_Diff_400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for model_i in model_index:\n",
    "    print(model_names[t])\n",
    "    print(types[model_i]+\"\\n\")\n",
    "    t = t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### test index is [1] ###############\n",
      "\n",
      "\n",
      "model index: 0\n",
      "[0]: m_0_0 for type: OV_six_fold_Annotation3000_400.\n",
      " full tr & ts: (186, 400), (31, 400)\n",
      "\n",
      "model index: 1\n",
      "[1]: m_1_0 for type: OV_six_fold_CV_400.\n",
      " full tr & ts: (186, 400), (31, 400)\n",
      "\n",
      "model index: 3\n",
      "[2]: m_3_0 for type: OV_survival_wo_new_Diff_400.\n",
      " full tr & ts: (186, 400), (139, 400)\n",
      "\n",
      "model index: 2\n",
      "[3]: m_2_0 for type: OV_six_fold_Var_400.\n",
      " full tr & ts: (186, 400), (31, 400)\n",
      "\n",
      "model index: 2\n",
      "[4]: m_2_1 for type: OV_six_fold_Var_400.\n",
      " full tr & ts: (186, 400), (31, 400)\n",
      "\n",
      "model index: 3\n",
      "[5]: m_3_2 for type: OV_survival_wo_new_Diff_400.\n",
      " full tr & ts: (186, 400), (139, 400)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_1 = path+types[0]+\".csv\"\n",
    "file_2 = path+types[1]+\".csv\"\n",
    "file_3 = path+types[2]+\".csv\"\n",
    "file_4 = path+types[3]+\".csv\"\n",
    "file_5 = path+types[4]+\".csv\"\n",
    "file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "full_data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "full_data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "full_data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "full_data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "full_data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "full_data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "inter_data_1 = full_data_1.iloc[list(full_data_1.iloc[:,-1]!=6)]\n",
    "inter_data_2 = full_data_2.iloc[list(full_data_2.iloc[:,-1]!=6)]\n",
    "inter_data_3 = full_data_3.iloc[list(full_data_3.iloc[:,-1]!=6)]\n",
    "inter_data_4 = full_data_4.iloc[list(full_data_4.iloc[:,-1]!=6)]\n",
    "inter_data_5 = full_data_5.iloc[list(full_data_5.iloc[:,-1]!=6)]\n",
    "inter_data_6 = full_data_6.iloc[list(full_data_6.iloc[:,-1]!=6)]\n",
    "\n",
    "full_ds_list = [full_data_1, full_data_2, full_data_3, full_data_4, full_data_5, full_data_6]\n",
    "inter_ds_list = [inter_data_1, inter_data_2, inter_data_3, inter_data_4, inter_data_5, inter_data_6]\n",
    "\n",
    "# Split Train Test Data & Make full & inter dataset\n",
    "\n",
    "full_dataset = {\"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "inter_dataset = {\"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "\n",
    "\n",
    "print(\"############### test index is [\"+str(ts_i)+\"] ###############\\n\\n\")\n",
    "for m in range(len(model_index)):\n",
    "    print(\"model index: \"+str(model_index[m]))\n",
    "    full_tr_data, full_ts_data, full_tr_y_val, full_tr_x_val, full_ts_y_val, full_ts_x_val = data_split(raw_data = full_ds_list[model_index[m]], index_col = -1, test_index = ts_i)\n",
    "    print(\"[\"+str(m)+\"]: \"+model_names[m]+\" for type: \"+types[model_index[m]]+\".\\n full tr & ts: \"+str(full_tr_x_val.shape)+\", \"+str(full_ts_x_val.shape)+\"\\n\")\n",
    "    full_dataset['tr_data'].append(full_tr_data)\n",
    "    full_dataset['ts_data'].append(full_ts_data)\n",
    "    full_dataset['tr_x_val'].append(full_tr_x_val)\n",
    "    full_dataset['tr_y_val'].append(full_tr_y_val)\n",
    "    full_dataset['ts_x_val'].append(full_ts_x_val)\n",
    "    full_dataset['ts_y_val'].append(full_ts_y_val)  \n",
    "    inter_tr_data, inter_ts_data, inter_tr_y_val, inter_tr_x_val, inter_ts_y_val, inter_ts_x_val = data_split(raw_data = inter_ds_list[model_index[m]], index_col = -1, test_index = ts_i)\n",
    "    inter_dataset['tr_data'].append(inter_tr_data)\n",
    "    inter_dataset['ts_data'].append(inter_ts_data)\n",
    "    inter_dataset['tr_x_val'].append(inter_tr_x_val)\n",
    "    inter_dataset['tr_y_val'].append(inter_tr_y_val)\n",
    "    inter_dataset['ts_x_val'].append(inter_ts_x_val)\n",
    "    inter_dataset['ts_y_val'].append(inter_ts_y_val)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_newDiff_dataset = {\"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "inter_tr_data, inter_ts_data, inter_tr_y_val, inter_tr_x_val, inter_ts_y_val, inter_ts_x_val = data_split(raw_data = inter_ds_list[3], index_col = -1, test_index = ts_i)\n",
    "inter_newDiff_dataset['tr_data']= inter_tr_data\n",
    "inter_newDiff_dataset['ts_data']= inter_ts_data\n",
    "inter_newDiff_dataset['tr_x_val']= inter_tr_x_val\n",
    "inter_newDiff_dataset['tr_y_val']= inter_tr_y_val\n",
    "inter_newDiff_dataset['ts_x_val']= inter_ts_x_val\n",
    "inter_newDiff_dataset['ts_y_val']= inter_ts_y_val    \n",
    "#inter_new_Diff_dataset = inter_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import separate models & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 2ms/step\n",
      "31/31 [==============================] - 0s 129us/step\n",
      "\n",
      "model: m_0_0\n",
      "tr & ts for inter data: 0.9262295023339694, 0.774193525314331\n",
      "\n",
      "122/122 [==============================] - 0s 956us/step\n",
      "31/31 [==============================] - 0s 64us/step\n",
      "\n",
      "model: m_1_0\n",
      "tr & ts for inter data: 1.0, 0.6451612710952759\n",
      "\n",
      "122/122 [==============================] - 0s 703us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5ab337e824ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m                      \u001b[1;34m\"ts_accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ts_sensitivity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ts_specificity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ts_predictions\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                      \u001b[1;34m\"labeled_ts_predictions\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ts_predictions_flat\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"roc_auc_ts\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                      \"roc_auc_total\"])\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mm_tr_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_tr_sensitivity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_tr_specificity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_tr_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_labeled_tr_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_tr_predictions_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_roc_auc_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_ts_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_ts_sensitivity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_ts_specificity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_ts_predictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_labeled_ts_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_ts_predictions_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_roc_auc_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_roc_auc_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nmodel: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2ac9892d6f44>\u001b[0m in \u001b[0;36mmodel_performance\u001b[1;34m(information, Input_Prediction_Passively, using_model, tr_predictions, ts_predictions, tr_x_val, tr_y_val, ts_x_val, ts_y_val, output_list)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mtr_predictions_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# roc(tr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mfpr_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_y_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mroc_auc_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \"\"\"\n\u001b[0;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 534\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    316\u001b[0m     if not (y_type == \"binary\" or\n\u001b[0;32m    317\u001b[0m             (y_type == \"multiclass\" and pos_label is not None)):\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} format is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "# model load & evaluation. <model_n_l> is full-layer model, <model_n_l_new> is without-sigmoid-layer model.\n",
    "'''\n",
    "Each model's tr_accuracy can be differ to original model, but ts_accuracy should be same to original tested models.\n",
    "Because we using full-size data(about 200 patients data used Transcriptome, Clinical, SNV models.) for train each models.\n",
    "In contrast, in this code, we using ensemble-input data(intersected 153 patients).\n",
    "For-training-patients may be different in ensemble data and whole size data, but for-test-patients are the same.\n",
    "'''\n",
    "\n",
    "model_list = []\n",
    "model_output_list = {\"tr_accuracy\":[], \"tr_sensitivity\":[], \"tr_specificity\":[], \"tr_predictions\":[],\n",
    "                 \"labeled_tr_predictions\":[], \"tr_predictions_flat\":[], \"roc_auc_tr\":[], \n",
    "                 \"ts_accuracy\":[], \"ts_sensitivity\":[], \"ts_specificity\":[], \"ts_predictions\":[],\n",
    "                 \"labeled_ts_predictions\":[], \"ts_predictions_flat\":[], \"roc_auc_ts\":[], \n",
    "                 \"roc_auc_total\":[], \"tr_result\":[], \"ts_result\":[]}\n",
    "tr_predictions = []\n",
    "ts_predictions = []\n",
    "\n",
    "for m in range(len(model_names)):\n",
    "    \n",
    "    model_l = load_model(input_model_path+model_names[m]+\".h5\")\n",
    "    model_list.append(model_l)\n",
    "    output_list = output_list = model_performance(\n",
    "        information = False, using_model=model_l,Input_Prediction_Passively = False, \n",
    "        tr_x_val=inter_dataset['tr_x_val'][m], tr_y_val=inter_dataset['tr_y_val'][m], ts_x_val=inter_dataset['ts_x_val'][m], ts_y_val=inter_dataset['ts_y_val'][m],\n",
    "        output_list=[\"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                     \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                     \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                     \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                     \"roc_auc_total\"])\n",
    "    m_tr_accuracy, m_tr_sensitivity, m_tr_specificity, m_tr_predictions, m_labeled_tr_predictions, m_tr_predictions_flat, m_roc_auc_tr, m_ts_accuracy, m_ts_sensitivity, m_ts_specificity, m_ts_predictions,m_labeled_ts_predictions, m_ts_predictions_flat, m_roc_auc_ts, m_roc_auc_total = output_list\n",
    "    print(\"\\nmodel: \"+model_names[m])\n",
    "    print(\"tr & ts for inter data: \"+str(m_tr_accuracy)+\", \"+str(m_ts_accuracy)+\"\\n\")\n",
    "    \n",
    "    model_l_new = Model(inputs = model_l.input, outputs=model_l.get_layer(model_l.layers[-2].name).output)\n",
    "    m_tr_result = model_l_new.predict([inter_dataset['tr_x_val'][m]])\n",
    "    m_ts_result = model_l_new.predict([inter_dataset['ts_x_val'][m]])\n",
    "    \n",
    "    model_output_list[\"tr_accuracy\"].append(m_tr_accuracy)\n",
    "    model_output_list[\"tr_sensitivity\"].append(m_tr_sensitivity)\n",
    "    model_output_list[\"tr_specificity\"].append(m_tr_specificity)\n",
    "    model_output_list[\"ts_accuracy\"].append(m_ts_accuracy)\n",
    "    model_output_list[\"ts_sensitivity\"].append(m_ts_sensitivity)\n",
    "    model_output_list[\"ts_specificity\"].append(m_ts_specificity)\n",
    "    model_output_list[\"tr_result\"].append(m_tr_result)\n",
    "    \n",
    "    model_output_list[\"tr_predictions\"].append(m_tr_predictions)\n",
    "    model_output_list[\"labeled_tr_predictions\"].append(m_labeled_tr_predictions)\n",
    "    model_output_list[\"tr_predictions_flat\"].append(m_tr_predictions_flat)\n",
    "    model_output_list[\"roc_auc_tr\"].append(m_roc_auc_tr)\n",
    "    model_output_list[\"ts_predictions\"].append(m_ts_predictions)\n",
    "    model_output_list[\"labeled_ts_predictions\"].append(m_labeled_ts_predictions)\n",
    "    model_output_list[\"ts_predictions_flat\"].append(m_ts_predictions_flat)\n",
    "    model_output_list[\"roc_auc_ts\"].append(m_roc_auc_ts)\n",
    "    model_output_list[\"ts_result\"].append(m_ts_result)\n",
    "    \n",
    "    model_output_list[\"roc_auc_total\"].append(m_roc_auc_total)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seperate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(len(model_names)):\n",
    "    print(\"#### [\"+str(m+1)+\"] \"+model_names[m]+\" ####\")\n",
    "    print(\"types: \"+types[model_index[m]])\n",
    "    print(\"tr: \"+str(model_output_list[\"tr_accuracy\"][m])+\", ts: \"+str(model_output_list[\"ts_accuracy\"][m])+\"\\n\")\n",
    "\n",
    "select = []\n",
    "while 1:\n",
    "    ch = input(\"input numbers for selection(1 ~ \"+str(len(model_names))+\". q for quit.: \")\n",
    "    if(ch == 'q'):\n",
    "        break\n",
    "    else:\n",
    "        select.append(int(ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_models_select = ensemble_coverage(model_list,inter_dataset[\"tr_x_val\"],inter_dataset[\"tr_y_val\"][0])\n",
    "print(\"model numbers: \"+str(len(e_models_select)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DNN-Combiner Ensmeble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Input listup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tr_predictions_select = []\n",
    "m_ts_predictions_select = []   \n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_tr_predictions_select.append(model_output_list[\"tr_predictions\"][select[i]-1])\n",
    "    m_ts_predictions_select.append(model_output_list[\"ts_predictions\"][select[i]-1])\n",
    "    #print(m_tr_predictions[select[i]-1].shape)\n",
    "    \n",
    "em_tr_x_val = np.concatenate(m_tr_predictions_select, axis=1)\n",
    "em_ts_x_val = np.concatenate(m_ts_predictions_select, axis=1)\n",
    "\n",
    "tr_y_val = inter_dataset[\"tr_y_val\"][0]\n",
    "ts_y_val = inter_dataset[\"ts_y_val\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em_tr_x_val.shape)\n",
    "print(em_ts_x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"################################## DNN em ##################################\")\n",
    "print(\"select: \"+str(select))\n",
    "for select_i in select:\n",
    "    print(\"\\n\"+types[model_index[select_i-1]])\n",
    "    print(model_names[select_i-1])\n",
    "\n",
    "    \n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "# 1) parameter setting\n",
    "em_adam = optimizers.Adam(lr=0.05)                                   \n",
    "em_input_drop_out = 0.3\n",
    "em_drop_out = 0\n",
    "em_batch_size = 5\n",
    "em_BN = True                           \n",
    "\n",
    "em_layers = [10]\n",
    "em_tr_loss_best = 100 # for saving best loss value \n",
    "em_best_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "em_input = Input(shape=(len(select),))\n",
    "em_dp = Dropout(em_input_drop_out)(em_input)\n",
    "for l in em_layers:\n",
    "    if em_BN == True:\n",
    "        em_m = Dense(l)(em_dp)\n",
    "        em_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(em_m)\n",
    "        em_dp = Activation(\"relu\")(em_bn)\n",
    "    else:\n",
    "        em_m = Dense(l,activation='relu')(em_dp)\n",
    "        em_dp = Dropout(drop_out_m)(em_m)\n",
    "\n",
    "em_final = em_dp\n",
    "em_output = Dense(1, activation=\"sigmoid\")(em_final)\n",
    "em_model = Model(inputs=em_input,outputs=em_output)\n",
    "em_model.compile(optimizer=em_adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    em_model.fit(em_tr_x_val, tr_y_val, batch_size=em_batch_size, nb_epoch=1, verbose = 0)\n",
    "    em_tr_loss=em_model.evaluate( em_tr_x_val, tr_y_val)[0]\n",
    "    if em_tr_loss < em_tr_loss_best: # new best model. count reset.\n",
    "        em_tr_loss_best = em_tr_loss\n",
    "        count=0\n",
    "        em_best_model = em_model\n",
    "    if count>10: # no increase three time. stop.\n",
    "        em_model = em_best_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "print(\"Model em\" +\"-\"+str(ts_i)+\" trained.\")\n",
    "\n",
    "# 4) save model\n",
    "em_model.save(save_model_path+\"/m_em-\"+str(ts_i)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _DNN Combiner_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_output_list = model_performance(\n",
    "    information = False, using_model=em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "em_tr_loss, em_tr_accuracy, em_tr_sensitivity, em_tr_specificity, em_tr_predictions, em_labeled_tr_predictions, em_tr_predictions_flat, em_roc_auc_tr, em_ts_loss, em_ts_accuracy, em_ts_sensitivity, em_ts_specificity, em_ts_predictions,em_labeled_ts_predictions, em_ts_predictions_flat, em_roc_auc_ts, em_roc_auc_total = em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", em_roc_auc_total)\n",
    "print(\"Train AUC: \", em_roc_auc_tr)\n",
    "print(\"Test AUC: \", em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(em_tr_sensitivity)+\", \"+str(em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(em_ts_sensitivity)+\", \"+str(em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(em_tr_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_em.to_csv(save_prediction_path+\"m_em-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(em_ts_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_em.to_csv(save_prediction_path+\"m_em-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mean Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _mean_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_em_tr_predictions=sum(m_tr_predictions_select)/len(select)\n",
    "mean_em_ts_predictions=sum(m_ts_predictions_select)/len(select)\n",
    "\n",
    "mean_em_output_list = model_performance(\n",
    "    information = False, using_model=None,Input_Prediction_Passively = True, \n",
    "    tr_predictions=mean_em_tr_predictions, ts_predictions=mean_em_ts_predictions, \n",
    "    tr_x_val=em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_sensitivity\", \"tr_specificity\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_sensitivity\", \"ts_specificity\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "mean_em_tr_sensitivity, mean_em_tr_specificity,  mean_em_labeled_tr_predictions, mean_em_tr_predictions_flat, mean_em_roc_auc_tr, mean_em_ts_sensitivity, mean_em_ts_specificity, mean_em_labeled_ts_predictions, mean_em_ts_predictions_flat, mean_em_roc_auc_ts, mean_em_roc_auc_total = mean_em_output_list\n",
    "\n",
    "mean_em_tr_accuracy = sum(mean_em_labeled_tr_predictions==tr_y_val.values)/len(tr_y_val)\n",
    "mean_em_ts_accuracy = sum(mean_em_labeled_ts_predictions==ts_y_val.values)/len(ts_y_val)\n",
    "\n",
    "print(\"Overall AUC: \", mean_em_roc_auc_total)\n",
    "print(\"Train AUC: \", mean_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", mean_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(mean_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mean_em_tr_sensitivity)+\", \"+str(mean_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(mean_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mean_em_ts_sensitivity)+\", \"+str(mean_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_mean = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(mean_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_mean.to_csv(save_prediction_path+\"m_mean-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_mean = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(mean_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_mean.to_csv(save_prediction_path+\"m_mean-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Transferred Ensemble Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making new input data for t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tr_result_select = []\n",
    "m_ts_result_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_tr_result_select.append(model_output_list[\"tr_result\"][select[i]-1])\n",
    "    m_ts_result_select.append(model_output_list[\"ts_result\"][select[i]-1])\n",
    "\n",
    "t_em_tr_x_val = np.concatenate(m_tr_result_select, axis=1)\n",
    "t_em_ts_x_val = np.concatenate(m_ts_result_select, axis=1)\n",
    "print(\"\\n############################################### t-em x val merged. ###############################################\\n\")\n",
    "print(t_em_tr_x_val.shape)\n",
    "print(t_em_ts_x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling t-ensemble  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"################################## Transferred em ##################################\")\n",
    "print(\"select: \"+str(select))\n",
    "for select_i in select:\n",
    "    print(\"\\n\"+types[model_index[select_i-1]])\n",
    "    print(model_names[select_i-1])\n",
    "\n",
    "    \n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "# 1) parameter setting\n",
    "t_em_adam = optimizers.Adam(lr=0.05)                                   \n",
    "t_em_input_drop_out = 0.3\n",
    "t_em_drop_out = 0\n",
    "t_em_batch_size = 5\n",
    "t_em_BN = True                           \n",
    "\n",
    "t_em_layers = [100]\n",
    "t_em_tr_loss_best = 100 # for saving best loss value \n",
    "t_em_best_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "t_em_input = Input(shape=(t_em_ts_x_val.shape[1],))\n",
    "t_em_dp = Dropout(t_em_input_drop_out)(t_em_input)\n",
    "for l in t_em_layers:\n",
    "    if t_em_BN == True:\n",
    "        t_em_m = Dense(l)(t_em_dp)\n",
    "        t_em_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(t_em_m)\n",
    "        t_em_dp = Activation(\"relu\")(t_em_bn)\n",
    "    else:\n",
    "        t_em_m = Dense(l,activation='relu')(t_em_dp)\n",
    "        t_em_dp = Dropout(drop_out_m)(t_em_m)\n",
    "\n",
    "t_em_final = t_em_dp\n",
    "t_em_output = Dense(1, activation=\"sigmoid\")(t_em_final)\n",
    "t_em_model = Model(inputs=t_em_input,outputs=t_em_output)\n",
    "t_em_model.compile(optimizer=t_em_adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    t_em_model.fit(t_em_tr_x_val, tr_y_val, batch_size=t_em_batch_size, nb_epoch=1, verbose = 0)\n",
    "    t_em_tr_loss=t_em_model.evaluate( t_em_tr_x_val, tr_y_val)[0]\n",
    "    if t_em_tr_loss < t_em_tr_loss_best: # new best model. count reset.\n",
    "        t_em_tr_loss_best = t_em_tr_loss\n",
    "        count=0\n",
    "        t_em_best_model = t_em_model\n",
    "    if count>10: # no increase three time. stop.\n",
    "        t_em_model = t_em_best_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "        \n",
    "print(\"Model t-em\" +\"-\"+str(ts_i)+\" trained.\")\n",
    "\n",
    "# 4) save model\n",
    "em_model.save(save_model_path+\"/m_t-em-\"+str(ts_i)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_em_output_list = model_performance(\n",
    "    information = False, using_model=t_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=t_em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=t_em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "t_em_tr_loss, t_em_tr_accuracy, t_em_tr_sensitivity, t_em_tr_specificity, t_em_tr_predictions, t_em_labeled_tr_predictions, t_em_tr_predictions_flat, t_em_roc_auc_tr, t_em_ts_loss, t_em_ts_accuracy, t_em_ts_sensitivity, t_em_ts_specificity, t_em_ts_predictions,t_em_labeled_ts_predictions, t_em_ts_predictions_flat, t_em_roc_auc_ts, t_em_roc_auc_total = t_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", t_em_roc_auc_total)\n",
    "print(\"Train AUC: \", t_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", t_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(t_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(t_em_tr_sensitivity)+\", \"+str(t_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(t_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(t_em_ts_sensitivity)+\", \"+str(t_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_t_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(t_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_t_em.to_csv(save_prediction_path+\"m_t-em-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_t_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(t_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_t_em.to_csv(save_prediction_path+\"m_t-em-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferred Ensemble(Modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mo_transferred ensemble input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset : raw data + prediction results\n",
    "mo_em_tr_x_val = np.concatenate([inter_newDiff_dataset[\"tr_x_val\"], em_tr_x_val], axis = 1)\n",
    "mo_em_ts_x_val = np.concatenate([inter_newDiff_dataset[\"ts_x_val\"], em_ts_x_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mo_em_tr_x_val.shape)\n",
    "print(mo_em_ts_x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_em_matrix = np.concatenate([full_em_ts_x_val, full_em_tr_x_val], axis = 0)\n",
    "#df_full_dataset = pd.DataFrame(full_em_matrix)\n",
    "#df_full_dataset.to_csv(index=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_full_dataset = pd.DataFrame(full_em_matrix)\n",
    "df_full_dataset.to_csv(\"C:/test/merge_newDiff_400_with_predictions.csv\",index=False)\n",
    "\n",
    "'''\n",
    "#df_full_dataset.loc[df_full_dataset.shape[1]] = patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"modified t-ensemble model\")\n",
    "\n",
    "# 1) parameter setting\n",
    "mo_em_adam = optimizers.Adam(lr=0.05)                                   \n",
    "mo_em_input_drop_out = 0.3\n",
    "mo_em_drop_out = 0\n",
    "mo_em_batch_size = 5\n",
    "mo_em_BN = True                           \n",
    "\n",
    "mo_em_layers = [100]\n",
    "mo_em_tr_loss_best = 100 # for saving best loss value \n",
    "mo_em_best_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "mo_em_input = Input(shape=(mo_em_ts_x_val.shape[1],))\n",
    "mo_em_dp = Dropout(mo_em_input_drop_out)(mo_em_input)\n",
    "for l in mo_em_layers:\n",
    "    if mo_em_BN == True:\n",
    "        mo_em_m = Dense(l)(mo_em_dp)\n",
    "        mo_em_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(mo_em_m)\n",
    "        mo_em_dp = Activation(\"relu\")(mo_em_bn)\n",
    "    else:\n",
    "        mo_em_m = Dense(l,activation='relu')(mo_em_dp)\n",
    "        mo_em_dp = Dropout(drop_out_m)(mo_em_m)\n",
    "\n",
    "mo_em_final = mo_em_dp\n",
    "mo_em_output = Dense(1, activation=\"sigmoid\")(mo_em_final)\n",
    "mo_em_model = Model(inputs=mo_em_input,outputs=mo_em_output)\n",
    "mo_em_model.compile(optimizer=mo_em_adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    mo_em_model.fit(mo_em_tr_x_val, tr_y_val, batch_size=mo_em_batch_size, nb_epoch=1, verbose = 0)\n",
    "    mo_em_tr_loss=mo_em_model.evaluate( mo_em_tr_x_val, tr_y_val)[0]\n",
    "    if mo_em_tr_loss < mo_em_tr_loss_best: # new best model. count reset.\n",
    "        mo_em_tr_loss_best = mo_em_tr_loss\n",
    "        count=0\n",
    "        mo_em_best_model = mo_em_model\n",
    "    if count>10: # no increase three time. stop.\n",
    "        mo_em_model = mo_em_best_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "        \n",
    "print(\"Model mo-em\" +\"-\"+str(ts_i)+\" trained.\")\n",
    "\n",
    "# 4) save model\n",
    "em_model.save(save_model_path+\"/m_mo-em-\"+str(ts_i)+\".h5\")\n",
    "\n",
    "# 5) evaluate model\n",
    "mo_em_output_list = model_performance(\n",
    "    information = False, using_model=mo_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=mo_em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=mo_em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "mo_em_tr_loss, mo_em_tr_accuracy, mo_em_tr_sensitivity, mo_em_tr_specificity, mo_em_tr_predictions, mo_em_labeled_tr_predictions, mo_em_tr_predictions_flat, mo_em_roc_auc_tr, mo_em_ts_loss, mo_em_ts_accuracy, mo_em_ts_sensitivity, mo_em_ts_specificity, mo_em_ts_predictions,mo_em_labeled_ts_predictions, mo_em_ts_predictions_flat, mo_em_roc_auc_ts, mo_em_roc_auc_total = mo_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", mo_em_roc_auc_total)\n",
    "print(\"Train AUC: \", mo_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", mo_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(mo_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mo_em_tr_sensitivity)+\", \"+str(mo_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(mo_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mo_em_ts_sensitivity)+\", \"+str(mo_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_mo_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(mo_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(mo_em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_mo_em.to_csv(save_prediction_path+\"m_mo-em-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_mo_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(mo_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(mo_em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_mo_em.to_csv(save_prediction_path+\"m_mo-em-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model_index = str(model_index[select[0]-1]) \n",
    "select_model_names = str(model_names[select[0]-1]) \n",
    "for select_i in select[1:]:\n",
    "    print(types[model_index[select_i-1]])\n",
    "    print(model_names[select_i-1]+\"\\n\")\n",
    "    select_model_index = select_model_index+\" & \"+str(model_index[select_i-1])\n",
    "    select_model_names = select_model_names+\" & \"+str(model_names[select_i-1]) \n",
    "\n",
    "print(select_model_index)\n",
    "print(select_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_accuracy_list = [em_tr_accuracy, mean_em_tr_accuracy, t_em_tr_accuracy, mo_em_tr_accuracy]\n",
    "ts_accuracy_list = [em_ts_accuracy, mean_em_ts_accuracy, t_em_ts_accuracy, mo_em_ts_accuracy]\n",
    "tr_sensitivity_list = [em_tr_sensitivity, mean_em_tr_sensitivity, t_em_tr_sensitivity, mo_em_tr_sensitivity]\n",
    "ts_sensitivity_list = [em_ts_sensitivity, mean_em_ts_sensitivity, t_em_ts_sensitivity, mo_em_ts_sensitivity]\n",
    "tr_specificity_list = [em_tr_specificity, mean_em_tr_specificity, t_em_tr_specificity, mo_em_tr_specificity]\n",
    "ts_specificity_list = [em_ts_specificity, mean_em_ts_specificity, t_em_ts_specificity, mo_em_ts_specificity]\n",
    "tr_roc_list = [em_roc_auc_tr, mean_em_roc_auc_tr, t_em_roc_auc_tr, mo_em_roc_auc_tr]\n",
    "ts_roc_list = [em_roc_auc_ts, mean_em_roc_auc_ts, t_em_roc_auc_ts, mo_em_roc_auc_ts]\n",
    "total_roc_list = [em_roc_auc_total, mean_em_roc_auc_total, t_em_roc_auc_total, mo_em_roc_auc_total]\n",
    "\n",
    "for type_index in range(4):\n",
    "    em_output_list[\"em_type\"].append(em_type[type_index])\n",
    "    em_output_list[\"test_index\"].append(ts_i)\n",
    "    em_output_list[\"ensemble_comb\"].append(select_model_index)\n",
    "    em_output_list[\"ensemble_names\"].append(select_model_names)\n",
    "    em_output_list[\"tr_accuracy\"].append(tr_accuracy_list[type_index] )\n",
    "    em_output_list[\"ts_accuracy\"].append(ts_accuracy_list[type_index] )\n",
    "    em_output_list[\"tr_sensitivity\"].append(tr_sensitivity_list[type_index] )\n",
    "    em_output_list[\"ts_sensitivity\"].append(ts_sensitivity_list[type_index] )\n",
    "    em_output_list[\"tr_specificity\"].append(tr_specificity_list[type_index] )\n",
    "    em_output_list[\"ts_specificity\"].append(ts_specificity_list[type_index] )\n",
    "    em_output_list[\"roc_auc_tr\"].append(tr_roc_list[type_index] )\n",
    "    em_output_list[\"roc_auc_ts\"].append(ts_roc_list[type_index] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_index in range(4):\n",
    "    df_sum = pd.DataFrame(data=em_output_list)\n",
    "    df_sum.to_csv(save_result_path+em_type[type_index]+\"_result_\"+str(cycle_index)+\".csv\", index=False, header=True,\n",
    "              columns = [\"ensemble_comb\", \"ensemble_names\",\"test_index\",\n",
    "                         \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \n",
    "                         \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\",\n",
    "                         \"roc_auc_total\", \"roc_auc_tr\", \"roc_auc_ts\"])\n",
    "cycle_index = cycle_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(em_output_list['test_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
