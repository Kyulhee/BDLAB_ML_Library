{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model, Sequential \n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide raw data into train / test & x_val / y_val\n",
    "def data_split(raw_data, index_col, test_index):\n",
    "    \n",
    "    train_data = raw_data.iloc[list(raw_data.iloc[:,index_col]!=test_index)]\n",
    "    test_data = raw_data.iloc[list(raw_data.iloc[:,index_col]==test_index)]\n",
    "    \n",
    "    y_val = train_data.Platinum_Status\n",
    "    x_val = train_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val = test_data.Platinum_Status\n",
    "    test_x_val = test_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    return train_data, test_data, y_val, x_val, test_y_val, test_x_val\n",
    "\n",
    "    # raw_data: have gene_expressions(maybe multiple columns), index column, Platinum_Status column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all of model performance \n",
    "# - predictions(probability) / labeled predictions(0/1) / Loss / Accuracy / Sensitivity / Specificity / AUC values of Train / Test dataset.\n",
    "# using trained models, or you can put predictions(probability) passively(in this case, Loss & Accuracy do not provided.)\n",
    "def model_performance(information=False, Input_Prediction_Passively=False, using_model=None, tr_predictions=None, ts_predictions=None, tr_x_val=None, tr_y_val=None, ts_x_val=None, ts_y_val=None, output_list=None):\n",
    "    \n",
    "    if information == True:            \n",
    "        print(\"options model_performance:\\n1) using_model: keras models that you want to check performance. \\\"Input_Prediction_Passive\\\" option for input prediction list instead using models.\\n3) tr_predictions & ts_predictions: prediction input passively. put this data only when not using keras model.\\n4) tr_x_val & ts_x_val: input samples of train/test samples.\\n4) tr_y_val & ts_y_val: results of train/test samples.\\n5) output_list: return values that you want to recieve.\\n CAUTION: Essential variable.\\n\\t tr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\\n\\n* CAUTION: if 'None' value is returned, please check your input tr inputs(None value for tr outputs) or ts inputs(None value for ts outputs).\") \n",
    "        return 0\n",
    "    elif information != False:\n",
    "        print(\"for using information options, please set 'information' variable for 'True'\")\n",
    "        return -1\n",
    "    \n",
    "    if using_model is None:\n",
    "        if Input_Prediction_Passively == False:\n",
    "            print(\"ERROR: There are no models for using.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\") \n",
    "            return -1\n",
    "        elif (tr_predictions is None) and (ts_predictions is None): # No model/prediction input. no performance should be calculated.\n",
    "                print(\"ERROR: Input prediction list instead using saved model.\")\n",
    "                return -1\n",
    "        else: # No model input, but Input_Prediction_Passively is True & input prediction is valid.\n",
    "            tr_loss,tr_accuracy= None, None\n",
    "            ts_loss,ts_accuracy= None, None\n",
    "            \n",
    "    elif Input_Prediction_Passively == True: # both of model/prediction putted, could cause confusing.\n",
    "        ch = input(\"You put both model and prediction. Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "        while 1:\n",
    "            if ch == 'p':\n",
    "                using_model = None\n",
    "                break\n",
    "            elif ch == 'm':\n",
    "                tr_predictions = None\n",
    "                ts_predictions = None\n",
    "                break\n",
    "            elif ch == 'e':\n",
    "                return 0\n",
    "            else:\n",
    "                print(\"you put worng option: \"+str(ch))\n",
    "            ch = input(\"Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "                \n",
    "    if output_list is None:\n",
    "        print(\"ERROR: There are no output_list for return.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\")\n",
    "        return -1\n",
    "    \n",
    "    if not(tr_x_val is None) and not(tr_y_val is None):\n",
    "        # predict tr result only when no tr_prediction input\n",
    "        if tr_predictions is None:\n",
    "            tr_loss,tr_accuracy= using_model.evaluate(tr_x_val,tr_y_val)\n",
    "            tr_predictions = using_model.predict(tr_x_val)\n",
    "        # tr sensitivity / specificity\n",
    "        labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "        tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, tr_y_val)\n",
    "        tr_predictions_flat = tr_predictions[:,0]   \n",
    "        # roc(tr)\n",
    "        fpr_tr, tpr_tr, threshold_tr = metrics.roc_curve(tr_y_val, tr_predictions)\n",
    "        roc_auc_tr = metrics.auc(fpr_tr, tpr_tr)\n",
    "    \n",
    "    if not(ts_x_val is None) and not(ts_y_val is None):\n",
    "        # predict ts result only when no ts_prediction input\n",
    "        if ts_predictions is None:\n",
    "            ts_loss,ts_accuracy= using_model.evaluate(ts_x_val,ts_y_val)\n",
    "            ts_predictions = using_model.predict(ts_x_val)\n",
    "        labeled_ts_predictions = np.where(ts_predictions > 0.5, 1, 0).flatten()\n",
    "        ts_sensitivity, ts_specificity = check_correct(labeled_ts_predictions, ts_y_val)\n",
    "        ts_predictions_flat = ts_predictions[:,0]   \n",
    "        # roc(ts)\n",
    "        fpr_ts, tpr_ts, threshold_ts = metrics.roc_curve(ts_y_val, ts_predictions)\n",
    "        roc_auc_ts = metrics.auc(fpr_ts, tpr_ts)    \n",
    "    \n",
    "    if (not(tr_x_val is None) and not(tr_y_val is None)) and (not(ts_x_val is None) and not(ts_y_val is None)):\n",
    "        y_true = np.append(tr_y_val, ts_y_val)\n",
    "        y_pred = np.append(tr_predictions, ts_predictions)\n",
    "        fpr_total, tpr_total, threshold_total = metrics.roc_curve(y_true, y_pred)\n",
    "        roc_auc_total = metrics.auc(fpr_total, tpr_total)\n",
    "        \n",
    "        \n",
    "    return_list = []\n",
    "    \n",
    "    for output in output_list:\n",
    "        \n",
    "        if(output == \"tr_loss\"):\n",
    "            return_list.append(tr_loss)\n",
    "                               \n",
    "        elif(output == \"tr_accuracy\"):\n",
    "            return_list.append(tr_accuracy)\n",
    "                               \n",
    "        elif(output == \"tr_sensitivity\"):\n",
    "            return_list.append(tr_sensitivity)\n",
    "                               \n",
    "        elif(output == \"tr_specificity\"):\n",
    "            return_list.append(tr_specificity)\n",
    "                               \n",
    "        elif(output == \"tr_predictions\"):\n",
    "            return_list.append(tr_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_tr_predictions\"):\n",
    "            return_list.append(labeled_tr_predictions)\n",
    "                               \n",
    "        elif(output == \"tr_predictions_flat\"):\n",
    "            return_list.append(tr_predictions_flat)\n",
    "            \n",
    "        elif(output == \"roc_auc_tr\"):\n",
    "            return_list.append(roc_auc_tr)\n",
    "\n",
    "        elif(output == \"ts_loss\"):\n",
    "            return_list.append(ts_loss)\n",
    "                               \n",
    "        elif(output == \"ts_accuracy\"):\n",
    "            return_list.append(ts_accuracy)\n",
    "                               \n",
    "        elif(output == \"ts_sensitivity\"):\n",
    "            return_list.append(ts_sensitivity)\n",
    "                               \n",
    "        elif(output == \"ts_specificity\"):\n",
    "            return_list.append(ts_specificity)\n",
    "                               \n",
    "        elif(output == \"ts_predictions\"):\n",
    "            return_list.append(ts_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_ts_predictions\"):\n",
    "            return_list.append(labeled_ts_predictions)\n",
    "                               \n",
    "        elif(output == \"ts_predictions_flat\"):\n",
    "            return_list.append(ts_predictions_flat)\n",
    "        \n",
    "        elif(output == \"roc_auc_ts\"):\n",
    "            return_list.append(roc_auc_ts)\n",
    "            \n",
    "        elif(output == \"roc_auc_total\"):\n",
    "            return_list.append(roc_auc_total)\n",
    "                               \n",
    "        else:\n",
    "            print(\"There are no options <\"+str(output)+\">. Please refer these output options:\\ntr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\")\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparation: Load Data Files, Select the Type for Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] file_name:  OV_six_fold_Annotation3000_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[2] file_name:  OV_six_fold_CV_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[3] file_name:  OV_six_fold_Var_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[4] file_name:  OV_six_fold_new_Diff_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[5] file_name:  OV_six_fold_Clin \n",
      "sample : 287  \n",
      "features : 35\n",
      "[6] file_name:  OV_six_fold_SNV_400 \n",
      "sample : 213  \n",
      "features : 402\n",
      "\n",
      "data files read.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "types = [\"OV_six_fold_Annotation3000_400\", \n",
    "         \"OV_six_fold_CV_400\", \n",
    "         \"OV_six_fold_Var_400\", \"OV_six_fold_new_Diff_400\",\n",
    "         \"OV_six_fold_Clin\", \n",
    "         \"OV_six_fold_SNV_400\" \n",
    "         ]\n",
    "\n",
    "# input pathes\n",
    "path = \"../TC_six_fold_subsamples/\"\n",
    "save_model_path = \"../models/\"\n",
    "save_prediction_path = \"../predictions/\"\n",
    "save_result_path = \"../result/\"\n",
    "\n",
    "file_1 = path+types[0]+\".csv\"\n",
    "file_2 = path+types[1]+\".csv\"\n",
    "file_3 = path+types[2]+\".csv\"\n",
    "file_4 = path+types[3]+\".csv\"\n",
    "file_5 = path+types[4]+\".csv\"\n",
    "file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "sample_1,features_1 = data_1.shape\n",
    "sample_2,features_2 = data_2.shape\n",
    "sample_3,features_3 = data_3.shape\n",
    "sample_4,features_4 = data_4.shape\n",
    "sample_5,features_5 = data_5.shape\n",
    "sample_6,features_6 = data_6.shape\n",
    "\n",
    "# Data frame include index & Platinum_Status column, substract 2 to calculate real number of features \n",
    "[features_1, features_2, features_3, features_4, features_5, features_6] = [features_1-2, features_2-2, features_3-2, features_4-2, features_5-2, features_6-2]\n",
    "\n",
    "ds_list = [data_1, data_2, data_3, data_4, data_5, data_6]\n",
    "sam_list = [sample_1, sample_2, sample_3, sample_4, sample_5, sample_6]\n",
    "fea_list = [features_1, features_2, features_3, features_4, features_5, features_6]\n",
    "\n",
    "print(\"[1] file_name: \", types[0], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "print(\"[2] file_name: \", types[1], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "print(\"[3] file_name: \", types[2], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "print(\"[4] file_name: \", types[3], \"\\nsample : {}  \\nfeatures : {}\".format(sample_4,features_4))\n",
    "print(\"[5] file_name: \", types[4], \"\\nsample : {}  \\nfeatures : {}\".format(sample_5,features_5))\n",
    "print(\"[6] file_name: \", types[5], \"\\nsample : {}  \\nfeatures : {}\".format(sample_6,features_6))\n",
    "\n",
    "print(\"\\ndata files read.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OV_six_fold_Annotation3000_400', 'OV_six_fold_CV_400', 'OV_six_fold_Var_400', 'OV_six_fold_new_Diff_400', 'OV_six_fold_Clin', 'OV_six_fold_SNV_400']\n",
      "select types: 3\n",
      "\n",
      "select: OV_six_fold_new_Diff_400\n",
      "data files splited with five fold.\n",
      "\n",
      "n of samples: \n",
      "[1]: 186 for train, 31 for test.\n",
      "[2]: 186 for train, 31 for test.\n",
      "[3]: 186 for train, 31 for test.\n",
      "[4]: 187 for train, 30 for test.\n",
      "[5]: 187 for train, 30 for test.\n"
     ]
    }
   ],
   "source": [
    "print(types)\n",
    "ch = input(\"select types: \")\n",
    "select_type = types[int(ch)]\n",
    "select_data = ds_list[int(ch)]\n",
    "select_sample = sam_list[int(ch)]\n",
    "select_features = fea_list[int(ch)]\n",
    "\n",
    "print(\"\\nselect: \"+select_type )\n",
    "\n",
    "dataset = {\"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "val_name = [\"tr_data\", \"ts_data\", \"tr_y_val\", \"tr_x_val\", \"ts_y_val\", \"ts_x_val\"]\n",
    "\n",
    "for i in range(1, 6):\n",
    "    tr_data, ts_data, tr_y_val, tr_x_val, ts_y_val, ts_x_val = data_split(raw_data = select_data, index_col = -1, test_index = i)\n",
    "    dataset['tr_data'].append(tr_data)\n",
    "    dataset['ts_data'].append(ts_data)\n",
    "    dataset['tr_x_val'].append(tr_x_val)\n",
    "    dataset['tr_y_val'].append(tr_y_val)\n",
    "    dataset['ts_x_val'].append(ts_x_val)\n",
    "    dataset['ts_y_val'].append(ts_y_val)      \n",
    "\n",
    "print(\"data files splited with five fold.\\n\")\n",
    "print(\"n of samples: \")\n",
    "print(\"[1]: \"+str(len(dataset['tr_x_val'][0]))+\" for train, \"+str(len(dataset['ts_x_val'][0]))+\" for test.\")\n",
    "print(\"[2]: \"+str(len(dataset['tr_x_val'][1]))+\" for train, \"+str(len(dataset['ts_x_val'][1]))+\" for test.\")\n",
    "print(\"[3]: \"+str(len(dataset['tr_x_val'][2]))+\" for train, \"+str(len(dataset['ts_x_val'][2]))+\" for test.\")\n",
    "print(\"[4]: \"+str(len(dataset['tr_x_val'][3]))+\" for train, \"+str(len(dataset['ts_x_val'][3]))+\" for test.\")\n",
    "print(\"[5]: \"+str(len(dataset['tr_x_val'][4]))+\" for train, \"+str(len(dataset['ts_x_val'][4]))+\" for test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test index(1~5): 1\n"
     ]
    }
   ],
   "source": [
    "# i is variable that indicate fold index number.\n",
    "ts_i = input(\"test index(1~5): \")\n",
    "ts_i = int(ts_i)\n",
    "\n",
    "model_list = []\n",
    "model_num_list = []\n",
    "test_index_list = []\n",
    "tr_acc_list = []\n",
    "ts_acc_list = []\n",
    "tr_sensitivity_list = []\n",
    "ts_sensitivity_list = []\n",
    "tr_specificity_list = []\n",
    "ts_specificity_list = []\n",
    "tr_auc_list = []\n",
    "ts_auc_list = []\n",
    "tot_auc_list = []\n",
    "\n",
    "k = 0\n",
    "lr_box = []\n",
    "layers_box = []\n",
    "batch_size_box = []\n",
    "drop_out_box = []\n",
    "input_drop_out_box = []\n",
    "batch_normalize_box = []\n",
    "count_lim_box = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter change\n",
    "\n",
    "lr_list = [0.05, 0.01]\n",
    "layer_list = [[50], [75], [100], [125]]\n",
    "count_lim_list = [5, 7, 10]\n",
    "\n",
    "#lr = 0.001\n",
    "drop_out_m = 0\n",
    "input_drop_out_m = 0.5\n",
    "batch_size = 5\n",
    "BN = True\n",
    "#layers = [100]\n",
    "#count_lim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for lr in lr_list:\n",
    "    for layers in layer_list:\n",
    "        for count_lim in count_lim_list:\n",
    "            \n",
    "            adam = optimizers.Adam(lr=lr)\n",
    "\n",
    "            m_tr_loss_best = 100 # for saving best loss value \n",
    "            best_m_model=[] #for saving best model\n",
    "            count=0 # for early stopping\n",
    "\n",
    "            # 2) model build\n",
    "            input_m = Input(shape=(select_features,))\n",
    "            m_m_dp = Dropout(input_drop_out_m)(input_m)\n",
    "            for l in layers:\n",
    "                if BN == True:\n",
    "                    m_m = Dense(l)(m_m_dp)\n",
    "                    m_m_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(m_m)\n",
    "                    m_m_dp = Activation(\"relu\")(m_m_bn)\n",
    "                else:\n",
    "                    m_m = Dense(l,activation='relu')(m_m_dp)\n",
    "                    m_m_dp = Dropout(drop_out_m)(m_m)\n",
    "\n",
    "            m_m_final = m_m_dp\n",
    "            output_m = Dense(1, activation=\"sigmoid\")(m_m_final)\n",
    "            m_model = Model(inputs=input_m,outputs=output_m)\n",
    "            m_model.compile(optimizer=adam, \n",
    "                            loss='binary_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "            # 3) Training: if no increase of tr_loss three times, stop training.\n",
    "            while 1:\n",
    "                m_model.fit(dataset['tr_x_val'][ts_i-1], dataset['tr_y_val'][ts_i-1], batch_size=batch_size, nb_epoch=1, verbose = 0)\n",
    "                m_tr_loss=m_model.evaluate( dataset['tr_x_val'][ts_i-1], dataset['tr_y_val'][ts_i-1])[0]\n",
    "                if m_tr_loss < m_tr_loss_best: # new best model. count reset.\n",
    "                    m_tr_loss_best = m_tr_loss\n",
    "                    count=0\n",
    "                    best_m_model = m_model\n",
    "                if count>count_lim: # no increase, stop.\n",
    "                    m_model = best_m_model\n",
    "                    break\n",
    "                else: count=count+1\n",
    "            print(\"Model \" + ch+\"-\"+str(ts_i)+\"_\"+str(k)+\" trained.\")\n",
    "\n",
    "            # 4) save model\n",
    "            #model_list.append(m_model)\n",
    "            m_model.save(save_model_path+\"/m_\"+ch+\"-\"+str(ts_i)+\"_\"+str(k)+\".h5\")\n",
    "            print(\"Model \"+ch+\"-\"+str(ts_i)+\"_\"+str(k)+\" saved.\")\n",
    "\n",
    "            # 5) evaluate model\n",
    "            m_output_list = model_performance(\n",
    "                information = False, using_model=m_model,Input_Prediction_Passively = False, \n",
    "                tr_x_val=dataset['tr_x_val'][ts_i-1], tr_y_val=dataset['tr_y_val'][ts_i-1], ts_x_val=dataset['ts_x_val'][ts_i-1], ts_y_val=dataset['ts_y_val'][ts_i-1],\n",
    "                output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                             \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                             \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                             \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                             \"roc_auc_total\"])\n",
    "\n",
    "            m_tr_loss, m_tr_accuracy, m_tr_sensitivity, m_tr_specificity, m_tr_predictions, m_labeled_tr_predictions, m_tr_predictions_flat, m_roc_auc_tr, m_ts_loss, m_ts_accuracy, m_ts_sensitivity, m_ts_specificity, m_ts_predictions,m_labeled_ts_predictions, m_ts_predictions_flat, m_roc_auc_ts, m_roc_auc_total = m_output_list\n",
    "\n",
    "            print(\"Overall AUC: \", m_roc_auc_total)\n",
    "            print(\"Train AUC: \", m_roc_auc_tr)\n",
    "            print(\"Test AUC: \", m_roc_auc_ts)\n",
    "\n",
    "            print(\"Train Accuracy: {}\".format(m_tr_accuracy))\n",
    "            print(\"Train Sensitivities & Specificities : \"+str(m_tr_sensitivity)+\", \"+str(m_tr_specificity))\n",
    "            print(\"Test Accuracy: {}\".format(m_ts_accuracy))\n",
    "            print(\"Test Sensitivities & Specificities : \"+str(m_ts_sensitivity)+\", \"+str(m_ts_specificity))\n",
    "\n",
    "            lr_box.append(lr)\n",
    "            layers_box.append(layers)\n",
    "            batch_size_box.append(batch_size)\n",
    "            drop_out_box.append(drop_out_m)\n",
    "            input_drop_out_box.append(input_drop_out_m)\n",
    "            batch_normalize_box.append(BN)\n",
    "            model_num_list.append(ch)\n",
    "            test_index_list.append(ts_i)\n",
    "            tr_acc_list.append(m_tr_accuracy)\n",
    "            ts_acc_list.append(m_ts_accuracy)\n",
    "            tr_sensitivity_list.append(m_tr_sensitivity)\n",
    "            ts_sensitivity_list.append(m_ts_sensitivity)\n",
    "            tr_specificity_list.append(m_tr_specificity)\n",
    "            ts_specificity_list.append(m_ts_specificity)\n",
    "            tr_auc_list.append(m_roc_auc_tr)\n",
    "            ts_auc_list.append(m_roc_auc_ts)\n",
    "            tot_auc_list.append(m_roc_auc_total)\n",
    "            count_lim_box.append(count_lim)\n",
    "\n",
    "            # save prediction result.\n",
    "\n",
    "            tr_df_m = pd.DataFrame(data={\"patient\":list(dataset['tr_data'][ts_i-1].index), \"hypothesis 1\": list(m_tr_predictions_flat), \n",
    "                                    \"prediction\":list(m_labeled_tr_predictions), \"Platinum_Status\":list(dataset['tr_y_val'][ts_i-1])}) \n",
    "            tr_df_m.to_csv(save_prediction_path+\"m_\"+ch+\"-\"+str(ts_i)+\"_\"+str(k)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "            ts_df_m = pd.DataFrame(data={\"patient\":list(dataset['ts_data'][ts_i-1].index), \"hypothesis 1\": list(m_ts_predictions_flat), \n",
    "                                    \"prediction\":list(m_labeled_ts_predictions), \"Platinum_Status\":list(dataset['ts_y_val'][ts_i-1])})\n",
    "            ts_df_m.to_csv(save_prediction_path+\"m_\"+ch+\"-\"+str(ts_i)+\"_\"+str(k)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "            k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df(data = {'model_numbers':model_num_list, \n",
    "            'index': range(0,k),\n",
    "            'rate':lr_box,\n",
    "            'count':count_lim_box,\n",
    "            'layers':layers_box,\n",
    "            'batch_size': batch_size_box,\n",
    "            'input_drop_out':input_drop_out_box,\n",
    "            'drop_out':drop_out_box,\n",
    "            'batch_normalize':batch_normalize_box,\n",
    "            'test_index':test_index_list ,\n",
    "            'tr_accuracy':tr_acc_list, \n",
    "            'tr_sensitivity':tr_sensitivity_list, \n",
    "            'tr_specificity':tr_specificity_list, \n",
    "            'ts_accuracy': ts_acc_list,\n",
    "            'ts_sensitivity':ts_sensitivity_list, \n",
    "            'ts_specificity':ts_specificity_list, \n",
    "            \"tr_auc\":tr_auc_list, \n",
    "            \"ts_auc\":ts_auc_list, \n",
    "            \"total_auc\":tot_auc_list}, \n",
    "      columns =['index','model_numbers', 'test_index', 'rate', 'count', 'layers','batch_size','input_drop_out','drop_out','batch_normalize', 'tr_accuracy', 'tr_sensitivity', 'tr_specificity', 'ts_accuracy', 'ts_sensitivity', 'ts_specificity', \"tr_auc\", \"ts_auc\", \"total_auc\"])\n",
    "df_1.to_csv(save_result_path+select_type+\"_\"+str(ts_i)+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
