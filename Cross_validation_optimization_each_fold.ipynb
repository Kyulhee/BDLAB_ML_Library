{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model, Sequential \n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide raw data into train / test & x_val / y_val\n",
    "def data_split(raw_data, index_col, test_index):\n",
    "    \n",
    "    train_data = raw_data.iloc[list(raw_data.iloc[:,index_col]!=test_index)]\n",
    "    test_data = raw_data.iloc[list(raw_data.iloc[:,index_col]==test_index)]\n",
    "    \n",
    "    y_val = train_data.Platinum_Status\n",
    "    x_val = train_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val = test_data.Platinum_Status\n",
    "    test_x_val = test_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    return train_data, test_data, y_val, x_val, test_y_val, test_x_val\n",
    "\n",
    "    # raw_data: have gene_expressions(maybe multiple columns), index column, Platinum_Status column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all of model performance \n",
    "# - predictions(probability) / labeled predictions(0/1) / Loss / Accuracy / Sensitivity / Specificity / AUC values of Train / Test dataset.\n",
    "# using trained models, or you can put predictions(probability) passively(in this case, Loss & Accuracy do not provided.)\n",
    "def model_performance(information=False, Input_Prediction_Passively=False, using_model=None, tr_predictions=None, ts_predictions=None, tr_x_val=None, tr_y_val=None, ts_x_val=None, ts_y_val=None, output_list=None):\n",
    "    \n",
    "    if information == True:            \n",
    "        print(\"options model_performance:\\n1) using_model: keras models that you want to check performance. \\\"Input_Prediction_Passive\\\" option for input prediction list instead using models.\\n3) tr_predictions & ts_predictions: prediction input passively. put this data only when not using keras model.\\n4) tr_x_val & ts_x_val: input samples of train/test samples.\\n4) tr_y_val & ts_y_val: results of train/test samples.\\n5) output_list: return values that you want to recieve.\\n CAUTION: Essential variable.\\n\\t tr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\\n\\n* CAUTION: if 'None' value is returned, please check your input tr inputs(None value for tr outputs) or ts inputs(None value for ts outputs).\") \n",
    "        return 0\n",
    "    elif information != False:\n",
    "        print(\"for using information options, please set 'information' variable for 'True'\")\n",
    "        return -1\n",
    "    \n",
    "    if using_model is None:\n",
    "        if Input_Prediction_Passively == False:\n",
    "            print(\"ERROR: There are no models for using.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\") \n",
    "            return -1\n",
    "        elif (tr_predictions is None) and (ts_predictions is None): # No model/prediction input. no performance should be calculated.\n",
    "                print(\"ERROR: Input prediction list instead using saved model.\")\n",
    "                return -1\n",
    "        else: # No model input, but Input_Prediction_Passively is True & input prediction is valid.\n",
    "            tr_loss,tr_accuracy= None, None\n",
    "            ts_loss,ts_accuracy= None, None\n",
    "            \n",
    "    elif Input_Prediction_Passively == True: # both of model/prediction putted, could cause confusing.\n",
    "        ch = input(\"You put both model and prediction. Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "        while 1:\n",
    "            if ch == 'p':\n",
    "                using_model = None\n",
    "                break\n",
    "            elif ch == 'm':\n",
    "                tr_predictions = None\n",
    "                ts_predictions = None\n",
    "                break\n",
    "            elif ch == 'e':\n",
    "                return 0\n",
    "            else:\n",
    "                print(\"you put worng option: \"+str(ch))\n",
    "            ch = input(\"Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "                \n",
    "    if output_list is None:\n",
    "        print(\"ERROR: There are no output_list for return.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\")\n",
    "        return -1\n",
    "    \n",
    "    if not(tr_x_val is None) and not(tr_y_val is None):\n",
    "        # predict tr result only when no tr_prediction input\n",
    "        if tr_predictions is None:\n",
    "            tr_loss,tr_accuracy= using_model.evaluate(tr_x_val,tr_y_val)\n",
    "            tr_predictions = using_model.predict(tr_x_val)\n",
    "        # tr sensitivity / specificity\n",
    "        labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "        tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, tr_y_val)\n",
    "        tr_predictions_flat = tr_predictions[:,0]   \n",
    "        # roc(tr)\n",
    "        fpr_tr, tpr_tr, threshold_tr = metrics.roc_curve(tr_y_val, tr_predictions)\n",
    "        roc_auc_tr = metrics.auc(fpr_tr, tpr_tr)\n",
    "    \n",
    "    if not(ts_x_val is None) and not(ts_y_val is None):\n",
    "        # predict ts result only when no ts_prediction input\n",
    "        if ts_predictions is None:\n",
    "            ts_loss,ts_accuracy= using_model.evaluate(ts_x_val,ts_y_val)\n",
    "            ts_predictions = using_model.predict(ts_x_val)\n",
    "        labeled_ts_predictions = np.where(ts_predictions > 0.5, 1, 0).flatten()\n",
    "        ts_sensitivity, ts_specificity = check_correct(labeled_ts_predictions, ts_y_val)\n",
    "        ts_predictions_flat = ts_predictions[:,0]   \n",
    "        # roc(ts)\n",
    "        fpr_ts, tpr_ts, threshold_ts = metrics.roc_curve(ts_y_val, ts_predictions)\n",
    "        roc_auc_ts = metrics.auc(fpr_ts, tpr_ts)    \n",
    "    \n",
    "    if (not(tr_x_val is None) and not(tr_y_val is None)) and (not(ts_x_val is None) and not(ts_y_val is None)):\n",
    "        y_true = np.append(tr_y_val, ts_y_val)\n",
    "        y_pred = np.append(tr_predictions, ts_predictions)\n",
    "        fpr_total, tpr_total, threshold_total = metrics.roc_curve(y_true, y_pred)\n",
    "        roc_auc_total = metrics.auc(fpr_total, tpr_total)\n",
    "        \n",
    "        \n",
    "    return_list = []\n",
    "    \n",
    "    for output in output_list:\n",
    "        \n",
    "        if(output == \"tr_loss\"):\n",
    "            return_list.append(tr_loss)\n",
    "                               \n",
    "        elif(output == \"tr_accuracy\"):\n",
    "            return_list.append(tr_accuracy)\n",
    "                               \n",
    "        elif(output == \"tr_sensitivity\"):\n",
    "            return_list.append(tr_sensitivity)\n",
    "                               \n",
    "        elif(output == \"tr_specificity\"):\n",
    "            return_list.append(tr_specificity)\n",
    "                               \n",
    "        elif(output == \"tr_predictions\"):\n",
    "            return_list.append(tr_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_tr_predictions\"):\n",
    "            return_list.append(labeled_tr_predictions)\n",
    "                               \n",
    "        elif(output == \"tr_predictions_flat\"):\n",
    "            return_list.append(tr_predictions_flat)\n",
    "            \n",
    "        elif(output == \"roc_auc_tr\"):\n",
    "            return_list.append(roc_auc_tr)\n",
    "\n",
    "        elif(output == \"ts_loss\"):\n",
    "            return_list.append(ts_loss)\n",
    "                               \n",
    "        elif(output == \"ts_accuracy\"):\n",
    "            return_list.append(ts_accuracy)\n",
    "                               \n",
    "        elif(output == \"ts_sensitivity\"):\n",
    "            return_list.append(ts_sensitivity)\n",
    "                               \n",
    "        elif(output == \"ts_specificity\"):\n",
    "            return_list.append(ts_specificity)\n",
    "                               \n",
    "        elif(output == \"ts_predictions\"):\n",
    "            return_list.append(ts_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_ts_predictions\"):\n",
    "            return_list.append(labeled_ts_predictions)\n",
    "                               \n",
    "        elif(output == \"ts_predictions_flat\"):\n",
    "            return_list.append(ts_predictions_flat)\n",
    "        \n",
    "        elif(output == \"roc_auc_ts\"):\n",
    "            return_list.append(roc_auc_ts)\n",
    "            \n",
    "        elif(output == \"roc_auc_total\"):\n",
    "            return_list.append(roc_auc_total)\n",
    "                               \n",
    "        else:\n",
    "            print(\"There are no options <\"+str(output)+\">. Please refer these output options:\\ntr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\")\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparation: Load Data Files, Select the Type for Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] file_name:  OV_six_fold_Annotation3000_100 \n",
      "sample : 217  \n",
      "features : 100\n",
      "[2] file_name:  OV_six_fold_CV_100 \n",
      "sample : 217  \n",
      "features : 100\n",
      "[3] file_name:  OV_six_fold_Var_100 \n",
      "sample : 217  \n",
      "features : 100\n",
      "[4] file_name:  OV_six_fold_new_Diff_100 \n",
      "sample : 217  \n",
      "features : 100\n",
      "[5] file_name:  OV_six_fold_Clin \n",
      "sample : 287  \n",
      "features : 35\n",
      "[6] file_name:  OV_six_fold_SNV \n",
      "sample : 213  \n",
      "features : 13814\n",
      "\n",
      "data files read.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "types = [\"OV_six_fold_Annotation3000_100\", \n",
    "         \"OV_six_fold_CV_100\", \n",
    "         \"OV_six_fold_Var_100\", \"OV_six_fold_new_Diff_100\",\n",
    "         \"OV_six_fold_Clin\", \n",
    "         \"OV_six_fold_SNV\" \n",
    "         ]\n",
    "\n",
    "# input pathes\n",
    "path = \"C:/test/TC_six_fold_subsamples/\"\n",
    "save_model_path = \"../models/Ovary\"\n",
    "save_prediction_path = \"../result/\"\n",
    "\n",
    "file_1 = path+types[0]+\".csv\"\n",
    "file_2 = path+types[1]+\".csv\"\n",
    "file_3 = path+types[2]+\".csv\"\n",
    "file_4 = path+types[3]+\".csv\"\n",
    "file_5 = path+types[4]+\".csv\"\n",
    "file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "sample_1,features_1 = data_1.shape\n",
    "sample_2,features_2 = data_2.shape\n",
    "sample_3,features_3 = data_3.shape\n",
    "sample_4,features_4 = data_4.shape\n",
    "sample_5,features_5 = data_5.shape\n",
    "sample_6,features_6 = data_6.shape\n",
    "\n",
    "# Data frame include index & Platinum_Status column, substract 2 to calculate real number of features \n",
    "[features_1, features_2, features_3, features_4, features_5, features_6] = [features_1-2, features_2-2, features_3-2, features_4-2, features_5-2, features_6-2]\n",
    "\n",
    "ds_list = [data_1, data_2, data_3, data_4, data_5, data_6]\n",
    "sam_list = [sample_1, sample_2, sample_3, sample_4, sample_5, sample_6]\n",
    "fea_list = [features_1, features_2, features_3, features_4, features_5, features_6]\n",
    "\n",
    "print(\"[1] file_name: \", types[0], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "print(\"[2] file_name: \", types[1], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "print(\"[3] file_name: \", types[2], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "print(\"[4] file_name: \", types[3], \"\\nsample : {}  \\nfeatures : {}\".format(sample_4,features_4))\n",
    "print(\"[5] file_name: \", types[4], \"\\nsample : {}  \\nfeatures : {}\".format(sample_5,features_5))\n",
    "print(\"[6] file_name: \", types[5], \"\\nsample : {}  \\nfeatures : {}\".format(sample_6,features_6))\n",
    "\n",
    "print(\"\\ndata files read.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OV_six_fold_Annotation3000_100', 'OV_six_fold_CV_100', 'OV_six_fold_Var_100', 'OV_six_fold_new_Diff_100', 'OV_six_fold_Clin', 'OV_six_fold_SNV']\n",
      "select types: 1\n",
      "\n",
      "select: OV_six_fold_CV_100\n",
      "data files splited with five fold.\n",
      "\n",
      "n of samples: \n",
      "[1]: 186 for train, 31 for test.\n",
      "[2]: 186 for train, 31 for test.\n",
      "[3]: 186 for train, 31 for test.\n",
      "[4]: 187 for train, 30 for test.\n",
      "[5]: 187 for train, 30 for test.\n"
     ]
    }
   ],
   "source": [
    "print(types)\n",
    "ch = input(\"select types: \")\n",
    "\n",
    "\n",
    "select_type = types[int(ch)]\n",
    "select_data = ds_list[int(ch)]\n",
    "select_sample = sam_list[int(ch)]\n",
    "select_features = fea_list[int(ch)]\n",
    "\n",
    "print(\"\\nselect: \"+data_type)\n",
    "\n",
    "# Split Train Test Data\n",
    "\n",
    "tr_data_1, ts_data_1, tr_y_val_1, tr_x_val_1, ts_y_val_1, ts_x_val_1 = data_split(raw_data = data_1, index_col = -1, test_index = 1)\n",
    "tr_data_2, ts_data_2, tr_y_val_2, tr_x_val_2, ts_y_val_2, ts_x_val_2 = data_split(raw_data = data_1, index_col = -1, test_index = 2)\n",
    "tr_data_3, ts_data_3, tr_y_val_3, tr_x_val_3, ts_y_val_3, ts_x_val_3 = data_split(raw_data = data_1, index_col = -1, test_index = 3)\n",
    "tr_data_4, ts_data_4, tr_y_val_4, tr_x_val_4, ts_y_val_4, ts_x_val_4 = data_split(raw_data = data_1, index_col = -1, test_index = 4)\n",
    "tr_data_5, ts_data_5, tr_y_val_5, tr_x_val_5, ts_y_val_5, ts_x_val_5 = data_split(raw_data = data_1, index_col = -1, test_index = 5)\n",
    "\n",
    "dataset = {\"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "val_name = [\"tr_data\", \"ts_data\", \"tr_y_val\", \"tr_x_val\", \"ts_y_val\", \"ts_x_val\"]\n",
    "\n",
    "for ts_i in range(1, 6):\n",
    "    tr_data, ts_data, tr_y_val, tr_x_val, ts_y_val, ts_x_val = data_split(raw_data = select_data, index_col = -1, test_index = ts_i)\n",
    "    dataset['tr_data'].append(tr_data)\n",
    "    dataset['ts_data'].append(ts_data)\n",
    "    dataset['tr_x_val'].append(tr_x_val)\n",
    "    dataset['tr_y_val'].append(tr_y_val)\n",
    "    dataset['ts_x_val'].append(ts_x_val)\n",
    "    dataset['ts_y_val'].append(ts_y_val)      \n",
    "\n",
    "print(\"data files splited with five fold.\\n\")\n",
    "print(\"n of samples: \")\n",
    "print(\"[1]: \"+str(len(dataset['tr_x_val'][0]))+\" for train, \"+str(len(dataset['ts_x_val'][0]))+\" for test.\")\n",
    "print(\"[2]: \"+str(len(dataset['tr_x_val'][1]))+\" for train, \"+str(len(dataset['ts_x_val'][1]))+\" for test.\")\n",
    "print(\"[3]: \"+str(len(dataset['tr_x_val'][2]))+\" for train, \"+str(len(dataset['ts_x_val'][2]))+\" for test.\")\n",
    "print(\"[4]: \"+str(len(dataset['tr_x_val'][3]))+\" for train, \"+str(len(dataset['ts_x_val'][3]))+\" for test.\")\n",
    "print(\"[5]: \"+str(len(dataset['tr_x_val'][4]))+\" for train, \"+str(len(dataset['ts_x_val'][4]))+\" for test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parameter_list = {\"lr\":[],\n",
    "                  \"batch_size\" :[],\n",
    "                  \"Batch_Normalize\":[],\n",
    "                  \"input_drop_out\":[],\n",
    "                  \"drop_out\":[],\n",
    "                  \"layers\":[]}\n",
    "'''\n",
    "parameter_list = {\"lr\":[0, 0, 0, 0, 0],\n",
    "                  \"batch_size\" :[0, 0, 0, 0, 0],\n",
    "                  \"Batch_Normalize\":[False, False, False, False, False],\n",
    "                  \"input_drop_out\":[0, 0, 0, 0, 0],\n",
    "                  \"drop_out\":[0, 0, 0, 0, 0],\n",
    "                  \"layers\":[0, 0, 0, 0, 0]}\n",
    "\n",
    "model_list = []\n",
    "model_num_list = []\n",
    "test_index_list = []\n",
    "tr_acc_list = []\n",
    "ts_acc_list = []\n",
    "tr_sensitivity_list = []\n",
    "ts_sensitivity_list = []\n",
    "tr_specificity_list = []\n",
    "ts_specificity_list = []\n",
    "tr_auc_list = []\n",
    "ts_auc_list = []\n",
    "tot_auc_list = []\n",
    "\n",
    "lr_box = []\n",
    "layers_box = []\n",
    "batch_size_box = []\n",
    "input_drop_out_box = []\n",
    "batch_normalize_box = []\n",
    "\n",
    "lr_list = [0.1, 0.05, 0.01, 0.005]\n",
    "layers_list = [[100], [100, 100], [50, 50, 50], [100, 50, 20], [100, 100, 100]]\n",
    "batch_size_list = [5, 10, 15]\n",
    "input_drop_out_list = [0.1, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-43f5e9798d80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[1;31m# 3) Training: if no increase of tr_loss three times, stop training.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                         \u001b[0mm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr_x_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr_y_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameter_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"batch_size\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                         \u001b[0mm_tr_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr_x_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr_y_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mm_tr_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mm_tr_loss_best\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# new best model. count reset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2697\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2699\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m# not already marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[1;32m--> 199\u001b[1;33m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m                 run_metadata):\n\u001b[0;32m   1319\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1379\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[1;32m-> 1381\u001b[1;33m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# i is variable that indicate fold index number.\n",
    "\n",
    "for i in range(1,6):\n",
    "    k = 0\n",
    "    Batch_Normalize = True\n",
    "    for lr in lr_list:\n",
    "        for layers in layers_list:\n",
    "            for batch_size in batch_size_list:\n",
    "                for input_drop_out in input_drop_out_list:\n",
    "                    lr_box.append(lr)\n",
    "                    layers_box.append(layers)\n",
    "                    batch_size_box.append(batch_size)\n",
    "                    input_drop_out_box.append(input_drop_out)\n",
    "                    batch_normalize_box.append(Batch_Normalize)\n",
    "                    '''lr = 0.001\n",
    "                    layers = [100,50,10]\n",
    "                    input_drop_out = 0.5\n",
    "                    batch_size = 20\n",
    "                    drop_out = 0'''\n",
    "\n",
    "                    parameter_list['Batch_Normalize'][i]=Batch_Normalize\n",
    "                    parameter_list['lr'][i] = lr\n",
    "                    parameter_list['layers'][i] = layers\n",
    "                    parameter_list['input_drop_out'][i] = input_drop_out\n",
    "                    parameter_list['batch_size'][i] = batch_size\n",
    "                    parameter_list['drop_out'][i] = drop_out\n",
    "\n",
    "\n",
    "                    adam = optimizers.Adam(lr=parameter_list[\"lr\"][i])                                   \n",
    "                    input_drop_out_m = parameter_list[\"input_drop_out\"][i]\n",
    "                    drop_out_m = parameter_list[\"drop_out\"][i]\n",
    "                    BN = parameter_list[\"Batch_Normalize\"][i]                           \n",
    "\n",
    "                    layers = parameter_list[\"layers\"][i]\n",
    "                    m_tr_loss_best = 100 # for saving best loss value \n",
    "                    best_m_model=[] #for saving best model\n",
    "                    count=0 # for early stopping\n",
    "\n",
    "                    # 2) model build\n",
    "                    input_m = Input(shape=(select_features,))\n",
    "                    m_m_dp = Dropout(input_drop_out_m)(input_m)\n",
    "                    for l in layers:\n",
    "                        if BN == True:\n",
    "                            m_m = Dense(l)(m_m_dp)\n",
    "                            m_m_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(m_m)\n",
    "                            m_m_dp = Activation(\"relu\")(m_m_bn)\n",
    "                        else:\n",
    "                            m_m = Dense(l,activation='relu')(m_m_dp)\n",
    "                            m_m_dp = Dropout(drop_out_m)(m_m)\n",
    "\n",
    "                    m_m_final = m_m_dp\n",
    "                    output_m = Dense(1, activation=\"sigmoid\")(m_m_final)\n",
    "                    m_model = Model(inputs=input_m,outputs=output_m)\n",
    "                    m_model.compile(optimizer=adam, \n",
    "                                    loss='binary_crossentropy',\n",
    "                                    metrics=['accuracy'])\n",
    "\n",
    "                    # 3) Training: if no increase of tr_loss three times, stop training.\n",
    "                    while 1:\n",
    "                        m_model.fit(dataset['tr_x_val'][i], dataset['tr_y_val'][i], batch_size=parameter_list[\"batch_size\"][i], nb_epoch=1, verbose = 0)\n",
    "                        m_tr_loss=m_model.evaluate( dataset['tr_x_val'][i], dataset['tr_y_val'][i])[0]\n",
    "                        if m_tr_loss < m_tr_loss_best: # new best model. count reset.\n",
    "                            m_tr_loss_best = m_tr_loss\n",
    "                            count=0\n",
    "                            best_m_model = m_model\n",
    "                        if count>10: # no increase three time. stop.\n",
    "                            m_model = best_m_model\n",
    "                            break\n",
    "                        else: count=count+1\n",
    "                    print(\"Model \" + ch+\"-\"+str(i)+\"_\"+str(k)+\" trained.\")\n",
    "\n",
    "                    # 4) save model\n",
    "                    model_list.append(m_model)\n",
    "                    m_model.save(save_model_path+\"/m_\"+ch+\"-\"+str(i)+\"_\"+str(k)+\".h5\")\n",
    "                    print(\"Model \"+ch+\"-\"+str(i)+\"_\"+str(k)+\" saved.\")\n",
    "\n",
    "                    # 5) evaluate model\n",
    "                    m_output_list = model_performance(\n",
    "                        information = False, using_model=m_model,Input_Prediction_Passively = False, \n",
    "                        tr_x_val=dataset['tr_x_val'][i], tr_y_val=dataset['tr_y_val'][i], ts_x_val=dataset['ts_x_val'][i], ts_y_val=dataset['ts_y_val'][i],\n",
    "                        output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                                     \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                                     \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                                     \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                                     \"roc_auc_total\"])\n",
    "\n",
    "                    m_tr_loss, m_tr_accuracy, m_tr_sensitivity, m_tr_specificity, m_tr_predictions, m_labeled_tr_predictions, m_tr_predictions_flat, m_roc_auc_tr, m_ts_loss, m_ts_accuracy, m_ts_sensitivity, m_ts_specificity, m_ts_predictions,m_labeled_ts_predictions, m_ts_predictions_flat, m_roc_auc_ts, m_roc_auc_total = m_output_list\n",
    "\n",
    "                    print(\"Overall AUC: \", m_roc_auc_total)\n",
    "                    print(\"Train AUC: \", m_roc_auc_tr)\n",
    "                    print(\"Test AUC: \", m_roc_auc_ts)\n",
    "\n",
    "                    print(\"Train Accuracy: {}\".format(m_tr_accuracy))\n",
    "                    print(\"Train Sensitivities & Specificities : \"+str(m_tr_sensitivity)+\", \"+str(m_tr_specificity))\n",
    "                    print(\"Test Accuracy: {}\".format(m_ts_accuracy))\n",
    "                    print(\"Test Sensitivities & Specificities : \"+str(m_ts_sensitivity)+\", \"+str(m_ts_specificity))\n",
    "\n",
    "                    model_num_list.append(ch)\n",
    "                    test_index_list.append(i)\n",
    "                    tr_acc_list.append(m_tr_accuracy)\n",
    "                    ts_acc_list.append(m_ts_accuracy)\n",
    "                    tr_sensitivity_list.append(m_tr_sensitivity)\n",
    "                    ts_sensitivity_list.append(m_ts_sensitivity)\n",
    "                    tr_specificity_list.append(m_tr_specificity)\n",
    "                    ts_specificity_list.append(m_ts_specificity)\n",
    "                    tr_auc_list.append(m_roc_auc_tr)\n",
    "                    ts_auc_list.append(m_roc_auc_ts)\n",
    "                    tot_auc_list.append(m_roc_auc_total)\n",
    "                    \n",
    "                    # save prediction result.\n",
    "\n",
    "                    tr_df_m = pd.DataFrame(data={\"patient\":list(dataset['tr_data'][i].index), \"hypothesis 1\": list(m_tr_predictions_flat), \n",
    "                                            \"prediction\":list(m_labeled_tr_predictions), \"Platinum_Status\":list(dataset['tr_y_val'][i])}) \n",
    "                    tr_df_m.to_csv(save_prediction_path+\"prediction_result_m_\"+ch+\"-\"+str(i)+\"_\"+str(k)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "                    ts_df_m = pd.DataFrame(data={\"patient\":list(dataset['ts_data'][i].index), \"hypothesis 1\": list(m_ts_predictions_flat), \n",
    "                                            \"prediction\":list(m_labeled_ts_predictions), \"Platinum_Status\":list(dataset['ts_y_val'][i])})\n",
    "                    ts_df_m.to_csv(save_prediction_path+\"prediction_result_m_\"+ch+\"-\"+str(i)+\"_\"+str(k)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "                    k = k+1\n",
    "    df_1 = df(data = {'rate':lr_box,\n",
    "                    'layers':layers_box,\n",
    "                    'batch_size': batch_size_box,\n",
    "                    'input_drop_out':input_drop_out_box,\n",
    "                    'batch_normalize':batch_normalize_box,\n",
    "                    'test_index':test_index_list ,\n",
    "                    'model_numbers':model_num_list, \n",
    "                    'tr_accuracy':tr_acc_list, \n",
    "                    'tr_sensitivity':tr_sensitivity_list, \n",
    "                    'tr_specificity':tr_specificity_list, \n",
    "                    'ts_accuracy': ts_acc_list,\n",
    "                    'ts_sensitivity':ts_sensitivity_list, \n",
    "                    'ts_specificity':ts_specificity_list, \n",
    "                    \"tr_auc\":tr_auc_list, \n",
    "                    \"ts_auc\":ts_auc_list, \n",
    "                    \"total_auc\":tot_auc_list}, \n",
    "              columns =['rate', 'layers','batch_size','input_drop_out','batch_normalize''test_index', 'model_numbers', 'tr_accuracy', 'tr_sensitivity', 'tr_specificity', 'ts_accuracy', 'ts_sensitivity', 'ts_specificity', \"tr_auc\", \"ts_auc\", \"total_auc\"])\n",
    "    df_1.to_csv(\"./batch_result\"+str(i)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-1bdd6c9a629b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                    \u001b[1;34m\"ts_auc\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mts_auc_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                    \"total_auc\":tot_auc_list}, \n\u001b[1;32m---> 17\u001b[1;33m              columns =['rate', 'layers','batch_size','input_drop_out','batch_normalize', 'test_index', 'model_numbers', 'tr_accuracy', 'tr_sensitivity', 'tr_specificity', 'ts_accuracy', 'ts_sensitivity', 'ts_specificity', \"tr_auc\", \"ts_auc\", \"total_auc\"])\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mdf_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./batch_result\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;31m# GH10856\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# raise ValueError if only scalars in dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   7400\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7402\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arrays must all be same length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7404\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    " df_1 = df(data = {'rate':lr_box,\n",
    "                    'layers':layers_box,\n",
    "                    'batch_size': batch_size_box,\n",
    "                    'input_drop_out':input_drop_out_box,\n",
    "                    'batch_normalize':batch_normalize_box,\n",
    "                    'test_index':test_index_list ,\n",
    "                    'model_numbers':model_num_list, \n",
    "                    'tr_accuracy':tr_acc_list, \n",
    "                    'tr_sensitivity':tr_sensitivity_list, \n",
    "                    'tr_specificity':tr_specificity_list, \n",
    "                    'ts_accuracy': ts_acc_list,\n",
    "                    'ts_sensitivity':ts_sensitivity_list, \n",
    "                    'ts_specificity':ts_specificity_list, \n",
    "                    \"tr_auc\":tr_auc_list, \n",
    "                    \"ts_auc\":ts_auc_list, \n",
    "                    \"total_auc\":tot_auc_list}, \n",
    "              columns =['rate', 'layers','batch_size','input_drop_out','batch_normalize', 'test_index', 'model_numbers', 'tr_accuracy', 'tr_sensitivity', 'tr_specificity', 'ts_accuracy', 'ts_sensitivity', 'ts_specificity', \"tr_auc\", \"ts_auc\", \"total_auc\"])\n",
    "df_1.to_csv(\"./batch_result\"+str(i)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6451612710952759,\n",
       " 0.6451612710952759,\n",
       " 0.5806451439857483,\n",
       " 0.7096773982048035,\n",
       " 0.6451612710952759,\n",
       " 0.6774193644523621,\n",
       " 0.6129032373428345,\n",
       " 0.6774193644523621,\n",
       " 0.5161290168762207,\n",
       " 0.6451612710952759,\n",
       " 0.6774193644523621,\n",
       " 0.6129032373428345,\n",
       " 0.6774193644523621,\n",
       " 0.6774193644523621,\n",
       " 0.7096773982048035,\n",
       " 0.7096773982048035,\n",
       " 0.7419354915618896,\n",
       " 0.6774193644523621,\n",
       " 0.6451612710952759,\n",
       " 0.774193525314331,\n",
       " 0.6451612710952759,\n",
       " 0.6774193644523621,\n",
       " 0.6774193644523621,\n",
       " 0.7096773982048035,\n",
       " 0.7096773982048035,\n",
       " 0.7096773982048035,\n",
       " 0.7096773982048035,\n",
       " 0.7096773982048035,\n",
       " 0.6451612710952759,\n",
       " 0.7096773982048035,\n",
       " 0.7096773982048035,\n",
       " 0.774193525314331,\n",
       " 0.7419354915618896,\n",
       " 0.774193525314331,\n",
       " 0.6451612710952759,\n",
       " 0.7096773982048035]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
