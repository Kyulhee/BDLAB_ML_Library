{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model, Sequential \n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide raw data into train / test & x_val / y_val\n",
    "def data_split(raw_data, index_col, test_index):\n",
    "    \n",
    "    train_data = raw_data.iloc[list(raw_data.iloc[:,index_col]!=test_index)]\n",
    "    test_data = raw_data.iloc[list(raw_data.iloc[:,index_col]==test_index)]\n",
    "    \n",
    "    y_val = train_data.Platinum_Status\n",
    "    x_val = train_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val = test_data.Platinum_Status\n",
    "    test_x_val = test_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    return train_data, test_data, y_val, x_val, test_y_val, test_x_val\n",
    "\n",
    "    # raw_data: have gene_expressions(maybe multiple columns), index column, Platinum_Status column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all of model performance \n",
    "# - predictions(probability) / labeled predictions(0/1) / Loss / Accuracy / Sensitivity / Specificity / AUC values of Train / Test dataset.\n",
    "# using trained models, or you can put predictions(probability) passively(in this case, Loss & Accuracy do not provided.)\n",
    "def model_performance(information=False, Input_Prediction_Passively=False, using_model=None, tr_predictions=None, ts_predictions=None, tr_x_val=None, tr_y_val=None, ts_x_val=None, ts_y_val=None, output_list=None):\n",
    "    \n",
    "    if information == True:            \n",
    "        print(\"options model_performance:\\n1) using_model: keras models that you want to check performance. \\\"Input_Prediction_Passive\\\" option for input prediction list instead using models.\\n3) tr_predictions & ts_predictions: prediction input passively. put this data only when not using keras model.\\n4) tr_x_val & ts_x_val: input samples of train/test samples.\\n4) tr_y_val & ts_y_val: results of train/test samples.\\n5) output_list: return values that you want to recieve.\\n CAUTION: Essential variable.\\n\\t tr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\\n\\n* CAUTION: if 'None' value is returned, please check your input tr inputs(None value for tr outputs) or ts inputs(None value for ts outputs).\") \n",
    "        return 0\n",
    "    elif information != False:\n",
    "        print(\"for using information options, please set 'information' variable for 'True'\")\n",
    "        return -1\n",
    "    \n",
    "    if using_model is None:\n",
    "        if Input_Prediction_Passively == False:\n",
    "            print(\"ERROR: There are no models for using.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\") \n",
    "            return -1\n",
    "        elif (tr_predictions is None) and (ts_predictions is None): # No model/prediction input. no performance should be calculated.\n",
    "                print(\"ERROR: Input prediction list instead using saved model.\")\n",
    "                return -1\n",
    "        else: # No model input, but Input_Prediction_Passively is True & input prediction is valid.\n",
    "            tr_loss,tr_accuracy= None, None\n",
    "            ts_loss,ts_accuracy= None, None\n",
    "            \n",
    "    elif Input_Prediction_Passively == True: # both of model/prediction putted, could cause confusing.\n",
    "        ch = input(\"You put both model and prediction. Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "        while 1:\n",
    "            if ch == 'p':\n",
    "                using_model = None\n",
    "                break\n",
    "            elif ch == 'm':\n",
    "                tr_predictions = None\n",
    "                ts_predictions = None\n",
    "                break\n",
    "            elif ch == 'e':\n",
    "                return 0\n",
    "            else:\n",
    "                print(\"you put worng option: \"+str(ch))\n",
    "            ch = input(\"Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "                \n",
    "    if output_list is None:\n",
    "        print(\"ERROR: There are no output_list for return.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\")\n",
    "        return -1\n",
    "    \n",
    "    if not(tr_x_val is None) and not(tr_y_val is None):\n",
    "        # predict tr result only when no tr_prediction input\n",
    "        if tr_predictions is None:\n",
    "            tr_loss,tr_accuracy= using_model.evaluate(tr_x_val,tr_y_val)\n",
    "            tr_predictions = using_model.predict(tr_x_val)\n",
    "        # tr sensitivity / specificity\n",
    "        labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "        tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, tr_y_val)\n",
    "        tr_predictions_flat = tr_predictions[:,0]   \n",
    "        # roc(tr)\n",
    "        fpr_tr, tpr_tr, threshold_tr = metrics.roc_curve(tr_y_val, tr_predictions)\n",
    "        roc_auc_tr = metrics.auc(fpr_tr, tpr_tr)\n",
    "    \n",
    "    if not(ts_x_val is None) and not(ts_y_val is None):\n",
    "        # predict ts result only when no ts_prediction input\n",
    "        if ts_predictions is None:\n",
    "            ts_loss,ts_accuracy= using_model.evaluate(ts_x_val,ts_y_val)\n",
    "            ts_predictions = using_model.predict(ts_x_val)\n",
    "        labeled_ts_predictions = np.where(ts_predictions > 0.5, 1, 0).flatten()\n",
    "        ts_sensitivity, ts_specificity = check_correct(labeled_ts_predictions, ts_y_val)\n",
    "        ts_predictions_flat = ts_predictions[:,0]   \n",
    "        # roc(ts)\n",
    "        fpr_ts, tpr_ts, threshold_ts = metrics.roc_curve(ts_y_val, ts_predictions)\n",
    "        roc_auc_ts = metrics.auc(fpr_ts, tpr_ts)    \n",
    "    \n",
    "    if (not(tr_x_val is None) and not(tr_y_val is None)) and (not(ts_x_val is None) and not(ts_y_val is None)):\n",
    "        y_true = np.append(tr_y_val, ts_y_val)\n",
    "        y_pred = np.append(tr_predictions, ts_predictions)\n",
    "        fpr_total, tpr_total, threshold_total = metrics.roc_curve(y_true, y_pred)\n",
    "        roc_auc_total = metrics.auc(fpr_total, tpr_total)\n",
    "        \n",
    "        \n",
    "    return_list = []\n",
    "    \n",
    "    for output in output_list:\n",
    "        \n",
    "        if(output == \"tr_loss\"):\n",
    "            return_list.append(tr_loss)\n",
    "                               \n",
    "        elif(output == \"tr_accuracy\"):\n",
    "            return_list.append(tr_accuracy)\n",
    "                               \n",
    "        elif(output == \"tr_sensitivity\"):\n",
    "            return_list.append(tr_sensitivity)\n",
    "                               \n",
    "        elif(output == \"tr_specificity\"):\n",
    "            return_list.append(tr_specificity)\n",
    "                               \n",
    "        elif(output == \"tr_predictions\"):\n",
    "            return_list.append(tr_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_tr_predictions\"):\n",
    "            return_list.append(labeled_tr_predictions)\n",
    "                               \n",
    "        elif(output == \"tr_predictions_flat\"):\n",
    "            return_list.append(tr_predictions_flat)\n",
    "            \n",
    "        elif(output == \"roc_auc_tr\"):\n",
    "            return_list.append(roc_auc_tr)\n",
    "\n",
    "        elif(output == \"ts_loss\"):\n",
    "            return_list.append(ts_loss)\n",
    "                               \n",
    "        elif(output == \"ts_accuracy\"):\n",
    "            return_list.append(ts_accuracy)\n",
    "                               \n",
    "        elif(output == \"ts_sensitivity\"):\n",
    "            return_list.append(ts_sensitivity)\n",
    "                               \n",
    "        elif(output == \"ts_specificity\"):\n",
    "            return_list.append(ts_specificity)\n",
    "                               \n",
    "        elif(output == \"ts_predictions\"):\n",
    "            return_list.append(ts_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_ts_predictions\"):\n",
    "            return_list.append(labeled_ts_predictions)\n",
    "                               \n",
    "        elif(output == \"ts_predictions_flat\"):\n",
    "            return_list.append(ts_predictions_flat)\n",
    "        \n",
    "        elif(output == \"roc_auc_ts\"):\n",
    "            return_list.append(roc_auc_ts)\n",
    "            \n",
    "        elif(output == \"roc_auc_total\"):\n",
    "            return_list.append(roc_auc_total)\n",
    "                               \n",
    "        else:\n",
    "            print(\"There are no options <\"+str(output)+\">. Please refer these output options:\\ntr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\")\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation: import & preprocessing data + import module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input path & name of models / raw data for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model_path & each model_name.\n",
    "# Caution: If you want to change input models, you also have to change selected data types.\n",
    "\n",
    "# ex) if you want to put these models: two CV, one Annot_3000,  one Var, one new_Diff, one Clin.\n",
    "\n",
    "'''\n",
    "\n",
    "m_1_name = CV_400_1.h5\n",
    "m_2_name = CV_400_2.h5\n",
    "m_3_name = Annot_3000_400_1.h5\n",
    "m_4_name = Var_400_0.h5\n",
    "m_5_name = new_Diff_400_2.h5\n",
    "m_6_name = Clin_400_1.h5\n",
    "--> if you change this part,\n",
    "\n",
    "select_types = [types[1], # \"inter_by_names_CV_400\"\n",
    "                types[1], # \"inter_by_names_CV_400\"\n",
    "                types[0], # \"inter_by_names_Annotation3000_400\"\n",
    "                types[2], # \"inter_by_names_Var_400\"\n",
    "                types[3], # \"inter_by_names_new_Diff_400\"\n",
    "                types[4]] # \"inter_by_names_Clin\"\n",
    "--> you also have to change this part.\n",
    "\n",
    "'''\n",
    "\n",
    "types = [\"inter_by_names_Annotation3000_400\", \"inter_by_names_CV_400\", \n",
    "         \"inter_by_names_Var_400\", \"inter_by_names_new_Diff_400\",\n",
    "         \"inter_by_names_Clin\", \n",
    "         \"inter_by_names_SNV\" \n",
    "         ]\n",
    "\n",
    "# input model path & ensemble data(Transcriptome, Cinical Information, Somatic Mutation data)\n",
    "# data path(server): /home/tjahn/TCGA_Ovary/01.Data/DNN/TC_intersect_subsamples_by_names \n",
    "model_path = \"G:/내 드라이브/Class/6과 7 사이(hell)/Lab/TCGA 난소암/Best_Models/18.09.15/\"\n",
    "path = \"C:/test/TC_intersect_subsamples_by_names/\"\n",
    "save_model_path = \"../models/Ovary\"\n",
    "save_prediction_path = \"../result/\"\n",
    "\n",
    "# change 'types' and 'load_model' part for using another models.\n",
    "m_1_name = \"Annot_3000_400_0.h5\"\n",
    "m_2_name = \"CV_400_0.h5\"\n",
    "m_3_name = \"Var_400_0.h5\"\n",
    "m_4_name = \"new_Diff_400_0.h5\"\n",
    "m_5_name = \"Clin_2.h5\"\n",
    "m_6_name = \"SNV_1.h5\"\n",
    "select_types = [types[0],\n",
    "                types[1],\n",
    "                types[2],\n",
    "                types[3],\n",
    "                types[4],\n",
    "                types[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6]\n",
      "['Annotation3000_400', 'new_Diff_400', 'Clin', 'SNV']\n"
     ]
    }
   ],
   "source": [
    "select = [1, 4, 5, 6]\n",
    "print(select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] file_name:  inter_by_names_Annotation3000_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[2] file_name:  inter_by_names_CV_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[3] file_name:  inter_by_names_Var_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[4] file_name:  inter_by_names_new_Diff_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[5] file_name:  inter_OV_Clin \n",
      "sample : 153  \n",
      "features : 35\n",
      "[6] file_name:  inter_OV_SNV \n",
      "sample : 153  \n",
      "features : 6970\n"
     ]
    }
   ],
   "source": [
    "file_1 = path+select_types[0]+\".csv\"\n",
    "file_2 = path+select_types[1]+\".csv\"\n",
    "file_3 = path+select_types[2]+\".csv\"\n",
    "file_4 = path+select_types[3]+\".csv\"\n",
    "file_5 = path+select_types[4]+\".csv\"\n",
    "file_6 = path+select_types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "sample_1,features_1 = data_1.shape\n",
    "sample_2,features_2 = data_2.shape\n",
    "sample_3,features_3 = data_3.shape\n",
    "sample_4,features_4 = data_4.shape\n",
    "sample_5,features_5 = data_5.shape\n",
    "sample_6,features_6 = data_6.shape\n",
    "\n",
    "# Data frame include index & Platinum_Status column, substract 2 to calculate real number of features \n",
    "[features_1, features_2, features_3, features_4, features_5, features_6] = [features_1-2, features_2-2, features_3-2, features_4-2, features_5-2, features_6-2]\n",
    "\n",
    "print(\"[1] file_name: \", select_types[0], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "print(\"[2] file_name: \", select_types[1], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "print(\"[3] file_name: \", select_types[2], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "print(\"[4] file_name: \", select_types[3], \"\\nsample : {}  \\nfeatures : {}\".format(sample_4,features_4))\n",
    "print(\"[5] file_name: \", select_types[4], \"\\nsample : {}  \\nfeatures : {}\".format(sample_5,features_5))\n",
    "print(\"[6] file_name: \", select_types[5], \"\\nsample : {}  \\nfeatures : {}\".format(sample_6,features_6))\n",
    "\n",
    "# Split Train Test Data\n",
    "\n",
    "train_data_1, test_data_1, y_val_1, x_val_1, test_y_val_1, test_x_val_1 = data_split(raw_data = data_1, index_col = -1, test_index = 1)\n",
    "train_data_2, test_data_2, y_val_2, x_val_2, test_y_val_2, test_x_val_2 = data_split(raw_data = data_2, index_col = -1, test_index = 1)\n",
    "train_data_3, test_data_3, y_val_3, x_val_3, test_y_val_3, test_x_val_3 = data_split(raw_data = data_3, index_col = -1, test_index = 1)\n",
    "train_data_4, test_data_4, y_val_4, x_val_4, test_y_val_4, test_x_val_4 = data_split(raw_data = data_4, index_col = -1, test_index = 1)\n",
    "train_data_5, test_data_5, y_val_5, x_val_5, test_y_val_5, test_x_val_5 = data_split(raw_data = data_5, index_col = -1, test_index = 1)\n",
    "train_data_6, test_data_6, y_val_6, x_val_6, test_y_val_6, test_x_val_6 = data_split(raw_data = data_6, index_col = -1, test_index = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import separate models & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 660us/step\n",
      "31/31 [==============================] - 0s 129us/step\n",
      "inter_by_names_Annotation3000_400\n",
      "Train Accuracy: 0.9262295091738466\n",
      "Test Accuracy: 0.774193525314331\n",
      "122/122 [==============================] - 0s 838us/step\n",
      "31/31 [==============================] - 0s 257us/step\n",
      "inter_by_names_CV_400\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.6451612710952759\n",
      "122/122 [==============================] - 0s 813us/step\n",
      "31/31 [==============================] - 0s 96us/step\n",
      "inter_by_names_Var_400\n",
      "Train Accuracy: 0.9426229449569202\n",
      "Test Accuracy: 0.7096773982048035\n",
      "122/122 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 96us/step\n",
      "inter_by_names_new_Diff_400\n",
      "Train Accuracy: 0.9508196662683956\n",
      "Test Accuracy: 0.8387096524238586\n",
      "122/122 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 225us/step\n",
      "inter_OV_Clin\n",
      "Train Accuracy: 0.7704918062100645\n",
      "Test Accuracy: 0.7419354915618896\n",
      "122/122 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 161us/step\n",
      "inter_OV_SNV\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.774193525314331\n"
     ]
    }
   ],
   "source": [
    "# model load & evaluation. <model_n_l> is full-layer model, <model_n_l_new> is without-sigmoid-layer model.\n",
    "'''\n",
    "Each model's tr_accuracy can be differ to original model, but ts_accuracy should be same to original tested models.\n",
    "Because we using full-size data(about 200 patients data used Transcriptome, Clinical, SNV models.) for train each models.\n",
    "In contrast, in this code, we using ensemble-input data(intersected 153 patients).\n",
    "For-training-patients may be different in ensemble data and whole size data, but for-test-patients are the same.\n",
    "'''\n",
    "\n",
    "model_1_l = load_model(model_path+m_1_name)\n",
    "m_1_l_tr_loss,m_1_l_tr_accuracy=model_1_l.evaluate(x_val_1,y_val_1)\n",
    "m_1_l_loss,m_1_l_accuracy= model_1_l.evaluate(test_x_val_1,test_y_val_1)\n",
    "model_1_l_new = Model(inputs = model_1_l.input, outputs=model_1_l.get_layer(model_1_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[0])\n",
    "print(\"Train Accuracy: {}\".format(m_1_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_1_l_accuracy))\n",
    "\n",
    "model_2_l = load_model(model_path+m_2_name)\n",
    "m_2_l_tr_loss,m_2_l_tr_accuracy=model_2_l.evaluate(x_val_2,y_val_2)\n",
    "m_2_l_loss,m_2_l_accuracy= model_2_l.evaluate(test_x_val_2,test_y_val_2)\n",
    "model_2_l_new = Model(inputs = model_2_l.input, outputs=model_2_l.get_layer(model_2_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[1])\n",
    "print(\"Train Accuracy: {}\".format(m_2_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_2_l_accuracy))\n",
    "\n",
    "model_3_l = load_model(model_path+m_3_name)\n",
    "m_3_l_tr_loss,m_3_l_tr_accuracy=model_3_l.evaluate(x_val_3,y_val_3)\n",
    "m_3_l_loss,m_3_l_accuracy= model_3_l.evaluate(test_x_val_3,test_y_val_3)\n",
    "model_3_l_new = Model(inputs = model_3_l.input, outputs=model_3_l.get_layer(model_3_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[2])\n",
    "print(\"Train Accuracy: {}\".format(m_3_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_3_l_accuracy))\n",
    "\n",
    "model_4_l = load_model(model_path+m_4_name)\n",
    "m_4_l_tr_loss,m_4_l_tr_accuracy=model_4_l.evaluate(x_val_4,y_val_4)\n",
    "m_4_l_loss,m_4_l_accuracy= model_4_l.evaluate(test_x_val_4,test_y_val_4)\n",
    "model_4_l_new = Model(inputs = model_4_l.input, outputs=model_4_l.get_layer(model_4_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[3])\n",
    "print(\"Train Accuracy: {}\".format(m_4_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_4_l_accuracy))\n",
    "\n",
    "model_5_l = load_model(model_path+m_5_name)\n",
    "m_5_l_tr_loss,m_5_l_tr_accuracy=model_5_l.evaluate(x_val_5,y_val_5)\n",
    "m_5_l_loss,m_5_l_accuracy= model_5_l.evaluate(test_x_val_5,test_y_val_5)\n",
    "model_5_l_new = Model(inputs = model_5_l.input, outputs=model_5_l.get_layer(model_5_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[4])\n",
    "print(\"Train Accuracy: {}\".format(m_5_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_5_l_accuracy))\n",
    "\n",
    "model_6_l = load_model(model_path+m_6_name)\n",
    "m_6_l_tr_loss,m_6_l_tr_accuracy=model_6_l.evaluate(x_val_6,y_val_6)\n",
    "m_6_l_loss,m_6_l_accuracy= model_6_l.evaluate(test_x_val_6,test_y_val_6)\n",
    "model_6_l_new = Model(inputs = model_6_l.input, outputs=model_6_l.get_layer(model_6_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[5])\n",
    "print(\"Train Accuracy: {}\".format(m_6_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_6_l_accuracy))\n",
    "\n",
    "m_1_predictions = model_1_l.predict(x_val_1)\n",
    "m_2_predictions = model_2_l.predict(x_val_2)\n",
    "m_3_predictions = model_3_l.predict(x_val_3)\n",
    "m_4_predictions = model_4_l.predict(x_val_4)\n",
    "m_5_predictions = model_5_l.predict(x_val_5)\n",
    "m_6_predictions = model_6_l.predict(x_val_6)\n",
    "\n",
    "m_1_test_predictions = model_1_l.predict(test_x_val_1)\n",
    "m_2_test_predictions = model_2_l.predict(test_x_val_2)\n",
    "m_3_test_predictions = model_3_l.predict(test_x_val_3)\n",
    "m_4_test_predictions = model_4_l.predict(test_x_val_4)\n",
    "m_5_test_predictions = model_5_l.predict(test_x_val_5)\n",
    "m_6_test_predictions = model_6_l.predict(test_x_val_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seperate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< inter_by_names_Annotation3000_400 > tr: 0.9262295091738466, ts: 0.774193525314331\n",
      "< inter_by_names_CV_400 > tr: 1.0, ts: 0.6451612710952759\n",
      "< inter_by_names_Var_400 > tr: 0.9426229449569202, ts: 0.7096773982048035\n",
      "< inter_by_names_new_Diff_400 > tr: 0.9508196662683956, ts: 0.8387096524238586\n",
      "< inter_OV_Clin > tr: 0.7704918062100645, ts: 0.7419354915618896\n",
      "< inter_OV_SNV > tr: 1.0, ts: 0.774193525314331\n"
     ]
    }
   ],
   "source": [
    "print(\"< \"+select_types[0]+\" > tr: \"+str(m_1_l_tr_accuracy)+\", ts: \"+str(m_1_l_accuracy))\n",
    "print(\"< \"+select_types[1]+\" > tr: \"+str(m_2_l_tr_accuracy)+\", ts: \"+str(m_2_l_accuracy))\n",
    "print(\"< \"+select_types[2]+\" > tr: \"+str(m_3_l_tr_accuracy)+\", ts: \"+str(m_3_l_accuracy))\n",
    "print(\"< \"+select_types[3]+\" > tr: \"+str(m_4_l_tr_accuracy)+\", ts: \"+str(m_4_l_accuracy))\n",
    "print(\"< \"+select_types[4]+\" > tr: \"+str(m_5_l_tr_accuracy)+\", ts: \"+str(m_5_l_accuracy))\n",
    "print(\"< \"+select_types[5]+\" > tr: \"+str(m_6_l_tr_accuracy)+\", ts: \"+str(m_6_l_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# select models for ensemble among loaded models.\n",
    "# CAUTION: Duplication(ex: select = [1, 1, 1, 3, 5]) is allowed, but it is same models, and have same predictions. They have same opinions.\n",
    "\n",
    "select = [1, 4, 5, 6]\n",
    "print(select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DNN-Combiner Ensmeble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building original ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################## DNN em ##################################\n",
      "select: [1, 4, 5, 6]\n",
      "inter_by_names_Annotation3000_400\n",
      "inter_by_names_new_Diff_400\n",
      "inter_OV_Clin\n",
      "inter_OV_SNV\n",
      "#############################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6553 - acc: 0.5902\n",
      "122/122 [==============================] - 0s 4ms/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.5705 - acc: 0.8279\n",
      "122/122 [==============================] - 0s 114us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 766us/step - loss: 0.5883 - acc: 0.7623\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 687us/step - loss: 0.5374 - acc: 0.7951\n",
      "122/122 [==============================] - 0s 131us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 662us/step - loss: 0.5567 - acc: 0.7623\n",
      "122/122 [==============================] - 0s 139us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 711us/step - loss: 0.4684 - acc: 0.8033\n",
      "122/122 [==============================] - 0s 123us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 617us/step - loss: 0.5053 - acc: 0.7623\n",
      "122/122 [==============================] - 0s 90us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 728us/step - loss: 0.5040 - acc: 0.7623\n",
      "122/122 [==============================] - 0s 65us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 621us/step - loss: 0.4718 - acc: 0.8033\n",
      "122/122 [==============================] - 0s 90us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 605us/step - loss: 0.4794 - acc: 0.7869\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 711us/step - loss: 0.3834 - acc: 0.8607\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 638us/step - loss: 0.3510 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 74us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 597us/step - loss: 0.4775 - acc: 0.8033\n",
      "122/122 [==============================] - 0s 114us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 670us/step - loss: 0.4170 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 613us/step - loss: 0.3098 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 74us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 654us/step - loss: 0.3453 - acc: 0.8770\n",
      "122/122 [==============================] - 0s 114us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 629us/step - loss: 0.3632 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 74us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 654us/step - loss: 0.3707 - acc: 0.8607\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 687us/step - loss: 0.4079 - acc: 0.8197\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 638us/step - loss: 0.4373 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 74us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 621us/step - loss: 0.3239 - acc: 0.8852\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 715us/step - loss: 0.3359 - acc: 0.8770\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 626us/step - loss: 0.3860 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 82us/step\n",
      "##################### DNN ensemble model trained. #####################\n",
      "##################### DNN ensemble model saved. #####################\n"
     ]
    }
   ],
   "source": [
    "print(\"################################## DNN em ##################################\")\n",
    "print(\"select: \"+str(select))\n",
    "for select_type_i in select:\n",
    "    print(select_types[select_type_i-1])\n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "input_drop_out_em = 0.3\n",
    "drop_out_em = 0.5\n",
    "layers = [5]\n",
    "em_tr_loss_best = 100 # for saving best loss value \n",
    "best_em_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "m_tr_predictions = [m_1_predictions, m_2_predictions, m_3_predictions, m_4_predictions, m_5_predictions, m_6_predictions]\n",
    "m_tr_predictions_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_tr_predictions_select.append(m_tr_predictions[select[i]-1])\n",
    "    #print(m_tr_predictions[select[i]-1].shape)\n",
    "    \n",
    "em_x_val = np.concatenate(m_tr_predictions_select, axis=1)\n",
    "\n",
    "input_em = Input(shape=(len(select),))\n",
    "em_m_dp = Dropout(input_drop_out_em)(input_em)\n",
    "for i in layers:\n",
    "    em_m = Dense(i,activation='relu')(em_m_dp)\n",
    "    em_m_dp = Dropout(drop_out_em)(em_m)\n",
    "em_m_final = em_m_dp\n",
    "output_em = Dense(1, activation=\"sigmoid\")(em_m_final)\n",
    "em_model = Model(inputs=input_em,outputs=output_em)\n",
    "em_model.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    em_model.fit(em_x_val, y_val_1, batch_size=5, nb_epoch=1)\n",
    "    em_tr_loss=em_model.evaluate(em_x_val,y_val_1)[0]\n",
    "    if em_tr_loss < em_tr_loss_best: # new best model. count reset.\n",
    "        em_tr_loss_best = em_tr_loss\n",
    "        count=0\n",
    "        best_em_model = em_model\n",
    "    if count>3: # no increase three time. stop.\n",
    "        em_model = best_em_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "print(\"##################### DNN ensemble model trained. #####################\")\n",
    "# 4) save model\n",
    "\n",
    "em_model.save(save_model_path+\"/m_em.h5\")\n",
    "print(\"##################### DNN ensemble model saved. #####################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _DNN Combiner_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 82us/step\n",
      "31/31 [==============================] - 0s 63us/step\n",
      "Overall AUC:  0.9632135306553911\n",
      "Train AUC:  1.0\n",
      "Test AUC:  0.7575757575757576\n",
      "Train Accuracy: 0.9426229449569202\n",
      "Train Sensitivities & Specificities : 0.7941176470588235, 1.0\n",
      "Test Accuracy: 0.7419354915618896\n",
      "Test Sensitivities & Specificities : 0.3333333333333333, 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "m_test_predictions = [m_1_test_predictions, m_2_test_predictions, m_3_test_predictions, m_4_test_predictions, m_5_test_predictions, m_6_test_predictions]\n",
    "m_test_predictions_select = []   \n",
    "#def prediction_result(pathway, tr_x_val, tr_y_val, ts_x_val, ts_y_val, tr_patients, ts_patients)\n",
    "for i in range(len(select)):\n",
    "    m_test_predictions_select.append(m_test_predictions[select[i]-1])\n",
    "    \n",
    "em_test_x_val = np.concatenate(m_test_predictions_select, axis=1)\n",
    "\n",
    "em_output_list = model_performance(\n",
    "    information = False, using_model=em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=em_x_val, tr_y_val=y_val_1, ts_x_val=em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "em_tr_loss, em_tr_accuracy, em_tr_sensitivity, em_tr_specificity, em_tr_predictions, em_labeled_tr_predictions, em_tr_predictions_flat, em_roc_auc_tr, em_ts_loss, em_ts_accuracy, em_ts_sensitivity, em_ts_specificity, em_ts_predictions,em_labeled_ts_predictions, em_ts_predictions_flat, em_roc_auc_ts, em_roc_auc_total = em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", em_roc_auc_total)\n",
    "print(\"Train AUC: \", em_roc_auc_tr)\n",
    "print(\"Test AUC: \", em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(em_tr_sensitivity)+\", \"+str(em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(em_ts_sensitivity)+\", \"+str(em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_em = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(em_tr_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_em.to_csv(save_prediction_path+\"prediction_result_DNN_em_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(em_ts_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_ts_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_em.to_csv(save_prediction_path+\"prediction_result_DNN_em_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mean Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _mean_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC:  0.8883720930232558\n",
      "Train AUC:  0.9378342245989304\n",
      "Test AUC:  0.6818181818181818\n",
      "Train Accuracy: 0.8360655737704918\n",
      "Train Sensitivities & Specificities : 0.5882352941176471, 0.9318181818181818\n",
      "Test Accuracy: 0.6129032258064516\n",
      "Test Sensitivities & Specificities : 0.0, 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "mean_em_tr_predictions=sum(m_tr_predictions_select)/len(select)\n",
    "mean_em_ts_predictions=sum(m_test_predictions_select)/len(select)\n",
    "\n",
    "mean_em_output_list = model_performance(\n",
    "    information = False, using_model=None,Input_Prediction_Passively = True, \n",
    "    tr_predictions=mean_em_tr_predictions, ts_predictions=mean_em_ts_predictions, \n",
    "    tr_x_val=em_x_val, tr_y_val=y_val_1, ts_x_val=em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_sensitivity\", \"tr_specificity\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_sensitivity\", \"ts_specificity\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "mean_em_tr_sensitivity, mean_em_tr_specificity,  mean_em_labeled_tr_predictions, mean_em_tr_predictions_flat, mean_em_roc_auc_tr, mean_em_ts_sensitivity, mean_em_ts_specificity, mean_em_labeled_ts_predictions, mean_em_ts_predictions_flat, mean_em_roc_auc_ts, mean_em_roc_auc_total = mean_em_output_list\n",
    "\n",
    "mean_em_tr_accuracy = sum(mean_em_labeled_tr_predictions==y_val_1.values)/len(y_val_1)\n",
    "mean_em_ts_accuracy = sum(mean_em_labeled_ts_predictions==test_y_val_1.values)/len(test_y_val_1)\n",
    "\n",
    "print(\"Overall AUC: \", mean_em_roc_auc_total)\n",
    "print(\"Train AUC: \", mean_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", mean_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(mean_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mean_em_tr_sensitivity)+\", \"+str(mean_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(mean_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mean_em_ts_sensitivity)+\", \"+str(mean_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_mean_em = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(mean_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_mean_em.to_csv(save_prediction_path+\"prediction_result_mean_em_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_mean_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(mean_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_ts_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_mean_em.to_csv(save_prediction_path+\"prediction_result_mean_em_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Transferred Ensemble Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making new input data for t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 1010)\n"
     ]
    }
   ],
   "source": [
    "results_m_1 = model_1_l_new.predict([x_val_1])\n",
    "results_m_2 = model_2_l_new.predict([x_val_2])\n",
    "results_m_3 = model_3_l_new.predict([x_val_3])\n",
    "results_m_4 = model_4_l_new.predict([x_val_4])\n",
    "results_m_5 = model_5_l_new.predict([x_val_5])\n",
    "results_m_6 = model_6_l_new.predict([x_val_6])\n",
    "\n",
    "results_m_sum = [results_m_1, results_m_2, results_m_3, results_m_4, results_m_5, results_m_6]\n",
    "results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    results_m_select.append(results_m_sum[select[i]-1])\n",
    "\n",
    "t_em_x_val = np.concatenate(results_m_select, axis=1)\n",
    "print(t_em_x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling t-ensemble  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6748 - acc: 0.6967\n",
      "122/122 [==============================] - 0s 2ms/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.4864 - acc: 0.7787\n",
      "122/122 [==============================] - 0s 114us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2880 - acc: 0.8770A: 0s - loss: 0.2785 - acc: 0.860\n",
      "122/122 [==============================] - 0s 131us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2445 - acc: 0.9344\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.3042 - acc: 0.9180\n",
      "122/122 [==============================] - 0s 131us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1544 - acc: 0.9590\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1328 - acc: 0.9426\n",
      "122/122 [==============================] - 0s 139us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.3731 - acc: 0.9016\n",
      "122/122 [==============================] - 0s 82us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1769 - acc: 0.9508\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "transffered ensemble model trained.\n"
     ]
    }
   ],
   "source": [
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.005)\n",
    "input_drop_out_t_em = 0.5\n",
    "drop_out_t_em = 0.5\n",
    "layers = [100, 100, 100, 100]\n",
    "t_em_tr_loss_best = 100 # for saving best loss value \n",
    "best_t_em_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_t_em = Input(shape=(t_em_x_val.shape[1],))\n",
    "t_em_m_dp = Dropout(input_drop_out_t_em)(input_t_em)\n",
    "for i in layers:\n",
    "    t_em_m = Dense(i,activation='relu')(t_em_m_dp)\n",
    "    t_em_m_dp = Dropout(drop_out_em)(t_em_m)\n",
    "t_em_m_final = t_em_m_dp\n",
    "output_t_em = Dense(1, activation=\"sigmoid\")(t_em_m_final)\n",
    "t_em_model = Model(inputs=input_t_em,outputs=output_t_em)\n",
    "t_em_model.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "\n",
    "#t_em_model.fit(t_em_x_val, y_val_1, batch_size=5, epochs = 100, validation_split = 0.1, callbacks=[early_stopping])\n",
    "while 1:\n",
    "    t_em_model.fit(t_em_x_val, y_val_1, batch_size=5, nb_epoch=1)\n",
    "    t_em_tr_loss=t_em_model.evaluate(t_em_x_val,y_val_1)[0]\n",
    "    if t_em_tr_loss < t_em_tr_loss_best: # new best model. count reset.\n",
    "        t_em_tr_loss_best = t_em_tr_loss\n",
    "        count=0\n",
    "        best_t_em_model = t_em_model\n",
    "    if count>3: # no increase three time. stop.\n",
    "        t_em_model = best_t_em_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "print(\"transffered ensemble model trained.\")\n",
    "t_em_model.save(save_model_path+\"t_em.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 98us/step\n",
      "31/31 [==============================] - 0s 63us/step\n",
      "Overall AUC:  0.9543340380549683\n",
      "Train AUC:  1.0\n",
      "Test AUC:  0.7222222222222222\n",
      "Train Accuracy: 0.9508196662683956\n",
      "Train Sensitivities & Specificities : 0.8235294117647058, 1.0\n",
      "Test Accuracy: 0.7419354915618896\n",
      "Test Sensitivities & Specificities : 0.3333333333333333, 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "test_results_m_1 = model_1_l_new.predict([test_x_val_1])\n",
    "test_results_m_2 = model_2_l_new.predict([test_x_val_2])\n",
    "test_results_m_3 = model_3_l_new.predict([test_x_val_3])\n",
    "test_results_m_4 = model_4_l_new.predict([test_x_val_4])\n",
    "test_results_m_5 = model_5_l_new.predict([test_x_val_5])\n",
    "test_results_m_6 = model_6_l_new.predict([test_x_val_6])\n",
    "\n",
    "test_results_m_sum = [test_results_m_1, test_results_m_2, test_results_m_3, test_results_m_4, test_results_m_5, test_results_m_6]\n",
    "test_results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    test_results_m_select.append(test_results_m_sum[select[i]-1])\n",
    "\n",
    "t_em_test_x_val = np.concatenate(test_results_m_select, axis=1)\n",
    "\n",
    "t_em_output_list = model_performance(\n",
    "    information = False, using_model=t_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=t_em_x_val, tr_y_val=y_val_1, ts_x_val=t_em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "t_em_tr_loss, t_em_tr_accuracy, t_em_tr_sensitivity, t_em_tr_specificity, t_em_tr_predictions, t_em_labeled_tr_predictions, t_em_tr_predictions_flat, t_em_roc_auc_tr, t_em_ts_loss, t_em_ts_accuracy, t_em_ts_sensitivity, t_em_ts_specificity, t_em_ts_predictions,t_em_labeled_ts_predictions, t_em_ts_predictions_flat, t_em_roc_auc_ts, t_em_roc_auc_total = t_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", t_em_roc_auc_total)\n",
    "print(\"Train AUC: \", t_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", t_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(t_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(t_em_tr_sensitivity)+\", \"+str(t_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(t_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(t_em_ts_sensitivity)+\", \"+str(t_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_t_em = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(t_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_t_em.to_csv(save_prediction_path+\"prediction_result_t_em_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_t_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(t_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_ts_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_t_em.to_csv(save_prediction_path+\"prediction_result_t_em_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< model1 > tr: 0.9262295091738466, ts: 0.774193525314331\n",
      "< model4 > tr: 0.9508196662683956, ts: 0.8387096524238586\n",
      "< model5 > tr: 0.7704918062100645, ts: 0.7419354915618896\n",
      "< model6 > tr: 1.0, ts: 0.774193525314331\n",
      "< mean-em > tr: 0.8360655737704918, ts: 0.6129032258064516\n",
      "< d-comb em > tr: 0.8934426180651931, ts: 0.7096773982048035\n",
      "< t-em > tr: 1.0, ts: 0.774193525314331\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "tr_accuracy_list = [m_1_l_tr_accuracy, m_2_l_tr_accuracy, m_3_l_tr_accuracy, m_4_l_tr_accuracy, m_5_l_tr_accuracy, m_6_l_tr_accuracy]\n",
    "ts_accuracy_list = [m_1_l_accuracy, m_2_l_accuracy, m_3_l_accuracy, m_4_l_accuracy, m_5_l_accuracy, m_6_l_accuracy]\n",
    "tr_accuracy_select = []\n",
    "ts_accuracy_select = []\n",
    "\n",
    "for i in select:\n",
    "    label.append(\"model\"+str(i))\n",
    "    tr_accuracy_select.append(tr_accuracy_list[i-1])\n",
    "    ts_accuracy_select.append(ts_accuracy_list[i-1])\n",
    "\n",
    "label = label+[\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "tr_accuracy_select= tr_accuracy_select + [mean_em_tr_accuracy, em_tr_accuracy, t_em_tr_accuracy]\n",
    "ts_accuracy_select= ts_accuracy_select + [mean_em_ts_accuracy, em_ts_accuracy, t_em_ts_accuracy]\n",
    "\n",
    "for model_num in range(len(label)):\n",
    "    print(\"< \"+label[model_num]+\" > tr: \"+str(tr_accuracy_select[model_num])+\", ts: \"+str(ts_accuracy_select[model_num]))\n",
    "\n",
    "#label = [\"model1\",\"model2\",\"model3\",\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "#accuracy = [m1_accuracy,m2_accuracy,m3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy ]\n",
    "#print(\"model1: \"+str(accuracy[0])+\"\\nmodel2: \"+str(accuracy[1])+\"\\nmodel3: \"+str(accuracy[2])+\"\\nmean-em: \"+str(accuracy[3])+\"\\nd-comb em: \"+str(accuracy[4])+\"\\nt-em: \"+str(accuracy[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot_bar_x():\n",
    "    # this is for plotting purpose\n",
    "    plt.figure(figsize=(30,20))\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([min(m_1_accuracy,m_2_accuracy,m_3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy)-0.02,1])\n",
    "    index = np.arange(len(label))\n",
    "    plt.bar(index, accuracy,color=['red', 'orange', 'yellow', \"green\",'blue', 'purple'],alpha=0.5,width=0.3)\n",
    "    plt.xlabel('Method', fontsize=35)\n",
    "    plt.ylabel('Accuracy', fontsize=35)\n",
    "    plt.yticks(fontsize=30)    \n",
    "    plt.xticks(index, label, fontsize=30, rotation=90)\n",
    "    plt.title('Performance Comparison for each Ensemble Model',fontsize=40)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plot_bar_x()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
