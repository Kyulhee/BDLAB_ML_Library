{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model, Sequential \n",
    "\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stacking(model,train,y,test,n_fold):\n",
    "    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "    test_pred=np.empty((test.shape[0],1),float)\n",
    "    train_pred=np.empty((0,1),float)\n",
    "    for train_indices,val_indices in folds.split(train,y.values):\n",
    "        x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "        y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "        model.fit(x_train,y_train)\n",
    "        \n",
    "        train_pred=np.append(train_pred,model.predict(x_val))\n",
    "        test_pred=np.append(test_pred,model.predict(test))\n",
    "    return test_pred.reshape(-1,1),train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation: import & preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input path & name of raw data for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model_path & each model_name.\n",
    "\n",
    "types = [\"OV_six_fold_Annotation3000_400\", \n",
    "         \"OV_six_fold_CV_400\", \n",
    "         \"OV_six_fold_Var_400\", \n",
    "         \"OV_six_fold_new_Diff_400\",\n",
    "         \"OV_six_fold_Clin\", \n",
    "         \"OV_six_fold_SNV_400\" \n",
    "         ]\n",
    "\n",
    "input_path = \"../TC_six_fold_subsamples/\"\n",
    "save_model_path = \"../models/\"\n",
    "save_prediction_path = \"../predictions/\"\n",
    "save_result_path = \"../result/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################## test index is 1 ##################################\n",
      "[0]: OV_six_fold_Annotation3000_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[1]: OV_six_fold_CV_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[2]: OV_six_fold_Var_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[3]: OV_six_fold_new_Diff_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[4]: OV_six_fold_Clin.\n",
      "full tr & ts: (256, 35), (31, 35)\n",
      "inter tr & ts: (122, 35), (31, 35)\n",
      "\n",
      "[5]: OV_six_fold_SNV_400.\n",
      "full tr & ts: (182, 402), (31, 402)\n",
      "inter tr & ts: (122, 402), (31, 402)\n",
      "\n",
      "################################## test index is 2 ##################################\n",
      "[0]: OV_six_fold_Annotation3000_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[1]: OV_six_fold_CV_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[2]: OV_six_fold_Var_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[3]: OV_six_fold_new_Diff_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[4]: OV_six_fold_Clin.\n",
      "full tr & ts: (256, 35), (31, 35)\n",
      "inter tr & ts: (122, 35), (31, 35)\n",
      "\n",
      "[5]: OV_six_fold_SNV_400.\n",
      "full tr & ts: (182, 402), (31, 402)\n",
      "inter tr & ts: (122, 402), (31, 402)\n",
      "\n",
      "################################## test index is 3 ##################################\n",
      "[0]: OV_six_fold_Annotation3000_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[1]: OV_six_fold_CV_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[2]: OV_six_fold_Var_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[3]: OV_six_fold_new_Diff_400.\n",
      "full tr & ts: (186, 400), (31, 400)\n",
      "inter tr & ts: (122, 400), (31, 400)\n",
      "\n",
      "[4]: OV_six_fold_Clin.\n",
      "full tr & ts: (256, 35), (31, 35)\n",
      "inter tr & ts: (122, 35), (31, 35)\n",
      "\n",
      "[5]: OV_six_fold_SNV_400.\n",
      "full tr & ts: (182, 402), (31, 402)\n",
      "inter tr & ts: (122, 402), (31, 402)\n",
      "\n",
      "################################## test index is 4 ##################################\n",
      "[0]: OV_six_fold_Annotation3000_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[1]: OV_six_fold_CV_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[2]: OV_six_fold_Var_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[3]: OV_six_fold_new_Diff_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[4]: OV_six_fold_Clin.\n",
      "full tr & ts: (257, 35), (30, 35)\n",
      "inter tr & ts: (123, 35), (30, 35)\n",
      "\n",
      "[5]: OV_six_fold_SNV_400.\n",
      "full tr & ts: (183, 402), (30, 402)\n",
      "inter tr & ts: (123, 402), (30, 402)\n",
      "\n",
      "################################## test index is 5 ##################################\n",
      "[0]: OV_six_fold_Annotation3000_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[1]: OV_six_fold_CV_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[2]: OV_six_fold_Var_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[3]: OV_six_fold_new_Diff_400.\n",
      "full tr & ts: (187, 400), (30, 400)\n",
      "inter tr & ts: (123, 400), (30, 400)\n",
      "\n",
      "[4]: OV_six_fold_Clin.\n",
      "full tr & ts: (257, 35), (30, 35)\n",
      "inter tr & ts: (123, 35), (30, 35)\n",
      "\n",
      "[5]: OV_six_fold_SNV_400.\n",
      "full tr & ts: (183, 402), (30, 402)\n",
      "inter tr & ts: (123, 402), (30, 402)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_1 = path+types[0]+\".csv\"\n",
    "file_2 = path+types[1]+\".csv\"\n",
    "file_3 = path+types[2]+\".csv\"\n",
    "file_4 = path+types[3]+\".csv\"\n",
    "file_5 = path+types[4]+\".csv\"\n",
    "file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "full_data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "full_data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "full_data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "full_data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "full_data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "full_data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "inter_data_1 = full_data_1.iloc[list(full_data_1.iloc[:,-1]!=6)]\n",
    "inter_data_2 = full_data_2.iloc[list(full_data_2.iloc[:,-1]!=6)]\n",
    "inter_data_3 = full_data_3.iloc[list(full_data_3.iloc[:,-1]!=6)]\n",
    "inter_data_4 = full_data_4.iloc[list(full_data_4.iloc[:,-1]!=6)]\n",
    "inter_data_5 = full_data_5.iloc[list(full_data_5.iloc[:,-1]!=6)]\n",
    "inter_data_6 = full_data_6.iloc[list(full_data_6.iloc[:,-1]!=6)]\n",
    "\n",
    "full_ds_list = [full_data_1, full_data_2, full_data_3, full_data_4, full_data_5, full_data_6]\n",
    "inter_ds_list = [inter_data_1, inter_data_2, inter_data_3, inter_data_4, inter_data_5, inter_data_6]\n",
    "\n",
    "# Split Train Test Data & Make full & inter dataset\n",
    "full_datasets=[]\n",
    "inter_datasets=[]\n",
    "\n",
    "for ts_i in range(1,6):\n",
    "    full_dataset = {\"types\":[], \"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "    inter_dataset = {\"types\":[], \"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "    print(\"################################## test index is \"+str(ts_i)+\" ##################################\")\n",
    "    for t in range(len(types)):    \n",
    "        full_tr_data, full_ts_data, full_tr_y_val, full_tr_x_val, full_ts_y_val, full_ts_x_val = data_split(raw_data = full_ds_list[t], index_col = -1, test_index = ts_i)\n",
    "        print(\"[\"+str(t)+\"]: \"+types[t]+\".\\nfull tr & ts: \"+str(full_tr_x_val.shape)+\", \"+str(full_ts_x_val.shape))\n",
    "        full_dataset['types'].append(types[t])\n",
    "        full_dataset['tr_data'].append(full_tr_data)\n",
    "        full_dataset['ts_data'].append(full_ts_data)\n",
    "        full_dataset['tr_x_val'].append(full_tr_x_val)\n",
    "        full_dataset['tr_y_val'].append(full_tr_y_val)\n",
    "        full_dataset['ts_x_val'].append(full_ts_x_val)\n",
    "        full_dataset['ts_y_val'].append(full_ts_y_val)\n",
    "        inter_tr_data, inter_ts_data, inter_tr_y_val, inter_tr_x_val, inter_ts_y_val, inter_ts_x_val = data_split(raw_data = inter_ds_list[t], index_col = -1, test_index = ts_i)\n",
    "        print(\"inter tr & ts: \"+str(inter_tr_x_val.shape)+\", \"+str(inter_ts_x_val.shape)+\"\\n\")\n",
    "        inter_dataset['types'].append(types[t])\n",
    "        inter_dataset['tr_data'].append(inter_tr_data)\n",
    "        inter_dataset['ts_data'].append(inter_ts_data)\n",
    "        inter_dataset['tr_x_val'].append(inter_tr_x_val)\n",
    "        inter_dataset['tr_y_val'].append(inter_tr_y_val)\n",
    "        inter_dataset['ts_x_val'].append(inter_ts_x_val)\n",
    "        inter_dataset['ts_y_val'].append(inter_ts_y_val)  \n",
    "    full_datasets.append(full_dataset)\n",
    "    inter_datasets.append(inter_dataset)\n",
    "    \n",
    "\n",
    "#  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each datasets variables (full_datasets & inter_datasets) have 5 splited data by each test index.\n",
      "-> 5\n",
      "There are 6 data types in one dataset that devided by one test index. Each data type consist of 7 list.\n",
      "-> 7\n",
      "-> dict_keys(['ts_x_val', 'types', 'tr_x_val', 'ts_y_val', 'tr_y_val', 'tr_data', 'ts_data'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-065013de76ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There are 6 data types in one dataset that devided by one test index. Each data type consist of 7 list.\\n-> \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-> \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Each list has real values to train, test, and others.\\n-> \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr_x_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# datasets description\n",
    "print(\"Each datasets variables (full_datasets & inter_datasets) have 5 splited data by each test index.\\n-> \"+str(len(full_datasets)))\n",
    "print(\"There are 6 data types in one dataset that devided by one test index. Each data type consist of 7 list.\\n-> \"+str(len(full_datasets[0])))\n",
    "print(\"-> \"+str(full_datasets[0].keys()))\n",
    "print(\"Each list has real values to train, test, and others.\\n-> \"+str(full_datasets[0]['tr_x_val'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ts_x_val', 'types', 'tr_x_val', 'ts_y_val', 'tr_y_val', 'tr_data', 'ts_data'])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_datasets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-246ea79cfcad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfull_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr_x_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "full_datasets[0]['tr_x_val'][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_i = 0\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OV_six_fold_Annotation3000_400\n",
      "(186, 400)\n"
     ]
    }
   ],
   "source": [
    "print(full_datasets[ts_i]['types'][t])\n",
    "print(full_datasets[ts_i]['tr_x_val'][t].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_newDiff_dataset = {\"tr_data\":[], \"ts_data\":[], \"tr_y_val\":[], \"tr_x_val\":[], \"ts_y_val\":[], \"ts_x_val\":[]}\n",
    "inter_tr_data, inter_ts_data, inter_tr_y_val, inter_tr_x_val, inter_ts_y_val, inter_ts_x_val = data_split(raw_data = inter_ds_list[3], index_col = -1, test_index = ts_i)\n",
    "inter_newDiff_dataset['tr_data']= inter_tr_data\n",
    "inter_newDiff_dataset['ts_data']= inter_ts_data\n",
    "inter_newDiff_dataset['tr_x_val']= inter_tr_x_val\n",
    "inter_newDiff_dataset['tr_y_val']= inter_tr_y_val\n",
    "inter_newDiff_dataset['ts_x_val']= inter_ts_x_val\n",
    "inter_newDiff_dataset['ts_y_val']= inter_ts_y_val    \n",
    "#inter_new_Diff_dataset = inter_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import separate models & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 2s 13ms/step\n",
      "31/31 [==============================] - 0s 223us/step\n",
      "\n",
      "model: m_0-1_49\n",
      "tr & ts for inter data: 1.0, 0.774193525314331\n",
      "\n",
      "122/122 [==============================] - 1s 12ms/step\n",
      "31/31 [==============================] - 0s 208us/step\n",
      "\n",
      "model: m_0-1_62\n",
      "tr & ts for inter data: 1.0, 0.7419354915618896\n",
      "\n",
      "122/122 [==============================] - 2s 13ms/step\n",
      "31/31 [==============================] - 0s 176us/step\n",
      "\n",
      "model: m_1-1_31\n",
      "tr & ts for inter data: 0.9918032728257726, 0.6451612710952759\n",
      "\n",
      "122/122 [==============================] - 2s 12ms/step\n",
      "31/31 [==============================] - 0s 159us/step\n",
      "\n",
      "model: m_1-1_43\n",
      "tr & ts for inter data: 1.0, 0.6451612710952759\n",
      "\n",
      "122/122 [==============================] - 2s 12ms/step\n",
      "31/31 [==============================] - 0s 155us/step\n",
      "\n",
      "model: m_1-1_6\n",
      "tr & ts for inter data: 0.959016387579871, 0.6129032373428345\n",
      "\n",
      "122/122 [==============================] - 2s 13ms/step\n",
      "31/31 [==============================] - 0s 158us/step\n",
      "\n",
      "model: m_2-1_57\n",
      "tr & ts for inter data: 1.0, 0.8387096524238586\n",
      "\n",
      "122/122 [==============================] - 2s 14ms/step\n",
      "31/31 [==============================] - 0s 207us/step\n",
      "\n",
      "model: m_2-1_89\n",
      "tr & ts for inter data: 1.0, 0.8387096524238586\n",
      "\n",
      "122/122 [==============================] - 2s 15ms/step\n",
      "31/31 [==============================] - 0s 220us/step\n",
      "\n",
      "model: m_3-1_15\n",
      "tr & ts for inter data: 0.9180327878623712, 0.8709677457809448\n",
      "\n",
      "122/122 [==============================] - 2s 16ms/step\n",
      "31/31 [==============================] - 0s 176us/step\n",
      "\n",
      "model: m_3-1_18\n",
      "tr & ts for inter data: 0.9836065573770492, 0.8709677457809448\n",
      "\n",
      "122/122 [==============================] - 2s 16ms/step\n",
      "31/31 [==============================] - 0s 159us/step\n",
      "\n",
      "model: m_4-1_101\n",
      "tr & ts for inter data: 0.9918032786885246, 0.774193525314331\n",
      "\n",
      "122/122 [==============================] - 2s 18ms/step\n",
      "31/31 [==============================] - 0s 146us/step\n",
      "\n",
      "model: m_4-1_40\n",
      "tr & ts for inter data: 0.9754098370426991, 0.7419354915618896\n",
      "\n",
      "122/122 [==============================] - 3s 21ms/step\n",
      "31/31 [==============================] - 0s 176us/step\n",
      "\n",
      "model: m_5-1_2\n",
      "tr & ts for inter data: 0.8360655747476171, 0.774193525314331\n",
      "\n",
      "122/122 [==============================] - 3s 26ms/step\n",
      "31/31 [==============================] - 0s 528us/step\n",
      "\n",
      "model: m_5-1_40\n",
      "tr & ts for inter data: 0.8032786836389636, 0.774193525314331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model load & evaluation. <model_n_l> is full-layer model, <model_n_l_new> is without-sigmoid-layer model.\n",
    "'''\n",
    "Each model's tr_accuracy can be differ to original model, but ts_accuracy should be same to original tested models.\n",
    "Because we using full-size data(about 200 patients data used Transcriptome, Clinical, SNV models.) for train each models.\n",
    "In contrast, in this code, we using ensemble-input data(intersected 153 patients).\n",
    "For-training-patients may be different in ensemble data and whole size data, but for-test-patients are the same.\n",
    "'''\n",
    "\n",
    "model_list = []\n",
    "model_output_list = {\"tr_accuracy\":[], \"tr_sensitivity\":[], \"tr_specificity\":[], \"tr_predictions\":[],\n",
    "                 \"labeled_tr_predictions\":[], \"tr_predictions_flat\":[], \"roc_auc_tr\":[], \n",
    "                 \"ts_accuracy\":[], \"ts_sensitivity\":[], \"ts_specificity\":[], \"ts_predictions\":[],\n",
    "                 \"labeled_ts_predictions\":[], \"ts_predictions_flat\":[], \"roc_auc_ts\":[], \n",
    "                 \"roc_auc_total\":[], \"tr_result\":[], \"ts_result\":[]}\n",
    "tr_predictions = []\n",
    "ts_predictions = []\n",
    "\n",
    "for m in range(len(model_names)):\n",
    "    \n",
    "    model_l = load_model(input_model_path+model_names[m]+\".h5\")\n",
    "    model_list.append(model_l)\n",
    "    output_list = output_list = model_performance(\n",
    "        information = False, using_model=model_l,Input_Prediction_Passively = False, \n",
    "        tr_x_val=inter_dataset['tr_x_val'][m], tr_y_val=inter_dataset['tr_y_val'][m], ts_x_val=inter_dataset['ts_x_val'][m], ts_y_val=inter_dataset['ts_y_val'][m],\n",
    "        output_list=[\"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                     \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                     \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                     \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                     \"roc_auc_total\"])\n",
    "    m_tr_accuracy, m_tr_sensitivity, m_tr_specificity, m_tr_predictions, m_labeled_tr_predictions, m_tr_predictions_flat, m_roc_auc_tr, m_ts_accuracy, m_ts_sensitivity, m_ts_specificity, m_ts_predictions,m_labeled_ts_predictions, m_ts_predictions_flat, m_roc_auc_ts, m_roc_auc_total = output_list\n",
    "    print(\"\\nmodel: \"+model_names[m])\n",
    "    print(\"tr & ts for inter data: \"+str(m_tr_accuracy)+\", \"+str(m_ts_accuracy)+\"\\n\")\n",
    "    \n",
    "    model_l_new = Model(inputs = model_l.input, outputs=model_l.get_layer(model_l.layers[-2].name).output)\n",
    "    m_tr_result = model_l_new.predict([inter_dataset['tr_x_val'][m]])\n",
    "    m_ts_result = model_l_new.predict([inter_dataset['ts_x_val'][m]])\n",
    "    \n",
    "    model_output_list[\"tr_accuracy\"].append(m_tr_accuracy)\n",
    "    model_output_list[\"tr_sensitivity\"].append(m_tr_sensitivity)\n",
    "    model_output_list[\"tr_specificity\"].append(m_tr_specificity)\n",
    "    model_output_list[\"ts_accuracy\"].append(m_ts_accuracy)\n",
    "    model_output_list[\"ts_sensitivity\"].append(m_ts_sensitivity)\n",
    "    model_output_list[\"ts_specificity\"].append(m_ts_specificity)\n",
    "    model_output_list[\"tr_result\"].append(m_tr_result)\n",
    "    \n",
    "    model_output_list[\"tr_predictions\"].append(m_tr_predictions)\n",
    "    model_output_list[\"labeled_tr_predictions\"].append(m_labeled_tr_predictions)\n",
    "    model_output_list[\"tr_predictions_flat\"].append(m_tr_predictions_flat)\n",
    "    model_output_list[\"roc_auc_tr\"].append(m_roc_auc_tr)\n",
    "    model_output_list[\"ts_predictions\"].append(m_ts_predictions)\n",
    "    model_output_list[\"labeled_ts_predictions\"].append(m_labeled_ts_predictions)\n",
    "    model_output_list[\"ts_predictions_flat\"].append(m_ts_predictions_flat)\n",
    "    model_output_list[\"roc_auc_ts\"].append(m_roc_auc_ts)\n",
    "    model_output_list[\"ts_result\"].append(m_ts_result)\n",
    "    \n",
    "    model_output_list[\"roc_auc_total\"].append(m_roc_auc_total)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seperate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### [1] m_0-1_49 ####\n",
      "types: OV_six_fold_Annotation3000_400\n",
      "tr: 1.0, ts: 0.774193525314331\n",
      "\n",
      "#### [2] m_0-1_62 ####\n",
      "types: OV_six_fold_Annotation3000_400\n",
      "tr: 1.0, ts: 0.7419354915618896\n",
      "\n",
      "#### [3] m_1-1_31 ####\n",
      "types: OV_six_fold_CV_400\n",
      "tr: 0.9918032728257726, ts: 0.6451612710952759\n",
      "\n",
      "#### [4] m_1-1_43 ####\n",
      "types: OV_six_fold_CV_400\n",
      "tr: 1.0, ts: 0.6451612710952759\n",
      "\n",
      "#### [5] m_1-1_6 ####\n",
      "types: OV_six_fold_CV_400\n",
      "tr: 0.959016387579871, ts: 0.6129032373428345\n",
      "\n",
      "#### [6] m_2-1_57 ####\n",
      "types: OV_six_fold_Var_400\n",
      "tr: 1.0, ts: 0.8387096524238586\n",
      "\n",
      "#### [7] m_2-1_89 ####\n",
      "types: OV_six_fold_Var_400\n",
      "tr: 1.0, ts: 0.8387096524238586\n",
      "\n",
      "#### [8] m_3-1_15 ####\n",
      "types: OV_six_fold_new_Diff_400\n",
      "tr: 0.9180327878623712, ts: 0.8709677457809448\n",
      "\n",
      "#### [9] m_3-1_18 ####\n",
      "types: OV_six_fold_new_Diff_400\n",
      "tr: 0.9836065573770492, ts: 0.8709677457809448\n",
      "\n",
      "#### [10] m_4-1_101 ####\n",
      "types: OV_six_fold_Clin\n",
      "tr: 0.9918032786885246, ts: 0.774193525314331\n",
      "\n",
      "#### [11] m_4-1_40 ####\n",
      "types: OV_six_fold_Clin\n",
      "tr: 0.9754098370426991, ts: 0.7419354915618896\n",
      "\n",
      "#### [12] m_5-1_2 ####\n",
      "types: OV_six_fold_SNV_400\n",
      "tr: 0.8360655747476171, ts: 0.774193525314331\n",
      "\n",
      "#### [13] m_5-1_40 ####\n",
      "types: OV_six_fold_SNV_400\n",
      "tr: 0.8032786836389636, ts: 0.774193525314331\n",
      "\n",
      "input numbers for selection(1 ~ 13. q for quit.: 1\n",
      "input numbers for selection(1 ~ 13. q for quit.: 2\n",
      "input numbers for selection(1 ~ 13. q for quit.: 3\n",
      "input numbers for selection(1 ~ 13. q for quit.: 4\n",
      "input numbers for selection(1 ~ 13. q for quit.: 5\n",
      "input numbers for selection(1 ~ 13. q for quit.: 6\n",
      "input numbers for selection(1 ~ 13. q for quit.: 7\n",
      "input numbers for selection(1 ~ 13. q for quit.: q\n"
     ]
    }
   ],
   "source": [
    "for m in range(len(model_names)):\n",
    "    print(\"#### [\"+str(m+1)+\"] \"+model_names[m]+\" ####\")\n",
    "    print(\"types: \"+types[model_index[m]])\n",
    "    print(\"tr: \"+str(model_output_list[\"tr_accuracy\"][m])+\", ts: \"+str(model_output_list[\"ts_accuracy\"][m])+\"\\n\")\n",
    "\n",
    "select = []\n",
    "while 1:\n",
    "    ch = input(\"input numbers for selection(1 ~ \"+str(len(model_names))+\". q for quit.: \")\n",
    "    if(ch == 'q'):\n",
    "        break\n",
    "    else:\n",
    "        select.append(int(ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0319 - acc: 0.9940\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.0147 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "167/167 [==============================] - 0s 977us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "167/167 [==============================] - 0s 888us/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 915us/step - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 933us/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 928us/step - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 900us/step - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 854us/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 772us/step - loss: 0.0258 - acc: 0.9940\n"
     ]
    }
   ],
   "source": [
    "model1 = load_model(input_model_path+model_names[0]+\".h5\")\n",
    "tr_x=full_dataset['tr_x_val'][0]\n",
    "tr_y=full_dataset['tr_y_val'][0]\n",
    "ts_x=full_dataset['ts_x_val'][0]\n",
    "ts_y=inter_dataset['ts_y_val'][0]\n",
    "\n",
    "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=tr_x,test=ts_x,y=tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186,)\n",
      "(341, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_pred1.shape)\n",
    "print(test_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186,)\n",
      "(31,)\n"
     ]
    }
   ],
   "source": [
    "print(tr_y.shape)\n",
    "print(ts_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.84154972e-01,\n",
       "       2.12075129e-01, 4.37520474e-01, 2.81035215e-01, 1.80363525e-02,\n",
       "       5.06485207e-03, 2.21067175e-01, 9.67483222e-01, 3.53960134e-03,\n",
       "       7.11818099e-01, 3.52693931e-03, 2.41143152e-01, 6.77669048e-01,\n",
       "       2.55317688e-01, 5.76208472e-01, 6.41319335e-01, 9.93684471e-01,\n",
       "       2.76848599e-02, 6.85885176e-02, 7.77335048e-01, 3.57893878e-04,\n",
       "       9.32377756e-01, 9.47473288e-01, 8.20726693e-01, 4.40062024e-02,\n",
       "       2.78738822e-04, 5.54257989e-01, 4.16352689e-01, 2.61900853e-03,\n",
       "       1.22558720e-01, 9.74911004e-02, 1.54444620e-01, 1.85881674e-01,\n",
       "       3.55816960e-01, 2.44673833e-01, 1.92207657e-02, 4.50958731e-03,\n",
       "       2.46608436e-01, 9.53690052e-01, 2.76870537e-03, 7.10921466e-01,\n",
       "       3.40628484e-03, 2.39724293e-01, 6.59360647e-01, 2.61648446e-01,\n",
       "       5.73739886e-01, 6.54111564e-01, 9.94122326e-01, 2.35952139e-02,\n",
       "       8.02102089e-02, 7.71217167e-01, 3.47498484e-04, 9.30279851e-01,\n",
       "       9.48100686e-01, 8.17340851e-01, 3.12467031e-02, 2.47198477e-04,\n",
       "       5.63462496e-01, 3.43849868e-01, 2.28737900e-03, 1.18027367e-01,\n",
       "       7.52786770e-02, 1.29906118e-01, 1.74452201e-01, 2.76997298e-01,\n",
       "       2.28344947e-01, 1.91636253e-02, 3.58289923e-03, 2.61709183e-01,\n",
       "       9.35351789e-01, 2.29162909e-03, 7.14110672e-01, 3.04612285e-03,\n",
       "       2.34363511e-01, 6.04739070e-01, 2.31598124e-01, 5.61969578e-01,\n",
       "       6.45616412e-01, 9.94377911e-01, 2.00566333e-02, 7.99258277e-02,\n",
       "       7.36042261e-01, 3.43384134e-04, 9.29438770e-01, 9.44940388e-01,\n",
       "       8.14028382e-01, 2.29421873e-02, 1.99975504e-04, 5.54496646e-01,\n",
       "       2.74868011e-01, 2.08740612e-03, 1.16422966e-01, 6.46103770e-02,\n",
       "       1.19321704e-01, 1.75196722e-01, 2.37643465e-01, 2.34138131e-01,\n",
       "       2.16445513e-02, 2.91386363e-03, 3.02176744e-01, 9.24399436e-01,\n",
       "       2.10854365e-03, 7.23755658e-01, 2.79722014e-03, 2.37584203e-01,\n",
       "       5.54938853e-01, 2.08201692e-01, 5.61701298e-01, 6.50575697e-01,\n",
       "       9.94830906e-01, 1.95028931e-02, 8.62922221e-02, 7.01357186e-01,\n",
       "       3.67699395e-04, 9.28578079e-01, 9.43163693e-01, 8.18547726e-01,\n",
       "       2.06960589e-02, 1.83189200e-04, 5.49733281e-01, 2.47011021e-01,\n",
       "       2.03701714e-03, 1.19273089e-01, 6.23053275e-02, 1.25093624e-01,\n",
       "       1.74699426e-01, 2.14198872e-01, 2.34591603e-01, 2.16614343e-02,\n",
       "       2.49188254e-03, 2.97433823e-01, 9.17011857e-01, 2.05249991e-03,\n",
       "       7.27891922e-01, 2.65742862e-03, 2.41402179e-01, 5.05995750e-01,\n",
       "       1.70513302e-01, 5.59814394e-01, 6.46412432e-01, 9.95226383e-01,\n",
       "       1.90730430e-02, 8.30964372e-02, 6.74401224e-01, 3.82240338e-04,\n",
       "       9.30197060e-01, 9.39819276e-01, 8.21575403e-01, 1.95489805e-02,\n",
       "       1.68030601e-04, 5.41466475e-01, 2.26155967e-01, 2.05060584e-03,\n",
       "       1.19398020e-01, 5.88995107e-02, 1.23509437e-01, 1.75104439e-01,\n",
       "       1.92532703e-01, 2.31670007e-01, 2.01697592e-02, 2.20569945e-03,\n",
       "       2.78830379e-01, 9.10612881e-01, 2.06239312e-03, 7.28318155e-01,\n",
       "       2.56889034e-03, 2.50178218e-01, 4.37190890e-01, 1.39947519e-01,\n",
       "       5.50929606e-01, 6.37993336e-01, 9.95384276e-01, 1.91895366e-02,\n",
       "       7.26757050e-02, 6.47125840e-01, 3.99450102e-04, 9.32265401e-01,\n",
       "       9.34840679e-01, 8.18998039e-01, 1.79096181e-02, 1.58855997e-04,\n",
       "       5.24346471e-01, 2.10113034e-01, 2.12802179e-03, 1.18181877e-01,\n",
       "       5.73737398e-02, 1.23062246e-01, 1.72308832e-01, 1.81025788e-01,\n",
       "       2.25601077e-01, 1.88980717e-02, 1.90201774e-03, 2.60063022e-01,\n",
       "       9.04400229e-01, 2.03824672e-03, 7.20813036e-01, 2.32644845e-03,\n",
       "       2.45656803e-01, 3.79283547e-01, 1.15885876e-01, 5.40678799e-01,\n",
       "       6.17997050e-01, 9.95332301e-01, 1.83640141e-02, 6.38138726e-02,\n",
       "       6.17295146e-01, 3.95783776e-04, 9.33590353e-01, 9.29469585e-01,\n",
       "       8.15059721e-01, 1.66857280e-02, 1.42311983e-04, 5.06941617e-01,\n",
       "       1.94339216e-01, 2.13929825e-03, 1.14467189e-01, 5.59821315e-02,\n",
       "       1.26195490e-01, 1.67880550e-01, 1.70462713e-01, 2.24143684e-01,\n",
       "       1.73936915e-02, 1.63079880e-03, 2.41356984e-01, 8.97660255e-01,\n",
       "       2.01062160e-03, 7.13384211e-01, 2.10310169e-03, 2.35459968e-01,\n",
       "       3.29583049e-01, 9.76203233e-02, 5.29663265e-01, 5.95403433e-01,\n",
       "       9.95090246e-01, 1.77485738e-02, 5.71712852e-02, 5.88301182e-01,\n",
       "       3.76970886e-04, 9.32848334e-01, 9.24352288e-01, 8.09583843e-01,\n",
       "       1.58484634e-02, 1.20036057e-04, 4.93869454e-01, 1.78568557e-01,\n",
       "       2.13855109e-03, 1.11239605e-01, 5.30959293e-02, 1.28909767e-01,\n",
       "       1.65512696e-01, 1.63192019e-01, 2.21414477e-01, 1.59799606e-02,\n",
       "       1.48614182e-03, 2.26540133e-01, 8.90561461e-01, 2.01833947e-03,\n",
       "       7.04091549e-01, 1.95238250e-03, 2.28082150e-01, 2.94057846e-01,\n",
       "       8.48971382e-02, 5.20257413e-01, 5.71104407e-01, 9.94923413e-01,\n",
       "       1.69004481e-02, 5.19126989e-02, 5.67174375e-01, 3.60937556e-04,\n",
       "       9.32991445e-01, 9.19619977e-01, 8.04700792e-01, 1.51895117e-02,\n",
       "       1.05004663e-04, 4.82129067e-01, 1.64896712e-01, 2.12443131e-03,\n",
       "       1.08802810e-01, 5.01893163e-02, 1.28313869e-01, 1.64166108e-01,\n",
       "       1.58853650e-01, 2.18023598e-01, 1.52338715e-02, 1.32751779e-03,\n",
       "       2.15412557e-01, 8.82940650e-01, 1.97913731e-03, 6.94561422e-01,\n",
       "       1.78775378e-03, 2.19529763e-01, 2.67556012e-01, 7.61622488e-02,\n",
       "       5.11509836e-01, 5.50719917e-01, 9.94819105e-01, 1.60714984e-02,\n",
       "       4.91039045e-02, 5.45820415e-01, 3.47336871e-04, 9.33213651e-01,\n",
       "       9.16140139e-01, 8.00689518e-01, 1.46209374e-02, 9.34538475e-05,\n",
       "       4.71561253e-01, 1.56517565e-01, 2.06471654e-03, 1.05352208e-01,\n",
       "       4.79429401e-02])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_models_select = ensemble_coverage(model_list,inter_dataset[\"tr_x_val\"],inter_dataset[\"tr_y_val\"][0])\n",
    "print(\"model numbers: \"+str(len(e_models_select)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DNN-Combiner Ensmeble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Input listup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tr_predictions_select = []\n",
    "m_ts_predictions_select = []   \n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_tr_predictions_select.append(model_output_list[\"tr_predictions\"][select[i]-1])\n",
    "    m_ts_predictions_select.append(model_output_list[\"ts_predictions\"][select[i]-1])\n",
    "    #print(m_tr_predictions[select[i]-1].shape)\n",
    "    \n",
    "em_tr_x_val = np.concatenate(m_tr_predictions_select, axis=1)\n",
    "em_ts_x_val = np.concatenate(m_ts_predictions_select, axis=1)\n",
    "\n",
    "tr_y_val = inter_dataset[\"tr_y_val\"][0]\n",
    "ts_y_val = inter_dataset[\"ts_y_val\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em_tr_x_val.shape)\n",
    "print(em_ts_x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"################################## DNN em ##################################\")\n",
    "print(\"select: \"+str(select))\n",
    "for select_i in select:\n",
    "    print(\"\\n\"+types[model_index[select_i-1]])\n",
    "    print(model_names[select_i-1])\n",
    "\n",
    "    \n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "# 1) parameter setting\n",
    "em_adam = optimizers.Adam(lr=0.05)                                   \n",
    "em_input_drop_out = 0.3\n",
    "em_drop_out = 0\n",
    "em_batch_size = 5\n",
    "em_BN = True                           \n",
    "\n",
    "em_layers = [10]\n",
    "em_tr_loss_best = 100 # for saving best loss value \n",
    "em_best_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "em_input = Input(shape=(len(select),))\n",
    "em_dp = Dropout(em_input_drop_out)(em_input)\n",
    "for l in em_layers:\n",
    "    if em_BN == True:\n",
    "        em_m = Dense(l)(em_dp)\n",
    "        em_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(em_m)\n",
    "        em_dp = Activation(\"relu\")(em_bn)\n",
    "    else:\n",
    "        em_m = Dense(l,activation='relu')(em_dp)\n",
    "        em_dp = Dropout(drop_out_m)(em_m)\n",
    "\n",
    "em_final = em_dp\n",
    "em_output = Dense(1, activation=\"sigmoid\")(em_final)\n",
    "em_model = Model(inputs=em_input,outputs=em_output)\n",
    "em_model.compile(optimizer=em_adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    em_model.fit(em_tr_x_val, tr_y_val, batch_size=em_batch_size, nb_epoch=1, verbose = 0)\n",
    "    em_tr_loss=em_model.evaluate( em_tr_x_val, tr_y_val)[0]\n",
    "    if em_tr_loss < em_tr_loss_best: # new best model. count reset.\n",
    "        em_tr_loss_best = em_tr_loss\n",
    "        count=0\n",
    "        em_best_model = em_model\n",
    "    if count>10: # no increase three time. stop.\n",
    "        em_model = em_best_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "print(\"Model em\" +\"-\"+str(ts_i)+\" trained.\")\n",
    "\n",
    "# 4) save model\n",
    "em_model.save(save_model_path+\"/m_em-\"+str(ts_i)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _DNN Combiner_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_output_list = model_performance(\n",
    "    information = False, using_model=em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "em_tr_loss, em_tr_accuracy, em_tr_sensitivity, em_tr_specificity, em_tr_predictions, em_labeled_tr_predictions, em_tr_predictions_flat, em_roc_auc_tr, em_ts_loss, em_ts_accuracy, em_ts_sensitivity, em_ts_specificity, em_ts_predictions,em_labeled_ts_predictions, em_ts_predictions_flat, em_roc_auc_ts, em_roc_auc_total = em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", em_roc_auc_total)\n",
    "print(\"Train AUC: \", em_roc_auc_tr)\n",
    "print(\"Test AUC: \", em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(em_tr_sensitivity)+\", \"+str(em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(em_ts_sensitivity)+\", \"+str(em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(em_tr_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_em.to_csv(save_prediction_path+\"m_em-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(em_ts_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_em.to_csv(save_prediction_path+\"m_em-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mean Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _mean_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_em_tr_predictions=sum(m_tr_predictions_select)/len(select)\n",
    "mean_em_ts_predictions=sum(m_ts_predictions_select)/len(select)\n",
    "\n",
    "mean_em_output_list = model_performance(\n",
    "    information = False, using_model=None,Input_Prediction_Passively = True, \n",
    "    tr_predictions=mean_em_tr_predictions, ts_predictions=mean_em_ts_predictions, \n",
    "    tr_x_val=em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_sensitivity\", \"tr_specificity\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_sensitivity\", \"ts_specificity\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "mean_em_tr_sensitivity, mean_em_tr_specificity,  mean_em_labeled_tr_predictions, mean_em_tr_predictions_flat, mean_em_roc_auc_tr, mean_em_ts_sensitivity, mean_em_ts_specificity, mean_em_labeled_ts_predictions, mean_em_ts_predictions_flat, mean_em_roc_auc_ts, mean_em_roc_auc_total = mean_em_output_list\n",
    "\n",
    "mean_em_tr_accuracy = sum(mean_em_labeled_tr_predictions==tr_y_val.values)/len(tr_y_val)\n",
    "mean_em_ts_accuracy = sum(mean_em_labeled_ts_predictions==ts_y_val.values)/len(ts_y_val)\n",
    "\n",
    "print(\"Overall AUC: \", mean_em_roc_auc_total)\n",
    "print(\"Train AUC: \", mean_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", mean_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(mean_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mean_em_tr_sensitivity)+\", \"+str(mean_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(mean_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mean_em_ts_sensitivity)+\", \"+str(mean_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_mean = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(mean_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_mean.to_csv(save_prediction_path+\"m_mean-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_mean = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(mean_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_mean.to_csv(save_prediction_path+\"m_mean-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Transferred Ensemble Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making new input data for t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tr_result_select = []\n",
    "m_ts_result_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_tr_result_select.append(model_output_list[\"tr_result\"][select[i]-1])\n",
    "    m_ts_result_select.append(model_output_list[\"ts_result\"][select[i]-1])\n",
    "\n",
    "t_em_tr_x_val = np.concatenate(m_tr_result_select, axis=1)\n",
    "t_em_ts_x_val = np.concatenate(m_ts_result_select, axis=1)\n",
    "print(\"\\n############################################### t-em x val merged. ###############################################\\n\")\n",
    "print(t_em_tr_x_val.shape)\n",
    "print(t_em_ts_x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling t-ensemble  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"################################## Transferred em ##################################\")\n",
    "print(\"select: \"+str(select))\n",
    "for select_i in select:\n",
    "    print(\"\\n\"+types[model_index[select_i-1]])\n",
    "    print(model_names[select_i-1])\n",
    "\n",
    "    \n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "# 1) parameter setting\n",
    "t_em_adam = optimizers.Adam(lr=0.05)                                   \n",
    "t_em_input_drop_out = 0.3\n",
    "t_em_drop_out = 0\n",
    "t_em_batch_size = 5\n",
    "t_em_BN = True                           \n",
    "\n",
    "t_em_layers = [100]\n",
    "t_em_tr_loss_best = 100 # for saving best loss value \n",
    "t_em_best_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "t_em_input = Input(shape=(t_em_ts_x_val.shape[1],))\n",
    "t_em_dp = Dropout(t_em_input_drop_out)(t_em_input)\n",
    "for l in t_em_layers:\n",
    "    if t_em_BN == True:\n",
    "        t_em_m = Dense(l)(t_em_dp)\n",
    "        t_em_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(t_em_m)\n",
    "        t_em_dp = Activation(\"relu\")(t_em_bn)\n",
    "    else:\n",
    "        t_em_m = Dense(l,activation='relu')(t_em_dp)\n",
    "        t_em_dp = Dropout(drop_out_m)(t_em_m)\n",
    "\n",
    "t_em_final = t_em_dp\n",
    "t_em_output = Dense(1, activation=\"sigmoid\")(t_em_final)\n",
    "t_em_model = Model(inputs=t_em_input,outputs=t_em_output)\n",
    "t_em_model.compile(optimizer=t_em_adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    t_em_model.fit(t_em_tr_x_val, tr_y_val, batch_size=t_em_batch_size, nb_epoch=1, verbose = 0)\n",
    "    t_em_tr_loss=t_em_model.evaluate( t_em_tr_x_val, tr_y_val)[0]\n",
    "    if t_em_tr_loss < t_em_tr_loss_best: # new best model. count reset.\n",
    "        t_em_tr_loss_best = t_em_tr_loss\n",
    "        count=0\n",
    "        t_em_best_model = t_em_model\n",
    "    if count>10: # no increase three time. stop.\n",
    "        t_em_model = t_em_best_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "        \n",
    "print(\"Model t-em\" +\"-\"+str(ts_i)+\" trained.\")\n",
    "\n",
    "# 4) save model\n",
    "em_model.save(save_model_path+\"/m_t-em-\"+str(ts_i)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_em_output_list = model_performance(\n",
    "    information = False, using_model=t_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=t_em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=t_em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "t_em_tr_loss, t_em_tr_accuracy, t_em_tr_sensitivity, t_em_tr_specificity, t_em_tr_predictions, t_em_labeled_tr_predictions, t_em_tr_predictions_flat, t_em_roc_auc_tr, t_em_ts_loss, t_em_ts_accuracy, t_em_ts_sensitivity, t_em_ts_specificity, t_em_ts_predictions,t_em_labeled_ts_predictions, t_em_ts_predictions_flat, t_em_roc_auc_ts, t_em_roc_auc_total = t_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", t_em_roc_auc_total)\n",
    "print(\"Train AUC: \", t_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", t_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(t_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(t_em_tr_sensitivity)+\", \"+str(t_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(t_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(t_em_ts_sensitivity)+\", \"+str(t_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_t_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(t_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_t_em.to_csv(save_prediction_path+\"m_t-em-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_t_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(t_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_t_em.to_csv(save_prediction_path+\"m_t-em-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferred Ensemble(Modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mo_transferred ensemble input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset : raw data + prediction results\n",
    "mo_em_tr_x_val = np.concatenate([inter_newDiff_dataset[\"tr_x_val\"], em_tr_x_val], axis = 1)\n",
    "mo_em_ts_x_val = np.concatenate([inter_newDiff_dataset[\"ts_x_val\"], em_ts_x_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mo_em_tr_x_val.shape)\n",
    "print(mo_em_ts_x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_em_matrix = np.concatenate([full_em_ts_x_val, full_em_tr_x_val], axis = 0)\n",
    "#df_full_dataset = pd.DataFrame(full_em_matrix)\n",
    "#df_full_dataset.to_csv(index=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_full_dataset = pd.DataFrame(full_em_matrix)\n",
    "df_full_dataset.to_csv(\"C:/test/merge_newDiff_400_with_predictions.csv\",index=False)\n",
    "\n",
    "'''\n",
    "#df_full_dataset.loc[df_full_dataset.shape[1]] = patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"modified t-ensemble model\")\n",
    "\n",
    "# 1) parameter setting\n",
    "mo_em_adam = optimizers.Adam(lr=0.05)                                   \n",
    "mo_em_input_drop_out = 0.3\n",
    "mo_em_drop_out = 0\n",
    "mo_em_batch_size = 5\n",
    "mo_em_BN = True                           \n",
    "\n",
    "mo_em_layers = [100]\n",
    "mo_em_tr_loss_best = 100 # for saving best loss value \n",
    "mo_em_best_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "mo_em_input = Input(shape=(mo_em_ts_x_val.shape[1],))\n",
    "mo_em_dp = Dropout(mo_em_input_drop_out)(mo_em_input)\n",
    "for l in mo_em_layers:\n",
    "    if mo_em_BN == True:\n",
    "        mo_em_m = Dense(l)(mo_em_dp)\n",
    "        mo_em_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(mo_em_m)\n",
    "        mo_em_dp = Activation(\"relu\")(mo_em_bn)\n",
    "    else:\n",
    "        mo_em_m = Dense(l,activation='relu')(mo_em_dp)\n",
    "        mo_em_dp = Dropout(drop_out_m)(mo_em_m)\n",
    "\n",
    "mo_em_final = mo_em_dp\n",
    "mo_em_output = Dense(1, activation=\"sigmoid\")(mo_em_final)\n",
    "mo_em_model = Model(inputs=mo_em_input,outputs=mo_em_output)\n",
    "mo_em_model.compile(optimizer=mo_em_adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    mo_em_model.fit(mo_em_tr_x_val, tr_y_val, batch_size=mo_em_batch_size, nb_epoch=1, verbose = 0)\n",
    "    mo_em_tr_loss=mo_em_model.evaluate( mo_em_tr_x_val, tr_y_val)[0]\n",
    "    if mo_em_tr_loss < mo_em_tr_loss_best: # new best model. count reset.\n",
    "        mo_em_tr_loss_best = mo_em_tr_loss\n",
    "        count=0\n",
    "        mo_em_best_model = mo_em_model\n",
    "    if count>10: # no increase three time. stop.\n",
    "        mo_em_model = mo_em_best_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "        \n",
    "print(\"Model mo-em\" +\"-\"+str(ts_i)+\" trained.\")\n",
    "\n",
    "# 4) save model\n",
    "em_model.save(save_model_path+\"/m_mo-em-\"+str(ts_i)+\".h5\")\n",
    "\n",
    "# 5) evaluate model\n",
    "mo_em_output_list = model_performance(\n",
    "    information = False, using_model=mo_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=mo_em_tr_x_val, tr_y_val=tr_y_val, ts_x_val=mo_em_ts_x_val, ts_y_val=ts_y_val,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "mo_em_tr_loss, mo_em_tr_accuracy, mo_em_tr_sensitivity, mo_em_tr_specificity, mo_em_tr_predictions, mo_em_labeled_tr_predictions, mo_em_tr_predictions_flat, mo_em_roc_auc_tr, mo_em_ts_loss, mo_em_ts_accuracy, mo_em_ts_sensitivity, mo_em_ts_specificity, mo_em_ts_predictions,mo_em_labeled_ts_predictions, mo_em_ts_predictions_flat, mo_em_roc_auc_ts, mo_em_roc_auc_total = mo_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", mo_em_roc_auc_total)\n",
    "print(\"Train AUC: \", mo_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", mo_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(mo_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mo_em_tr_sensitivity)+\", \"+str(mo_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(mo_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mo_em_ts_sensitivity)+\", \"+str(mo_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_mo_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"tr_data\"][0].index), \"hypothesis 1\": list(mo_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(mo_em_labeled_tr_predictions), \"Platinum_Status\":list(tr_y_val)})\n",
    "tr_df_mo_em.to_csv(save_prediction_path+\"m_mo-em-\"+str(ts_i)+\"_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_mo_em = pd.DataFrame(data={\"patient\":list(inter_dataset[\"ts_data\"][0].index), \"hypothesis 1\": list(mo_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(mo_em_labeled_ts_predictions), \"Platinum_Status\":list(ts_y_val)})\n",
    "ts_df_mo_em.to_csv(save_prediction_path+\"m_mo-em-\"+str(ts_i)+\"_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model_index = str(model_index[select[0]-1]) \n",
    "select_model_names = str(model_names[select[0]-1]) \n",
    "for select_i in select[1:]:\n",
    "    print(types[model_index[select_i-1]])\n",
    "    print(model_names[select_i-1]+\"\\n\")\n",
    "    select_model_index = select_model_index+\" & \"+str(model_index[select_i-1])\n",
    "    select_model_names = select_model_names+\" & \"+str(model_names[select_i-1]) \n",
    "\n",
    "print(select_model_index)\n",
    "print(select_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_accuracy_list = [em_tr_accuracy, mean_em_tr_accuracy, t_em_tr_accuracy, mo_em_tr_accuracy]\n",
    "ts_accuracy_list = [em_ts_accuracy, mean_em_ts_accuracy, t_em_ts_accuracy, mo_em_ts_accuracy]\n",
    "tr_sensitivity_list = [em_tr_sensitivity, mean_em_tr_sensitivity, t_em_tr_sensitivity, mo_em_tr_sensitivity]\n",
    "ts_sensitivity_list = [em_ts_sensitivity, mean_em_ts_sensitivity, t_em_ts_sensitivity, mo_em_ts_sensitivity]\n",
    "tr_specificity_list = [em_tr_specificity, mean_em_tr_specificity, t_em_tr_specificity, mo_em_tr_specificity]\n",
    "ts_specificity_list = [em_ts_specificity, mean_em_ts_specificity, t_em_ts_specificity, mo_em_ts_specificity]\n",
    "tr_roc_list = [em_roc_auc_tr, mean_em_roc_auc_tr, t_em_roc_auc_tr, mo_em_roc_auc_tr]\n",
    "ts_roc_list = [em_roc_auc_ts, mean_em_roc_auc_ts, t_em_roc_auc_ts, mo_em_roc_auc_ts]\n",
    "total_roc_list = [em_roc_auc_total, mean_em_roc_auc_total, t_em_roc_auc_total, mo_em_roc_auc_total]\n",
    "\n",
    "for type_index in range(4):\n",
    "    em_output_list[\"em_type\"].append(em_type[type_index])\n",
    "    em_output_list[\"test_index\"].append(ts_i)\n",
    "    em_output_list[\"ensemble_comb\"].append(select_model_index)\n",
    "    em_output_list[\"ensemble_names\"].append(select_model_names)\n",
    "    em_output_list[\"tr_accuracy\"].append(tr_accuracy_list[type_index] )\n",
    "    em_output_list[\"ts_accuracy\"].append(ts_accuracy_list[type_index] )\n",
    "    em_output_list[\"tr_sensitivity\"].append(tr_sensitivity_list[type_index] )\n",
    "    em_output_list[\"ts_sensitivity\"].append(ts_sensitivity_list[type_index] )\n",
    "    em_output_list[\"tr_specificity\"].append(tr_specificity_list[type_index] )\n",
    "    em_output_list[\"ts_specificity\"].append(ts_specificity_list[type_index] )\n",
    "    em_output_list[\"roc_auc_tr\"].append(tr_roc_list[type_index] )\n",
    "    em_output_list[\"roc_auc_ts\"].append(ts_roc_list[type_index] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_index in range(4):\n",
    "    df_sum = pd.DataFrame(data=em_output_list)\n",
    "    df_sum.to_csv(save_result_path+em_type[type_index]+\"_result_\"+str(cycle_index)+\".csv\", index=False, header=True,\n",
    "              columns = [\"ensemble_comb\", \"ensemble_names\",\"test_index\",\n",
    "                         \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \n",
    "                         \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\",\n",
    "                         \"roc_auc_total\", \"roc_auc_tr\", \"roc_auc_ts\"])\n",
    "cycle_index = cycle_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(em_output_list['test_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
