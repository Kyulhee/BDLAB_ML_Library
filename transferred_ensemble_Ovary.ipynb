{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] file_name:  OV_Annotation3000_1000_idx12 \n",
      "sample : 217  \n",
      "features : 1000\n",
      "[2] file_name:  OV_CV_1000_idx12 \n",
      "sample : 217  \n",
      "features : 1000\n",
      "[3] file_name:  OV_Var_1000_idx12 \n",
      "sample : 217  \n",
      "features : 1000\n",
      "[4] file_name:  OV_new_Diff_1000_idx12 \n",
      "sample : 217  \n",
      "features : 1000\n",
      "[5] file_name:  inter_OV_Clin \n",
      "sample : 153  \n",
      "features : 35\n",
      "[6] file_name:  OV_SNV_400_idx12_sen_boost \n",
      "sample : 275  \n",
      "features : 400\n"
     ]
    }
   ],
   "source": [
    "path = \"C://test/TC_subsamples_idx12/\"\n",
    "types = [\"OV_Annotation3000_1000_idx12\", \"OV_CV_1000_idx12\", \n",
    "         \"OV_Var_1000_idx12\", \"OV_new_Diff_1000_idx12\",\n",
    "         \"inter_OV_Clin\", \n",
    "         \"OV_SNV_400_idx12_sen_boost\" \n",
    "         ]\n",
    "\n",
    "file_1 = path+types[0]+\".csv\"\n",
    "file_2 = path+types[1]+\".csv\"\n",
    "file_3 = path+types[2]+\".csv\"\n",
    "file_4 = path+types[3]+\".csv\"\n",
    "file_5 = path+types[4]+\".csv\"\n",
    "file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "sample_1,features_1 = data_1.shape\n",
    "sample_2,features_2 = data_2.shape\n",
    "sample_3,features_3 = data_3.shape\n",
    "sample_4,features_4 = data_4.shape\n",
    "sample_5,features_5 = data_5.shape\n",
    "sample_6,features_6 = data_6.shape\n",
    "\n",
    "# Data frame include index & Platinum_Status column, substract 2 to calculate real number of features \n",
    "[features_1, features_2, features_3, features_4, features_5, features_6] = [features_1-2, features_2-2, features_3-2, features_4-2, features_5-2, features_6-2]\n",
    "\n",
    "print(\"[1] file_name: \", types[0], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "print(\"[2] file_name: \", types[1], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "print(\"[3] file_name: \", types[2], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "print(\"[4] file_name: \", types[3], \"\\nsample : {}  \\nfeatures : {}\".format(sample_4,features_4))\n",
    "print(\"[5] file_name: \", types[4], \"\\nsample : {}  \\nfeatures : {}\".format(sample_5,features_5))\n",
    "print(\"[6] file_name: \", types[5], \"\\nsample : {}  \\nfeatures : {}\".format(sample_6,features_6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_Annotation3000_1000_idx12  ######################################\n",
      "(186, 1000) (31, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[0]+'  ######################################')\n",
    "\n",
    "train_data_1 = data_1.iloc[list(data_1.iloc[:,-1]!=1)]\n",
    "test_data_1 = data_1.iloc[list(data_1.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_1 = train_data_1.Platinum_Status\n",
    "x_val_1 = train_data_1.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_1 = test_data_1.Platinum_Status\n",
    "test_x_val_1 = test_data_1.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_1.shape, test_x_val_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_CV_1000_idx12  ######################################\n",
      "(186, 1000) (31, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[1]+'  ######################################')\n",
    "\n",
    "train_data_2 = data_2.iloc[list(data_2.iloc[:,-1]!=1)]\n",
    "test_data_2 = data_2.iloc[list(data_2.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_2 = train_data_2.Platinum_Status\n",
    "x_val_2 = train_data_2.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_2 = test_data_2.Platinum_Status\n",
    "test_x_val_2 = test_data_2.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_2.shape, test_x_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_Var_1000_idx12  ######################################\n",
      "(186, 1000) (31, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[2]+'  ######################################')\n",
    "\n",
    "train_data_3 = data_3.iloc[list(data_3.iloc[:,-1]!=1)]\n",
    "test_data_3 = data_3.iloc[list(data_3.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_3 = train_data_3.Platinum_Status\n",
    "x_val_3 = train_data_3.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_3 = test_data_3.Platinum_Status\n",
    "test_x_val_3 = test_data_3.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_3.shape, test_x_val_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_new_Diff_1000_idx12  ######################################\n",
      "(186, 1000) (31, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[3]+'  ######################################')\n",
    "\n",
    "train_data_4 = data_4.iloc[list(data_4.iloc[:,-1]!=1)]\n",
    "test_data_4 = data_4.iloc[list(data_4.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_4 = train_data_4.Platinum_Status\n",
    "x_val_4 = train_data_4.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_4 = test_data_4.Platinum_Status\n",
    "test_x_val_4 = test_data_4.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_4.shape, test_x_val_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  inter_OV_Clin  ######################################\n",
      "(122, 35) (31, 35)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[4]+'  ######################################')\n",
    "\n",
    "train_data_5 = data_5.iloc[list(data_5.iloc[:,-1]!=1)]\n",
    "test_data_5 = data_5.iloc[list(data_5.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_5 = train_data_5.Platinum_Status\n",
    "x_val_5 = train_data_5.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_5 = test_data_5.Platinum_Status\n",
    "test_x_val_5 = test_data_5.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_5.shape, test_x_val_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_SNV_400_idx12_sen_boost  ######################################\n",
      "(244, 400) (31, 400)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[5]+'  ######################################')\n",
    "\n",
    "train_data_6 = data_6.iloc[list(data_6.iloc[:,-1]!=1)]\n",
    "test_data_6 = data_6.iloc[list(data_6.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_6 = train_data_6.Platinum_Status\n",
    "x_val_6 = train_data_6.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_6 = test_data_6.Platinum_Status\n",
    "test_x_val_6 = test_data_6.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_6.shape, test_x_val_6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Seperate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building seperate model for ensemble(model 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000 1000 35 400\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "print(features_1, features_2, features_3, features_4, features_5, features_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_Annotation3000_1000_idx12  ######################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 6ms/step - loss: 0.7124 - acc: 0.6075\n",
      "186/186 [==============================] - 0s 556us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6149 - acc: 0.7097\n",
      "186/186 [==============================] - 0s 159us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6236 - acc: 0.7097\n",
      "186/186 [==============================] - 0s 91us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6030 - acc: 0.7097\n",
      "186/186 [==============================] - 0s 102us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6079 - acc: 0.7097\n",
      "186/186 [==============================] - 0s 131us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 980us/step - loss: 0.6093 - acc: 0.7097\n",
      "186/186 [==============================] - 0s 113us/step\n",
      "186/186 [==============================] - 0s 108us/step\n",
      "31/31 [==============================] - 0s 211us/step\n",
      "Train Accuracy: 0.7096774219184794\n",
      "Train Sensitivities & Specificities : 0.0, 1.0\n",
      "Test Accuracy: 0.7096773982048035\n",
      "Test Sensitivities & Specificities : 0.0, 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\saving.py:126: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[0]+'  ######################################')\n",
    "\n",
    "input_drop_out = 0.1\n",
    "drop_out_1 = 0.3\n",
    "\n",
    "\n",
    "m_1_tr_loss_best = 100\n",
    "count=0\n",
    "best_model_1=[]\n",
    "input_m_1 = Input(shape=(features_1,))\n",
    "m_1_dp = Dropout(input_drop_out)(input_m_1)\n",
    "\n",
    "for i in [30, 20, 10, 5]:\n",
    "    m_1 = Dense(i,activation='relu')(m_1_dp)\n",
    "    m_1_dp = Dropout(drop_out_1)(m_1)\n",
    "\n",
    "m_1_final = m_1_dp\n",
    "output_m_1 = Dense(1, activation=\"sigmoid\")(m_1_final)\n",
    "model_1 = Model(inputs=input_m_1,outputs=output_m_1)\n",
    "\n",
    "model_1.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "while 1:\n",
    "    model_1.fit(x_val_1, y_val_1, batch_size=5, nb_epoch=1)\n",
    "    m_1_tr_loss=model_1.evaluate(x_val_1,y_val_1)[0]\n",
    "    if m_1_tr_loss < m_1_tr_loss_best:\n",
    "        m_1_tr_loss_best = m_1_tr_loss\n",
    "        count=0\n",
    "        best_model_1 = model_1\n",
    "        \n",
    "    if count>3:\n",
    "        model_1 = best_model_1\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_1_tr_loss,m_1_tr_accuracy=model_1.evaluate(x_val_1,y_val_1)\n",
    "m_1_loss,m_1_accuracy= model_1.evaluate(test_x_val_1,test_y_val_1)\n",
    "    \n",
    "    #print(time)\n",
    "    #print(\"Train Accuracy: {}\".format(m_1_tr_accuracy))\n",
    "    #print(\"Test Accuracy: {}\".format(m_1_accuracy))\n",
    "\n",
    "\n",
    "m_1_predictions = model_1.predict(x_val_1)\n",
    "labeled_m_1_predictions = np.where(m_1_predictions > 0.5, 1, 0).flatten()\n",
    "m_1_tr_sensitivity, m_1_tr_specificity = check_correct(labeled_m_1_predictions, y_val_1)\n",
    "m_1_test_predictions = model_1.predict(test_x_val_1)\n",
    "labeled_m_1_test_predictions = np.where(m_1_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_1_sensitivity, m_1_specificity = check_correct(labeled_m_1_test_predictions, test_y_val_1)\n",
    "#m_1_test_predictions = m_1_test_predictions.flatten()\n",
    "m_1_test_predictions_flat = m_1_test_predictions[:,0]\n",
    "\n",
    "df_1 = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis_1\": list(m_1_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_1_test_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "df_1.to_csv(\"../result/prediction_result_m_1.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_1_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_1_tr_sensitivity)+\", \"+str(m_1_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_1_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_1_sensitivity)+\", \"+str(m_1_specificity))\n",
    "\n",
    "model_1.save(\"../models/Ovary/m_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_1_l = keras.models.load_model(\"../models/Ovary/m_5.h5\")\n",
    "model_1_l.compile(optimizer='adam', \n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "m_1_l_tr_loss,m_1_l_tr_accuracy=model_1_l.evaluate(x_val_1,y_val_1)\n",
    "m_1_l_loss,m_1_accuracy= model_1_l.evaluate(test_x_val_1,test_y_val_1)\n",
    "print(\"Train Accuracy: {}\".format(m_1_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_1_l_accuracy))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''keep_prob_2 = 0.5\n",
    "\n",
    "m_2_tr_loss_best = 100\n",
    "count=0\n",
    "best_model_2=[]\n",
    "input_m_2 = Input(shape=(features_2,))\n",
    "m_2_dp = Dropout(0.3)(input_m_2)\n",
    "\n",
    "for i in [200, 200, 150, 100]:\n",
    "    m_2 = Dense(i,activation='relu')(m_2_dp)\n",
    "\n",
    "m_2_dp = Dropout(keep_prob_2)(m_2)\n",
    "m_2_final = m_2_dp\n",
    "output_m_2 = Dense(1, activation=\"sigmoid\")(m_2_final)\n",
    "model_2 = Model(inputs=input_m_2,outputs=output_m_2)\n",
    "\n",
    "model_2.compile(optimizer=keras.optimizers.Adam(lr=0.01), \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1000)\n",
      "(31, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(x_val_2.shape)\n",
    "print(test_x_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_CV_1000_idx12  ######################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 5ms/step - loss: 1.8884 - acc: 0.6129\n",
      "186/186 [==============================] - 0s 977us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 0.9922 - acc: 0.7634\n",
      "186/186 [==============================] - 0s 231us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 0.5404 - acc: 0.8495\n",
      "186/186 [==============================] - 0s 247us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 0.4112 - acc: 0.8925\n",
      "186/186 [==============================] - ETA:  - 0s 231us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 0.2295 - acc: 0.9140\n",
      "186/186 [==============================] - 0s 214us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.9247\n",
      "186/186 [==============================] - 0s 236us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 1.0587 - acc: 0.8763\n",
      "186/186 [==============================] - 0s 214us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 3ms/step - loss: 1.3859 - acc: 0.8656\n",
      "186/186 [==============================] - 0s 231us/step\n",
      "186/186 [==============================] - 0s 214us/step\n",
      "31/31 [==============================] - 0s 193us/step\n",
      "Train Accuracy: 0.9247311834366091\n",
      "Train Sensitivities & Specificities : 0.7407407407407407, 1.0\n",
      "Test Accuracy: 0.7096773982048035\n",
      "Test Sensitivities & Specificities : 0.3333333333333333, 0.8636363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\saving.py:126: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[1]+'  ######################################')\n",
    "\n",
    "input_drop_out_2 = 0.2\n",
    "drop_out_2 = 0.3\n",
    "\n",
    "m_2_tr_loss_best = 100\n",
    "count=0\n",
    "best_model_2=[]\n",
    "input_m_2 = Input(shape=(features_2,))\n",
    "m_2_dp = Dropout(input_drop_out_2)(input_m_2)\n",
    "\n",
    "for i in [1000]:\n",
    "    m_2 = Dense(i,activation='relu')(m_2_dp)\n",
    "    m_2_dp = Dropout(drop_out_2)(m_2)\n",
    "m_2_final = m_2_dp\n",
    "output_m_2 = Dense(1, activation=\"sigmoid\")(m_2_final)\n",
    "model_2 = Model(inputs=input_m_2,outputs=output_m_2)\n",
    "\n",
    "model_2.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "while 1:\n",
    "    model_2.fit(x_val_2, y_val_2, batch_size=5, nb_epoch=1)\n",
    "    m_2_tr_loss=model_2.evaluate(x_val_2,y_val_2)[0]\n",
    "    if m_2_tr_loss < m_2_tr_loss_best:\n",
    "        m_2_tr_loss_best = m_2_tr_loss\n",
    "        count=0\n",
    "        best_model_2 = model_2\n",
    "    if count>3:\n",
    "        model_2 = best_model_2\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_2_tr_loss,m_2_tr_accuracy=model_2.evaluate(x_val_2,y_val_2)\n",
    "m_2_loss,m_2_accuracy= model_2.evaluate(test_x_val_2,test_y_val_2)\n",
    "\n",
    "m_2_predictions = model_2.predict(x_val_2)\n",
    "labeled_m_2_predictions = np.where(m_2_predictions > 0.5, 1, 0).flatten()\n",
    "m_2_tr_sensitivity, m_2_tr_specificity = check_correct(labeled_m_2_predictions, y_val_2)\n",
    "m_2_test_predictions = model_2.predict(test_x_val_2)\n",
    "labeled_m_2_test_predictions = np.where(m_2_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_2_sensitivity, m_2_specificity = check_correct(labeled_m_2_test_predictions, test_y_val_2)\n",
    "#m_2_test_predictions = m_2_test_predictions.flatten()\n",
    "m_2_test_predictions_flat = m_2_test_predictions[:,0]\n",
    "\n",
    "df_2 = pd.DataFrame(data={\"patient\":list(test_data_2.index), \"hypothesis_1\": list(m_2_test_predictions_flat), \n",
    "  \"prediction\":list(labeled_m_2_test_predictions), \"Platinum_Status\":list(test_y_val_2)})\n",
    "df_2.to_csv(\"../result/prediction_result_m_2.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_2_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_2_tr_sensitivity)+\", \"+str(m_2_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_2_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_2_sensitivity)+\", \"+str(m_2_specificity))\n",
    "\n",
    "model_2.save(\"../models/Ovary/m_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-a2ef7d1639c7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-a2ef7d1639c7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    keras_model(inputs=model_2$input, outputs = get_layer(model_2, m_2_final)$output))\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "keras_model(inputs=model_2$input, outputs = get_layer(model_2, m_2_final)$output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[2]+'  ######################################')\n",
    "\n",
    "nodes = [[100,100,100], [100,150,100], [200,150,100], [100, 150, 200], [80, 80, 80, 40]]\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "batch_sizes = [3, 5, 10]\n",
    "input_drop_outs = [0, 0.1, 0.2, 0.3]\n",
    "drop_outs = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "model_num = 0\n",
    "nodes_box = []\n",
    "learning_rate_box = []\n",
    "batch_size_box = []\n",
    "tr_accuracy_box = []\n",
    "ts_accuracy_box = []\n",
    "tr_sensitivity_box = []\n",
    "ts_sensitivity_box = []\n",
    "tr_specificity_box = []\n",
    "ts_specificity_box = []\n",
    "input_drop_out_box = []\n",
    "drop_out_box = []\n",
    "t_box = []\n",
    "\n",
    "index = []\n",
    "for node in nodes:\n",
    "    for l_rate in learning_rates:\n",
    "        for b_size in batch_sizes:\n",
    "            for input_drop_out_3 in input_drop_outs:\n",
    "                for drop_out_3 in drop_outs:\n",
    "                    for t in range(5):\n",
    "                        \n",
    "                        #input_drop_out_3 = 0.3\n",
    "                        #drop_out_3 = 0.4\n",
    "\n",
    "                        m_3_tr_loss_best = 100\n",
    "                        count=0\n",
    "                        best_model_3=[]\n",
    "                        input_m_3 = Input(shape=(features_3,))\n",
    "                        m_3_dp = Dropout(input_drop_out_3)(input_m_3)\n",
    "\n",
    "                        for i in node:\n",
    "                            m_3 = Dense(i,activation='relu')(m_3_dp)\n",
    "                            m_3_dp = Dropout(drop_out_3)(m_3)\n",
    "\n",
    "                        m_3_final = m_3_dp\n",
    "                        output_m_3 = Dense(1, activation=\"sigmoid\")(m_3_final)\n",
    "                        model_3 = Model(inputs=input_m_3,outputs=output_m_3)\n",
    "\n",
    "                        model_3.compile(optimizer=tf.train.AdamOptimizer(learning_rate=l_rate), \n",
    "                                        loss='binary_crossentropy',\n",
    "                                        metrics=['accuracy'])\n",
    "\n",
    "                        while 1:\n",
    "                            model_3.fit(x_val_3, y_val_3, batch_size=b_size, nb_epoch=1)\n",
    "                            m_3_tr_loss, m_3_tr_accuracy=model_3.evaluate(x_val_3,y_val_3)\n",
    "                            if m_3_tr_loss < m_3_tr_loss_best:\n",
    "                                m_3_tr_loss_best = m_3_tr_loss\n",
    "                                count=0\n",
    "                                best_model_3 = model_3\n",
    "                            if count>3 or m_3_tr_accuracy == 1:\n",
    "                                model_3 = best_model_3\n",
    "                                break\n",
    "                            else: count=count+1\n",
    "\n",
    "                        m_3_tr_loss,m_3_tr_accuracy=model_3.evaluate(x_val_3,y_val_3)\n",
    "                        m_3_loss,m_3_accuracy= model_3.evaluate(test_x_val_3,test_y_val_3)\n",
    "\n",
    "                        m_3_predictions = model_3.predict(x_val_3)\n",
    "                        labeled_m_3_predictions = np.where(m_3_predictions > 0.5, 1, 0).flatten()\n",
    "                        m_3_tr_sensitivity, m_3_tr_specificity = check_correct(labeled_m_3_predictions, y_val_3)\n",
    "                        m_3_test_predictions = model_3.predict(test_x_val_3)\n",
    "                        labeled_m_3_test_predictions = np.where(m_3_test_predictions > 0.5, 1, 0).flatten()\n",
    "                        m_3_sensitivity, m_3_specificity = check_correct(labeled_m_3_test_predictions, test_y_val_3)\n",
    "                        #m_3_test_predictions = m_3_test_predictions.flatten()\n",
    "                        m_3_test_predictions_flat = m_3_test_predictions[:,0]\n",
    "\n",
    "                        df_3 = pd.DataFrame(data={\"patient\":list(test_data_3.index), \"hypothesis_1\": list(m_3_test_predictions_flat), \n",
    "                          \"prediction\":list(labeled_m_3_test_predictions), \"Platinum_Status\":list(test_y_val_3)})\n",
    "                        df_3.to_csv(\"../result/prediction_result_m_3_\"+str(model_num)+\".csv\", index=False, header=True, \n",
    "                                    columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "                        print(\"Train Accuracy: {}\".format(m_3_tr_accuracy))\n",
    "                        print(\"Train Sensitivities & Specificities : \"+str(m_3_tr_sensitivity)+\", \"+str(m_3_tr_specificity))\n",
    "                        print(\"Test Accuracy: {}\".format(m_3_accuracy))\n",
    "                        print(\"Test Sensitivities & Specificities : \"+str(m_3_sensitivity)+\", \"+str(m_3_specificity))\n",
    "\n",
    "                        nodes_box.append(node)\n",
    "                        learning_rate_box.append(l_rate)\n",
    "                        batch_size_box.append(b_size)\n",
    "\n",
    "                        # save accuracy & sensitivity & specificity\n",
    "                        tr_accuracy_box.append(m_3_tr_accuracy)\n",
    "                        tr_sensitivity_box.append(m_3_tr_sensitivity)\n",
    "                        tr_specificity_box.append(m_3_tr_specificity)\n",
    "\n",
    "                        ts_accuracy_box.append(m_3_accuracy)\n",
    "                        ts_sensitivity_box.append(m_3_sensitivity)\n",
    "                        ts_specificity_box.append(m_3_specificity)\n",
    "                        input_drop_out_box.append(input_drop_out_3)\n",
    "                        drop_out_box.append(drop_out_3)\n",
    "                        t_box.append(t)\n",
    "                        input_drop_out_box = []\n",
    "                        idrop_out_box = []\n",
    "                        model_3.save(\"../models/Ovary/m_3_\"+str(model_num)+\".h5\")\n",
    "                        model_num = model_num+1\n",
    "                        \n",
    "                        if m_3_tr_accuracy < 0.5 or m_3_accuracy == 1.0 or m_3_sensitivity < 0.4:\n",
    "                            break\n",
    "\n",
    "\n",
    "df_all=pd.DataFrame(data={\"nodes\": nodes_box, \"learning_rate\": learning_rate_box, \"batch_size\": batch_size_box, \n",
    "                          \"tr_accuracy\": tr_accuracy_box, \"ts_accuracy\": ts_accuracy_box, \n",
    "                          \"tr_sensitivity\": tr_sensitivity_box, \"ts_sensitivity\": tr_sensitivity_box, \n",
    "                          \"tr_specificity\": tr_specificity_box, \"ts_specificity\": ts_specificity_box,\n",
    "                          \"input_drop_out\": input_drop_out_box , \"drop_out\": drop_out_box, \"try\": t_box\n",
    "                         })\n",
    "df_all.to_csv(\"../result/results.csv\", index=True, header=True, columns = [ \"nodes\", \"learning_rate\", \"batch_size\", \"tr_accuracy\", \"ts_accuracy\",\n",
    "                                                                            \"tr_sensitivity\", \"ts_sensitivity\", \"tr_specificity\", \"ts_specificity\",\n",
    "                                                                           \"input_drop_out\", \"drop_out\", \"try\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('######################################  '+types[3]+'  ######################################')\n",
    "\n",
    "nodes = [[100,100,100], [100,150,100], [200,150,100], [100, 150, 200], [80, 80, 80, 40]]\n",
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "batch_sizes = [3, 5, 10]\n",
    "input_drop_outs = [0, 0.1, 0.2, 0.3]\n",
    "drop_outs = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "model_num = 0\n",
    "nodes_box = []\n",
    "learning_rate_box = []\n",
    "batch_size_box = []\n",
    "tr_accuracy_box = []\n",
    "ts_accuracy_box = []\n",
    "tr_sensitivity_box = []\n",
    "ts_sensitivity_box = []\n",
    "tr_specificity_box = []\n",
    "ts_specificity_box = []\n",
    "input_drop_out_box = []\n",
    "drop_out_box = []\n",
    "t_box = []\n",
    "\n",
    "index = []\n",
    "for node in nodes:\n",
    "    for l_rate in learning_rates:\n",
    "        for b_size in batch_sizes:\n",
    "            for input_drop_out_4 in input_drop_outs:\n",
    "                for drop_out_4 in drop_outs:\n",
    "                    for t in range(5):\n",
    "\n",
    "                        #input_drop_out_4 = 0.1\n",
    "                        #drop_out_4 = 0.3\n",
    "\n",
    "                        m_4_tr_loss_best = 100\n",
    "                        count=0\n",
    "                        best_model_4=[]\n",
    "                        input_m_4 = Input(shape=(features_4,))\n",
    "                        m_4_dp = Dropout(input_drop_out_4)(input_m_4)\n",
    "\n",
    "                        for i in node:\n",
    "                            m_4 = Dense(i,activation='relu')(m_4_dp)\n",
    "                            m_4_dp = Dropout(drop_out_4)(m_4)\n",
    "                        m_4_final = m_4_dp\n",
    "                        output_m_4 = Dense(1, activation=\"sigmoid\")(m_4_final)\n",
    "                        model_4 = Model(inputs=input_m_4,outputs=output_m_4)\n",
    "\n",
    "                        model_4.compile(optimizer=tf.train.AdamOptimizer(learning_rate=l_rate), \n",
    "                                        loss='binary_crossentropy',\n",
    "                                        metrics=['accuracy'])\n",
    "\n",
    "                        while 1:\n",
    "                            model_4.fit(x_val_4, y_val_4, batch_size=b_size, nb_epoch=1)\n",
    "                            m_4_tr_loss, m_4_tr_accuracy=model_4.evaluate(x_val_4,y_val_4)\n",
    "                            if m_4_tr_loss < m_4_tr_loss_best:\n",
    "                                m_4_tr_loss_best = m_4_tr_loss\n",
    "                                count=0\n",
    "                                best_model_4 = model_4\n",
    "                            if count>3 or m_4_tr_accuracy == 1:\n",
    "                                model_4 = best_model_4\n",
    "                                break\n",
    "                            else: count=count+1\n",
    "                        m_4_accuracy= model_4.evaluate(test_x_val_4,test_y_val_4)[1]\n",
    "\n",
    "                        m_4_tr_loss,m_4_tr_accuracy=model_4.evaluate(x_val_4,y_val_4)\n",
    "                        m_4_loss,m_4_accuracy= model_4.evaluate(test_x_val_4,test_y_val_4)\n",
    "\n",
    "                        m_4_predictions = model_4.predict(x_val_4)\n",
    "                        labeled_m_4_predictions = np.where(m_4_predictions > 0.5, 1, 0).flatten()\n",
    "                        m_4_tr_sensitivity, m_4_tr_specificity = check_correct(labeled_m_4_predictions, y_val_4)\n",
    "                        m_4_test_predictions = model_4.predict(test_x_val_4)\n",
    "                        labeled_m_4_test_predictions = np.where(m_4_test_predictions > 0.5, 1, 0).flatten()\n",
    "                        m_4_sensitivity, m_4_specificity = check_correct(labeled_m_4_test_predictions, test_y_val_4)\n",
    "                        #m_4_test_predictions = m_4_test_predictions.flatten()\n",
    "                        m_4_test_predictions_flat = m_4_test_predictions[:,0]\n",
    "\n",
    "                        df_4 = pd.DataFrame(data={\"patient\":list(test_data_4.index), \"hypothesis_1\": list(m_4_test_predictions_flat), \n",
    "                          \"prediction\":list(labeled_m_4_test_predictions), \"Platinum_Status\":list(test_y_val_4)})\n",
    "                        df_4.to_csv(\"../result/prediction_result_m_4_\"+str(model_num)+\".csv\", index=False, header=True, \n",
    "                                    columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "                        print(\"Train Accuracy: {}\".format(m_4_tr_accuracy))\n",
    "                        print(\"Train Sensitivities & Specificities : \"+str(m_4_tr_sensitivity)+\", \"+str(m_4_tr_specificity))\n",
    "                        print(\"Test Accuracy: {}\".format(m_4_accuracy))\n",
    "                        print(\"Test Sensitivities & Specificities : \"+str(m_4_sensitivity)+\", \"+str(m_4_specificity))\n",
    "\n",
    "                        nodes_box.append(node)\n",
    "                        learning_rate_box.append(l_rate)\n",
    "                        batch_size_box.append(b_size)\n",
    "\n",
    "                        # save accuracy & sensitivity & specificity\n",
    "                        tr_accuracy_box.append(m_4_tr_accuracy)\n",
    "                        tr_sensitivity_box.append(m_4_tr_sensitivity)\n",
    "                        tr_specificity_box.append(m_4_tr_specificity)\n",
    "\n",
    "                        ts_accuracy_box.append(m_4_accuracy)\n",
    "                        ts_sensitivity_box.append(m_4_sensitivity)\n",
    "                        ts_specificity_box.append(m_4_specificity)\n",
    "                        input_drop_out_box.append(input_drop_out_4)\n",
    "                        drop_out_box.append(drop_out_4)\n",
    "                        t_box.append(t)\n",
    "                        input_drop_out_box = []\n",
    "                        idrop_out_box = []\n",
    "                        model_4.save(\"../models/Ovary/m_4_\"+str(model_num)+\".h5\")\n",
    "                        model_num = model_num+1\n",
    "\n",
    "                        if m_4_tr_accuracy < 0.5 or m_4_accuracy == 1.0 or m_4_sensitivity < 0.4:\n",
    "                            break\n",
    "\n",
    "\n",
    "df_all=pd.DataFrame(data={\"nodes\": nodes_box, \"learning_rate\": learning_rate_box, \"batch_size\": batch_size_box, \n",
    "                          \"tr_accuracy\": tr_accuracy_box, \"ts_accuracy\": ts_accuracy_box, \n",
    "                          \"tr_sensitivity\": tr_sensitivity_box, \"ts_sensitivity\": tr_sensitivity_box, \n",
    "                          \"tr_specificity\": tr_specificity_box, \"ts_specificity\": ts_specificity_box,\n",
    "                          \"input_drop_out\": input_drop_out_box , \"drop_out\": drop_out_box, \"try\": t_box\n",
    "                         })\n",
    "df_all.to_csv(\"../result/new_Diff_1000_results.csv\", index=True, header=True, columns = [ \"nodes\", \"learning_rate\", \"batch_size\", \"tr_accuracy\", \"ts_accuracy\",\n",
    "                                                                            \"tr_sensitivity\", \"ts_sensitivity\", \"tr_specificity\", \"ts_specificity\",\n",
    "                                                                           \"input_drop_out\", \"drop_out\", \"try\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=pd.DataFrame(data={\"nodes\": nodes_box, \"learning_rate\": learning_rate_box, \"batch_size\": batch_size_box, \n",
    "                          \"tr_accuracy\": tr_accuracy_box, \"ts_accuracy\": ts_accuracy_box, \n",
    "                          \"tr_sensitivity\": tr_sensitivity_box, \"ts_sensitivity\": tr_sensitivity_box, \n",
    "                          \"tr_specificity\": tr_specificity_box, \"ts_specificity\": ts_specificity_box,\n",
    "                          \"input_drop_out\": input_drop_out_box , \"drop_out\": drop_out_box, \"try\": t_box\n",
    "                         })\n",
    "df_all.to_csv(\"../result/new_Diff_1000_results.csv\", index=True, header=True, columns = [ \"nodes\", \"learning_rate\", \"batch_size\", \"tr_accuracy\", \"ts_accuracy\",\n",
    "                                                                            \"tr_sensitivity\", \"ts_sensitivity\", \"tr_specificity\", \"ts_specificity\",\n",
    "                                                                           \"input_drop_out\", \"drop_out\", \"try\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.to_csv(\"../result/new_Diff_1000_results.csv\", index=True, header=True, columns = [ \"nodes\", \"learning_rate\", \"batch_size\", \"tr_accuracy\", \"ts_accuracy\",\n",
    "#                                                                            \"tr_sensitivity\", \"ts_sensitivity\", \"tr_specificity\", \"ts_specificity\",\n",
    "#                                                                           \"input_drop_out\", \"drop_out\", \"try\"])\n",
    "\n",
    "for a in range(len(ts_accuracy_box)):\n",
    "    if tr_accuracy_box[a] < 1 and ts_accuracy_box[a] > 0.6 and ts_sensitivity_box[a] > 0.3:\n",
    "        print(\"find!!!! \"+str(a))\n",
    "        print(tr_accuracy_box[a])\n",
    "        print(ts_accuracy_box[a])\n",
    "        print(tr_sensitivity_box[a])\n",
    "        print(ts_sensitivity_box[a])\n",
    "        \n",
    "        print(\"####################\")\n",
    "        print(node)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[4]+'  ######################################')\n",
    "\n",
    "input_drop_out_5 = 0.1\n",
    "drop_out_5 = 0.3\n",
    "\n",
    "m_5_tr_loss_best = 100\n",
    "count=0\n",
    "best_model_5=[]\n",
    "input_m_5 = Input(shape=(features_5,))\n",
    "m_5_dp = Dropout(input_drop_out_5)(input_m_5)\n",
    "\n",
    "for i in [200, 200, 150, 100]:\n",
    "    m_5 = Dense(i,activation='relu')(m_5_dp)\n",
    "    m_5_dp = Dropout(drop_out_5)(m_5)\n",
    "\n",
    "m_5_final = m_5_dp\n",
    "output_m_5 = Dense(1, activation=\"sigmoid\")(m_5_final)\n",
    "model_5 = Model(inputs=input_m_5,outputs=output_m_5)\n",
    "\n",
    "model_5.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "while 1:\n",
    "    model_5.fit(x_val_5, y_val_5, batch_size=5, nb_epoch=1)\n",
    "    m_5_tr_loss=model_5.evaluate(x_val_5,y_val_5)[0]\n",
    "    if m_5_tr_loss < m_5_tr_loss_best:\n",
    "        m_5_tr_loss_best = m_5_tr_loss\n",
    "        count=0\n",
    "        best_model_5 = model_5\n",
    "    if count>3:\n",
    "        model_5 = best_model_5\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_5_tr_loss,m_5_tr_accuracy=model_5.evaluate(x_val_5,y_val_5)\n",
    "m_5_loss,m_5_accuracy= model_5.evaluate(test_x_val_5,test_y_val_5)\n",
    "m_5_predictions = model_5.predict(x_val_5)\n",
    "labeled_m_5_predictions = np.where(m_5_predictions > 0.5, 1, 0).flatten()\n",
    "m_5_tr_sensitivity, m_5_tr_specificity = check_correct(labeled_m_5_predictions, y_val_5)\n",
    "m_5_test_predictions = model_5.predict(test_x_val_5)\n",
    "labeled_m_5_test_predictions = np.where(m_5_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_5_sensitivity, m_5_specificity = check_correct(labeled_m_5_test_predictions, test_y_val_5)\n",
    "#m_5_test_predictions = m_5_test_predictions.flatten()\n",
    "m_5_test_predictions_flat = m_5_test_predictions[:,0]\n",
    "\n",
    "df_5 = pd.DataFrame(data={\"patient\":list(test_data_5.index), \"hypothesis_1\": list(m_5_test_predictions_flat), \n",
    "  \"prediction\":list(labeled_m_5_test_predictions), \"Platinum_Status\":list(test_y_val_5)})\n",
    "df_5.to_csv(\"../result/prediction_result_m_5.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_5_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_5_tr_sensitivity)+\", \"+str(m_5_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_5_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_5_sensitivity)+\", \"+str(m_5_specificity))\n",
    "\n",
    "model_5.save(\"../models/Ovary/m_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_SNV_400_idx12_sen_boost  ######################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 8s 32ms/step - loss: 0.7139 - acc: 0.5000\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.3930 - acc: 0.8484\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.1830 - acc: 0.9344\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.0330 - acc: 0.9877\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.0134 - acc: 0.9918\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.1198 - acc: 0.9754\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.0266 - acc: 0.9918\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.0255 - acc: 0.9959\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 7.1251e-04 - acc: 1.0000\n",
      "244/244 [==============================] - 4s 16ms/step\n",
      "244/244 [==============================] - 0s 290us/step\n",
      "31/31 [==============================] - 0s 289us/step\n",
      "Train Accuracy: 1.0\n",
      "Train Sensitivities & Specificities : 1.0, 1.0\n",
      "Test Accuracy: 0.7096773982048035\n",
      "Test Sensitivities & Specificities : 0.2857142857142857, 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\saving.py:126: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 7s 30ms/step - loss: 0.7115 - acc: 0.4959\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 0s 932us/step - loss: 0.4725 - acc: 0.8074\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 0s 968us/step - loss: 0.1678 - acc: 0.9385\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 0s 977us/step - loss: 0.0946 - acc: 0.9672\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 0s 916us/step - loss: 0.0175 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 0s 982us/step - loss: 0.0139 - acc: 0.9959\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.0417 - acc: 0.9877\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.0840 - acc: 0.9877\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 0s 907us/step - loss: 0.0343 - acc: 0.9877\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 0s 883us/step - loss: 0.0393 - acc: 0.9877\n",
      "244/244 [==============================] - 3s 14ms/step\n",
      "244/244 [==============================] - 0s 266us/step\n",
      "31/31 [==============================] - 0s 322us/step\n",
      "Train Accuracy: 1.0\n",
      "Train Sensitivities & Specificities : 1.0, 1.0\n",
      "Test Accuracy: 0.6774193644523621\n",
      "Test Sensitivities & Specificities : 0.2857142857142857, 0.7916666666666666\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[5]+'  ######################################')\n",
    "\n",
    "input_drop_out_6 = 0\n",
    "drop_out_6 = 0.5\n",
    "\n",
    "for input_drop_out_6, drop_out_6 in [[0, 0.4], [0, 0.5], [0, 0.6], [0.4, 0.3], [0.4, 0.5]]:\n",
    "    for node in [[100, 200, 150], [100, 100, 100], [80, 90, 100], [50, 100, 200, 150]]:\n",
    "            \n",
    "        m_6_tr_loss_best = 100\n",
    "        count=0\n",
    "        best_model_5=[]\n",
    "        input_m_6 = Input(shape=(features_6,))\n",
    "        m_6_dp = Dropout(input_drop_out_6)(input_m_6)\n",
    "\n",
    "        for i in node:\n",
    "            m_6 = Dense(i,activation='relu')(m_6_dp)\n",
    "            m_6_dp = Dropout(drop_out_6)(m_6)\n",
    "\n",
    "        m_6_final = m_6_dp\n",
    "        output_m_6 = Dense(1, activation=\"sigmoid\")(m_6_final)\n",
    "        model_6 = Model(inputs=input_m_6,outputs=output_m_6)\n",
    "\n",
    "        model_6.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        #model_6.fit(x_val_6, y_val_6, batch_size=10, epochs=10)\n",
    "        #'''\n",
    "        while 1:\n",
    "            model_6.fit(x_val_6, y_val_6, batch_size=10, nb_epoch=10)\n",
    "            m_6_tr_loss, m_6_tr_accuracy=model_6.evaluate(x_val_6,y_val_6)\n",
    "            if m_6_tr_loss < m_6_tr_loss_best:\n",
    "                m_6_tr_loss_best = m_6_tr_loss\n",
    "                count=0\n",
    "                best_model_6 = model_6\n",
    "            if count>3 or m_6_tr_accuracy==1:\n",
    "                model_6 = best_model_6\n",
    "                break\n",
    "            else: count=count+1\n",
    "        #'''\n",
    "\n",
    "        m_6_tr_loss,m_6_tr_accuracy=model_6.evaluate(x_val_6,y_val_6)\n",
    "        m_6_loss,m_6_accuracy= model_6.evaluate(test_x_val_6,test_y_val_6)\n",
    "\n",
    "        m_6_predictions = model_6.predict(x_val_6)\n",
    "        labeled_m_6_predictions = np.where(m_6_predictions > 0.5, 1, 0).flatten()\n",
    "        m_6_tr_sensitivity, m_6_tr_specificity = check_correct(labeled_m_6_predictions, y_val_6)\n",
    "        m_6_test_predictions = model_6.predict(test_x_val_6)\n",
    "        labeled_m_6_test_predictions = np.where(m_6_test_predictions > 0.5, 1, 0).flatten()\n",
    "        m_6_sensitivity, m_6_specificity = check_correct(labeled_m_6_test_predictions, test_y_val_6)\n",
    "        #m_6_test_predictions = m_6_test_predictions.flatten()\n",
    "        m_6_test_predictions_flat = m_6_test_predictions[:,0]\n",
    "\n",
    "        df_6 = pd.DataFrame(data={\"patient\":list(test_data_6.index), \"hypothesis_1\": list(m_6_test_predictions_flat), \n",
    "          \"prediction\":list(labeled_m_6_test_predictions), \"Platinum_Status\":list(test_y_val_6)})\n",
    "        df_6.to_csv(\"../result/prediction_result_m_6.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "        print(\"Train Accuracy: {}\".format(m_6_tr_accuracy))\n",
    "        print(\"Train Sensitivities & Specificities : \"+str(m_6_tr_sensitivity)+\", \"+str(m_6_tr_specificity))\n",
    "        print(\"Test Accuracy: {}\".format(m_6_accuracy))\n",
    "        print(\"Test Sensitivities & Specificities : \"+str(m_6_sensitivity)+\", \"+str(m_6_specificity))\n",
    "\n",
    "        model_6.save(\"../models/Ovary/m_6.h5\")\n",
    "    if m_6_accuracy > 0.7 and m_6_sensitivity > 0.5:\n",
    "        print(\"find!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6.evaluate(test_x_val_6,test_y_val_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seperate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"< \"+types[0]+\" > tr: \"+str(m_1_tr_accuracy)+\", ts: \"+str(m_1_accuracy))\n",
    "print(\"< \"+types[1]+\" > tr: \"+str(m_2_tr_accuracy)+\", ts: \"+str(m_2_accuracy))\n",
    "print(\"< \"+types[2]+\" > tr: \"+str(m_3_tr_accuracy)+\", ts: \"+str(m_3_accuracy))\n",
    "print(\"< \"+types[3]+\" > tr: \"+str(m_4_tr_accuracy)+\", ts: \"+str(m_4_accuracy))\n",
    "print(\"< \"+types[4]+\" > tr: \"+str(m_5_tr_accuracy)+\", ts: \"+str(m_5_accuracy))\n",
    "print(\"< \"+types[5]+\" > tr: \"+str(m_6_tr_accuracy)+\", ts: \"+str(m_6_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building original ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = [1, 2, 3, 4, 5, 6]\n",
    "print(\"################################## DNN Ensemble ##################################\")\n",
    "print(select)\n",
    "for type_i in select:\n",
    "    print(types[type_i-1])\n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "m_1_predictions = model_1.predict(x_val_1)\n",
    "m_2_predictions = model_2.predict(x_val_2)\n",
    "m_3_predictions = model_3.predict(x_val_3)\n",
    "m_4_predictions = model_4.predict(x_val_4)\n",
    "m_5_predictions = model_5.predict(x_val_5)\n",
    "m_6_predictions = model_6.predict(x_val_6)\n",
    "m_predictions = [m_1_predictions, m_2_predictions, m_3_predictions, m_4_predictions, m_5_predictions, m_6_predictions]\n",
    "m_predictions_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_predictions_select.append(m_predictions[select[i]-1])\n",
    "    \n",
    "ensemble_x_val = np.concatenate(m_predictions_select, axis=1)\n",
    "\n",
    "ensemble_model = keras.Sequential([\n",
    "    keras.layers.Dense(3,input_shape=(len(select),),name=\"input_layer\"),\n",
    "    keras.layers.Dense(2,activation=\"relu\"),        \n",
    "    keras.layers.Dense(1,activation='sigmoid',name=\"output_layer\")])\n",
    "\n",
    "ensemble_model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ensemble_model.fit(ensemble_x_val, y_val_1, epochs=5, batch_size= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _DNN Combiner_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_1_test_predictions = model_1.predict(test_x_val_1)\n",
    "m_2_test_predictions = model_2.predict(test_x_val_2)\n",
    "m_3_test_predictions = model_3.predict(test_x_val_3)\n",
    "m_4_test_predictions = model_4.predict(test_x_val_4)\n",
    "m_5_test_predictions = model_5.predict(test_x_val_5)\n",
    "m_6_test_predictions = model_6.predict(test_x_val_6)\n",
    "m_test_predictions = [m_1_test_predictions, m_2_test_predictions, m_3_test_predictions, m_4_test_predictions, m_5_test_predictions, m_6_test_predictions]\n",
    "m_test_predictions_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_test_predictions_select.append(m_test_predictions[select[i]-1])\n",
    "\n",
    "ensemble_test_x_val = np.concatenate(m_test_predictions_select, axis=1)\n",
    "\n",
    "em_tr_loss,em_tr_accuracy= ensemble_model.evaluate(ensemble_x_val,y_val_1)\n",
    "em_loss,em_accuracy= ensemble_model.evaluate(ensemble_test_x_val,test_y_val_1)\n",
    "\n",
    "ensemble_predictions = ensemble_model.predict(ensemble_x_val)\n",
    "labeled_ensemble_predictions = np.where(ensemble_predictions > 0.5, 1, 0).flatten()\n",
    "ensemble_tr_sensitivity, ensemble_tr_specificity = check_correct(labeled_ensemble_predictions, y_val_1)\n",
    "\n",
    "ensemble_test_predictions = ensemble_model.predict(ensemble_test_x_val)\n",
    "labeled_ensemble_test_predictions = np.where(ensemble_test_predictions > 0.5, 1, 0).flatten()\n",
    "ensemble_sensitivity, ensemble_specificity = check_correct(labeled_ensemble_test_predictions, test_y_val_1)\n",
    "\n",
    "ensemble_test_predictions_flat = ensemble_test_predictions[:,0]\n",
    "\n",
    "df_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis_1\": list(ensemble_test_predictions_flat), \n",
    "  \"prediction\":list(labeled_ensemble_test_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "df_em.to_csv(\"../result/prediction_result_EM_DNN.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(ensemble_tr_sensitivity)+\", \"+str(ensemble_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(em_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(ensemble_sensitivity)+\", \"+str(ensemble_specificity))\n",
    "\n",
    "ensemble_model.save(\"../models/Ovary/EM_DNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _mean_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tr_predictions=sum(m_predictions_select)/len(select)\n",
    "mean_em_labeled_tr_predictions = np.where(mean_tr_predictions > 0.5, 1, 0).flatten()\n",
    "mean_em_tr_accuracy = sum(mean_em_labeled_tr_predictions==y_val_1.values)/len(y_val_1)\n",
    "mean_ensemble_tr_sensitivity, mean_ensemble_tr_specificity = check_correct(mean_em_labeled_tr_predictions, y_val_1)\n",
    "\n",
    "mean_predictions=sum(m_test_predictions_select)/len(select)\n",
    "mean_em_labeled_predictions = np.where(mean_predictions > 0.5, 1, 0).flatten()\n",
    "mean_em_accuracy = sum(mean_em_labeled_predictions==test_y_val_1.values)/len(test_y_val_1)\n",
    "mean_ensemble_sensitivity, mean_ensemble_specificity = check_correct(mean_em_labeled_predictions, test_y_val_1)\n",
    "\n",
    "mean_ensemble_test_predictions_flat = mean_predictions[:,0]\n",
    "\n",
    "df_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis_1\": list(mean_ensemble_test_predictions_flat), \n",
    "  \"prediction\":list(mean_em_labeled_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "df_em.to_csv(\"../result/prediction_result_EM_mean.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "print(\"Train Accuracy for mean ensemble : {}\".format(mean_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mean_ensemble_tr_sensitivity)+\", \"+str(mean_ensemble_tr_specificity))\n",
    "print(\"Test Accuracy for mean ensemble : {}\".format(mean_em_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mean_ensemble_sensitivity)+\", \"+str(mean_ensemble_specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferred Ensemble Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making new input data for t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=[input_m_1], outputs=[m_1_final])\n",
    "results_m_1 = model.predict([x_val_1])\n",
    "\n",
    "model = Model(inputs=[input_m_2], outputs=[m_2_final])\n",
    "results_m_2 = model.predict([x_val_2])\n",
    "\n",
    "model = Model(inputs=[input_m_3], outputs=[m_3_final])\n",
    "results_m_3 = model.predict([x_val_3])\n",
    "\n",
    "model = Model(inputs=[input_m_4], outputs=[m_4_final])\n",
    "results_m_4 = model.predict([x_val_4])\n",
    "\n",
    "model = Model(inputs=[input_m_5], outputs=[m_5_final])\n",
    "results_m_5 = model.predict([x_val_5])\n",
    "\n",
    "model = Model(inputs=[input_m_6], outputs=[m_6_final])\n",
    "results_m_6 = model.predict([x_val_6])\n",
    "\n",
    "results_m_sum = [results_m_1, results_m_2, results_m_3, results_m_4, results_m_5, results_m_6]\n",
    "results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    results_m_select.append(results_m_sum[select[i]-1])\n",
    "\n",
    "t_ensemble_x_val = np.concatenate(results_m_select, axis=1)\n",
    "print(t_ensemble_x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling t-ensemble  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ensemble_input = Input(shape=(t_ensemble_x_val.shape[1],))\n",
    "t_ensemble_h1 = Dense(20,activation='relu')(t_ensemble_input)\n",
    "t_ensemble_h2 = Dense(10,activation='relu')(t_ensemble_h1)\n",
    "t_ensemble_h3 = Dense(5,activation='relu')(t_ensemble_h2)\n",
    "t_ensemble_output = Dense(1,activation='sigmoid')(t_ensemble_h3)\n",
    "\n",
    "t_ensemble_model = Model(inputs=[t_ensemble_input],outputs=[t_ensemble_output])\n",
    "t_ensemble_model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "t_ensemble_model.fit(t_ensemble_x_val, y_val_1, epochs=2,batch_size=5)\n",
    "ensemble_model.save(\"../models/Ovary/t_EM_DNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_m_1], outputs=[m_1_final])\n",
    "test_results_m_1 = model.predict([test_x_val_1])\n",
    "\n",
    "model = Model(inputs=[input_m_2], outputs=[m_2_final])\n",
    "test_results_m_2 = model.predict([test_x_val_2])\n",
    "\n",
    "model = Model(inputs=[input_m_3], outputs=[m_3_final])\n",
    "test_results_m_3 = model.predict([test_x_val_3])\n",
    "\n",
    "model = Model(inputs=[input_m_4], outputs=[m_4_final])\n",
    "test_results_m_4 = model.predict([test_x_val_4])\n",
    "\n",
    "model = Model(inputs=[input_m_5], outputs=[m_5_final])\n",
    "test_results_m_5 = model.predict([test_x_val_5])\n",
    "\n",
    "model = Model(inputs=[input_m_6], outputs=[m_6_final])\n",
    "test_results_m_6 = model.predict([test_x_val_6])\n",
    "\n",
    "test_results_m_sum = [test_results_m_1, test_results_m_2, test_results_m_3, test_results_m_4, test_results_m_5, test_results_m_6]\n",
    "test_results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    test_results_m_select.append(test_results_m_sum[select[i]-1])\n",
    "\n",
    "t_ensemble_test_x_val = np.concatenate(test_results_m_select, axis=1)\n",
    "t_em_tr_accuracy = t_ensemble_model.evaluate(t_ensemble_x_val,y_val_1)[1]\n",
    "t_em_accuracy = t_ensemble_model.evaluate(t_ensemble_test_x_val,test_y_val_1)[1]\n",
    "\n",
    "t_ensemble_predictions = t_ensemble_model.predict(t_ensemble_x_val)\n",
    "labeled_t_ensemble_predictions = np.where(t_ensemble_predictions > 0.5, 1, 0).flatten()\n",
    "t_ensemble_tr_sensitivity, t_ensemble_tr_specificity = check_correct(labeled_t_ensemble_predictions, y_val_1)\n",
    "\n",
    "t_ensemble_test_predictions = t_ensemble_model.predict(t_ensemble_test_x_val)\n",
    "labeled_t_ensemble_test_predictions = np.where(t_ensemble_test_predictions > 0.5, 1, 0).flatten()\n",
    "t_ensemble_sensitivity, t_ensemble_specificity = check_correct(labeled_t_ensemble_test_predictions, test_y_val_1)\n",
    "\n",
    "t_ensemble_test_predictions_flat = t_ensemble_test_predictions[:,0]\n",
    "\n",
    "df_t_EM = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis_1\": list(t_ensemble_test_predictions_flat), \n",
    "  \"prediction\":list(labeled_t_ensemble_test_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "df_t_EM.to_csv(\"../result/prediction_result_EM_t.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(t_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(t_ensemble_tr_sensitivity)+\", \"+str(t_ensemble_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(t_em_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(t_ensemble_sensitivity)+\", \"+str(t_ensemble_specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "tr_accuracy_list = [m_1_tr_accuracy, m_2_tr_accuracy, m_3_tr_accuracy, m_4_tr_accuracy, m_5_tr_accuracy, m_6_tr_accuracy]\n",
    "ts_accuracy_list = [m_1_accuracy, m_2_accuracy, m_3_accuracy, m_4_accuracy, m_5_accuracy, m_6_accuracy]\n",
    "tr_accuracy_select = []\n",
    "ts_accuracy_select = []\n",
    "\n",
    "for i in select:\n",
    "    label.append(\"model\"+str(i))\n",
    "    tr_accuracy_select.append(tr_accuracy_list[i-1])\n",
    "    ts_accuracy_select.append(ts_accuracy_list[i-1])\n",
    "\n",
    "label = label+[\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "tr_accuracy_select= tr_accuracy_select + [mean_em_tr_accuracy, em_tr_accuracy, t_em_tr_accuracy]\n",
    "ts_accuracy_select= ts_accuracy_select + [mean_em_accuracy, em_accuracy, t_em_accuracy]\n",
    "\n",
    "for model_num in range(len(label)):\n",
    "    print(\"< \"+label[model_num]+\" > tr: \"+str(tr_accuracy_select[model_num])+\", ts: \"+str(ts_accuracy_select[model_num]))\n",
    "\n",
    "#label = [\"model1\",\"model2\",\"model3\",\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "#accuracy = [m1_accuracy,m2_accuracy,m3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy ]\n",
    "#print(\"model1: \"+str(accuracy[0])+\"\\nmodel2: \"+str(accuracy[1])+\"\\nmodel3: \"+str(accuracy[2])+\"\\nmean-em: \"+str(accuracy[3])+\"\\nd-comb em: \"+str(accuracy[4])+\"\\nt-em: \"+str(accuracy[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot_bar_x():\n",
    "    # this is for plotting purpose\n",
    "    plt.figure(figsize=(30,20))\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([min(m_1_accuracy,m_2_accuracy,m_3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy)-0.02,1])\n",
    "    index = np.arange(len(label))\n",
    "    plt.bar(index, accuracy,color=['red', 'orange', 'yellow', \"green\",'blue', 'purple'],alpha=0.5,width=0.3)\n",
    "    plt.xlabel('Method', fontsize=35)\n",
    "    plt.ylabel('Accuracy', fontsize=35)\n",
    "    plt.yticks(fontsize=30)    \n",
    "    plt.xticks(index, label, fontsize=30, rotation=90)\n",
    "    plt.title('Performance Comparison for each Ensemble Model',fontsize=40)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plot_bar_x()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
