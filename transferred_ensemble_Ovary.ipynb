{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C://test/TC_subsamples_idx12/\"\n",
    "types = [\"OV_Annotation3000_1000_idx12\", \"OV_CV_1000_idx12\", \n",
    "         \"OV_Var_1000_idx12\", \"OV_new_Diff_1000_idx12\",\n",
    "         \"OV_Clin_idx12\", \n",
    "         \"OV_SNV_400_idx12\" \n",
    "         ]\n",
    "\n",
    "file_1 = path+types[0]+\".csv\"\n",
    "file_2 = path+types[1]+\".csv\"\n",
    "file_3 = path+types[2]+\".csv\"\n",
    "file_4 = path+types[3]+\".csv\"\n",
    "file_5 = path+types[4]+\".csv\"\n",
    "file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "sample_1,features_1 = data_1.shape\n",
    "sample_2,features_2 = data_2.shape\n",
    "sample_3,features_3 = data_3.shape\n",
    "sample_4,features_4 = data_4.shape\n",
    "sample_5,features_5 = data_5.shape\n",
    "sample_6,features_6 = data_6.shape\n",
    "\n",
    "# Data frame include index & Platinum_Status column, substract 2 to calculate real number of features \n",
    "[features_1, features_2, features_3, features_4, features_5, features_6] = [features_1-2, features_2-2, features_3-2, features_4-2, features_5-2, features_6-2]\n",
    "\n",
    "print(\"[1] file_name: \", types[0], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "print(\"[2] file_name: \", types[1], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "print(\"[3] file_name: \", types[2], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "print(\"[4] file_name: \", types[3], \"\\nsample : {}  \\nfeatures : {}\".format(sample_4,features_4))\n",
    "print(\"[5] file_name: \", types[4], \"\\nsample : {}  \\nfeatures : {}\".format(sample_5,features_5))\n",
    "print(\"[6] file_name: \", types[5], \"\\nsample : {}  \\nfeatures : {}\".format(sample_6,features_6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[0]+'  ######################################')\n",
    "\n",
    "train_data_1 = data_1.iloc[list(data_1.iloc[:,-1]!=1)]\n",
    "test_data_1 = data_1.iloc[list(data_1.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_1 = train_data_1.Platinum_Status\n",
    "x_val_1 = train_data_1.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_1 = test_data_1.Platinum_Status\n",
    "test_x_val_1 = test_data_1.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_1.shape, test_x_val_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[1]+'  ######################################')\n",
    "\n",
    "train_data_2 = data_2.iloc[list(data_2.iloc[:,-1]!=1)]\n",
    "test_data_2 = data_2.iloc[list(data_2.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_2 = train_data_2.Platinum_Status\n",
    "x_val_2 = train_data_2.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_2 = test_data_2.Platinum_Status\n",
    "test_x_val_2 = test_data_2.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_2.shape, test_x_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[2]+'  ######################################')\n",
    "\n",
    "train_data_3 = data_3.iloc[list(data_3.iloc[:,-1]!=1)]\n",
    "test_data_3 = data_3.iloc[list(data_3.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_3 = train_data_3.Platinum_Status\n",
    "x_val_3 = train_data_3.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_3 = test_data_3.Platinum_Status\n",
    "test_x_val_3 = test_data_3.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_3.shape, test_x_val_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_new_Diff_1000_idx12  ######################################\n",
      "(186, 1000) (31, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[3]+'  ######################################')\n",
    "\n",
    "train_data_4 = data_4.iloc[list(data_4.iloc[:,-1]!=1)]\n",
    "test_data_4 = data_4.iloc[list(data_4.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_4 = train_data_4.Platinum_Status\n",
    "x_val_4 = train_data_4.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_4 = test_data_4.Platinum_Status\n",
    "test_x_val_4 = test_data_4.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_4.shape, test_x_val_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_Clin_idx12  ######################################\n",
      "(256, 35) (31, 35)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[4]+'  ######################################')\n",
    "\n",
    "train_data_5 = data_5.iloc[list(data_5.iloc[:,-1]!=1)]\n",
    "test_data_5 = data_5.iloc[list(data_5.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_5 = train_data_5.Platinum_Status\n",
    "x_val_5 = train_data_5.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_5 = test_data_5.Platinum_Status\n",
    "test_x_val_5 = test_data_5.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_5.shape, test_x_val_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_SNV_400_idx12  ######################################\n",
      "(182, 400) (31, 400)\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[5]+'  ######################################')\n",
    "\n",
    "train_data_6 = data_6.iloc[list(data_6.iloc[:,-1]!=1)]\n",
    "test_data_6 = data_6.iloc[list(data_6.iloc[:,-1]==1)]\n",
    "\n",
    "y_val_6 = train_data_6.Platinum_Status\n",
    "x_val_6 = train_data_6.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "test_y_val_6 = test_data_6.Platinum_Status\n",
    "test_x_val_6 = test_data_6.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "print(x_val_6.shape, test_x_val_6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Seperate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building seperate model for ensemble(model 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000 1000 35 400\n"
     ]
    }
   ],
   "source": [
    "print(features_1, features_2, features_3, features_4, features_5, features_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################  OV_Annotation3000_1000_idx12  ######################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "186/186 [==============================] - 1s 4ms/step - loss: 0.9731 - acc: 0.6935\n",
      "186/186 [==============================] - 0s 408us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6619 - acc: 0.6935\n",
      "186/186 [==============================] - 0s 102us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6822 - acc: 0.6828\n",
      "186/186 [==============================] - 0s 86us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6435 - acc: 0.7097\n",
      "186/186 [==============================] - 0s 113us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6223 - acc: 0.7043\n",
      "186/186 [==============================] - 0s 96us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.7097A: 0s - loss: 0.6592 - acc: 0.722\n",
      "186/186 [==============================] - 0s 123us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6182 - acc: 0.7151\n",
      "186/186 [==============================] - 0s 80us/step\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 1ms/step - loss: 0.6540 - acc: 0.7043\n",
      "186/186 [==============================] - 0s 134us/step\n",
      "186/186 [==============================] - 0s 91us/step\n",
      "31/31 [==============================] - 0s 129us/step\n",
      "Overall AUC:  0.5\n",
      "train AUC:  0.5\n",
      "test AUC:  0.5\n",
      "Train Accuracy: 0.7096774219184794\n",
      "Train Sensitivities & Specificities : 0.0, 1.0\n",
      "Test Accuracy: 0.7096773982048035\n",
      "Test Sensitivities & Specificities : 0.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "print('######################################  '+types[0]+'  ######################################')\n",
    "\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "input_drop_out_1 = 0.1\n",
    "drop_out_1 = 0.3\n",
    "layers = [100, 50, 40, 10]\n",
    "m_1_tr_loss_best = 100 # for saving best loss value \n",
    "best_model_1=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_m_1 = Input(shape=(features_1,))\n",
    "m_1_dp = Dropout(input_drop_out_1)(input_m_1)\n",
    "for i in layers:\n",
    "    m_1 = Dense(i,activation='relu')(m_1_dp)\n",
    "    m_1_dp = Dropout(drop_out_1)(m_1)\n",
    "m_1_final = m_1_dp\n",
    "output_m_1 = Dense(1, activation=\"sigmoid\")(m_1_final)\n",
    "model_1 = Model(inputs=input_m_1,outputs=output_m_1)\n",
    "model_1.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    model_1.fit(x_val_1, y_val_1, batch_size=5, nb_epoch=1)\n",
    "    m_1_tr_loss=model_1.evaluate(x_val_1,y_val_1)[0]\n",
    "    if m_1_tr_loss < m_1_tr_loss_best: # new best model. count reset.\n",
    "        m_1_tr_loss_best = m_1_tr_loss\n",
    "        count=0\n",
    "        best_model_1 = model_1\n",
    "    if count>3: # no increase three time. stop.\n",
    "        model_1 = best_model_1\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_1_tr_loss,m_1_tr_accuracy=model_1.evaluate(x_val_1,y_val_1)\n",
    "m_1_loss,m_1_accuracy= model_1.evaluate(test_x_val_1,test_y_val_1)\n",
    "    \n",
    "    \n",
    "# 4) m_1 train & test prediction table + sensitivity / specificity\n",
    "m_1_tr_predictions = model_1.predict(x_val_1)\n",
    "labeled_m_1_tr_predictions = np.where(m_1_tr_predictions > 0.5, 1, 0).flatten()\n",
    "m_1_tr_sensitivity, m_1_tr_specificity = check_correct(labeled_m_1_tr_predictions, y_val_1)\n",
    "m_1_tr_predictions_flat = m_1_tr_predictions[:,0]\n",
    "\n",
    "m_1_test_predictions = model_1.predict(test_x_val_1)\n",
    "labeled_m_1_test_predictions = np.where(m_1_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_1_sensitivity, m_1_specificity = check_correct(labeled_m_1_test_predictions, test_y_val_1)\n",
    "m_1_test_predictions_flat = m_1_test_predictions[:,0]\n",
    "\n",
    "\n",
    "# 5) m_1 auc\n",
    "tr_df_1 = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(m_1_tr_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_1_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_1.to_csv(\"../result/prediction_result_m_1_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "ts_df_1 = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(m_1_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_1_test_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_1.to_csv(\"../result/prediction_result_m_1_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "y_true = np.append(y_val_1, test_y_val_1)\n",
    "y_pred = np.append(labeled_m_1_tr_predictions, labeled_m_1_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_1, labeled_m_1_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_1, labeled_m_1_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_1_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_1_tr_sensitivity)+\", \"+str(m_1_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_1_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_1_sensitivity)+\", \"+str(m_1_specificity))\n",
    "\n",
    "# 6) save model\n",
    "model_1.save(\"../models/Ovary/m_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[1]+'  ######################################')\n",
    "\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "input_drop_out_2 = 0.1\n",
    "drop_out_2 = 0.3\n",
    "layers = [100, 50, 40, 10]\n",
    "m_2_tr_loss_best = 100 # for saving best loss value \n",
    "best_model_2=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_m_2 = Input(shape=(features_2,))\n",
    "m_2_dp = Dropout(input_drop_out_2)(input_m_2)\n",
    "for i in layers:\n",
    "    m_2 = Dense(i,activation='relu')(m_2_dp)\n",
    "    m_2_dp = Dropout(drop_out_2)(m_2)\n",
    "m_2_final = m_2_dp\n",
    "output_m_2 = Dense(1, activation=\"sigmoid\")(m_2_final)\n",
    "model_2 = Model(inputs=input_m_2,outputs=output_m_2)\n",
    "model_2.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    model_2.fit(x_val_2, y_val_2, batch_size=5, nb_epoch=1)\n",
    "    m_2_tr_loss=model_2.evaluate(x_val_2,y_val_2)[0]\n",
    "    if m_2_tr_loss < m_2_tr_loss_best: # new best model. count reset.\n",
    "        m_2_tr_loss_best = m_2_tr_loss\n",
    "        count=0\n",
    "        best_model_2 = model_2\n",
    "    if count>3: # no increase three time. stop.\n",
    "        model_2 = best_model_2\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_2_tr_loss,m_2_tr_accuracy=model_2.evaluate(x_val_2,y_val_2)\n",
    "m_2_loss,m_2_accuracy= model_2.evaluate(test_x_val_2,test_y_val_2)\n",
    "    \n",
    "    \n",
    "# 4) m_2 train & test prediction table + sensitivity / specificity\n",
    "m_2_tr_predictions = model_2.predict(x_val_2)\n",
    "labeled_m_2_tr_predictions = np.where(m_2_tr_predictions > 0.5, 1, 0).flatten()\n",
    "m_2_tr_sensitivity, m_2_tr_specificity = check_correct(labeled_m_2_tr_predictions, y_val_2)\n",
    "m_2_tr_predictions_flat = m_2_tr_predictions[:,0]\n",
    "\n",
    "m_2_test_predictions = model_2.predict(test_x_val_2)\n",
    "labeled_m_2_test_predictions = np.where(m_2_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_2_sensitivity, m_2_specificity = check_correct(labeled_m_2_test_predictions, test_y_val_2)\n",
    "m_2_test_predictions_flat = m_2_test_predictions[:,0]\n",
    "\n",
    "\n",
    "# 5) m_2 auc\n",
    "tr_df_2 = pd.DataFrame(data={\"patient\":list(train_data_2.index), \"hypothesis 1\": list(m_2_tr_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_2_tr_predictions), \"Platinum_Status\":list(y_val_2)})\n",
    "tr_df_2.to_csv(\"../result/prediction_result_m_2_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "ts_df_2 = pd.DataFrame(data={\"patient\":list(test_data_2.index), \"hypothesis 1\": list(m_2_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_2_test_predictions), \"Platinum_Status\":list(test_y_val_2)})\n",
    "ts_df_2.to_csv(\"../result/prediction_result_m_2_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "y_true = np.append(y_val_2, test_y_val_2)\n",
    "y_pred = np.append(labeled_m_2_tr_predictions, labeled_m_2_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_2, labeled_m_2_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_2, labeled_m_2_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_2_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_2_tr_sensitivity)+\", \"+str(m_2_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_2_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_2_sensitivity)+\", \"+str(m_2_specificity))\n",
    "\n",
    "# 6) save model\n",
    "model_2.save(\"../models/Ovary/m_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[2]+'  ######################################')\n",
    "\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "input_drop_out_3 = 0.1\n",
    "drop_out_3 = 0.3\n",
    "layers = [100, 50, 40, 10]\n",
    "m_3_tr_loss_best = 100 # for saving best loss value \n",
    "best_model_3=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_m_3 = Input(shape=(features_3,))\n",
    "m_3_dp = Dropout(input_drop_out_3)(input_m_3)\n",
    "for i in layers:\n",
    "    m_3 = Dense(i,activation='relu')(m_3_dp)\n",
    "    m_3_dp = Dropout(drop_out_3)(m_3)\n",
    "m_3_final = m_3_dp\n",
    "output_m_3 = Dense(1, activation=\"sigmoid\")(m_3_final)\n",
    "model_3 = Model(inputs=input_m_3,outputs=output_m_3)\n",
    "model_3.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    model_3.fit(x_val_3, y_val_3, batch_size=5, nb_epoch=1)\n",
    "    m_3_tr_loss=model_3.evaluate(x_val_3,y_val_3)[0]\n",
    "    if m_3_tr_loss < m_3_tr_loss_best: # new best model. count reset.\n",
    "        m_3_tr_loss_best = m_3_tr_loss\n",
    "        count=0\n",
    "        best_model_3 = model_3\n",
    "    if count>3: # no increase three time. stop.\n",
    "        model_3 = best_model_3\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_3_tr_loss,m_3_tr_accuracy=model_3.evaluate(x_val_3,y_val_3)\n",
    "m_3_loss,m_3_accuracy= model_3.evaluate(test_x_val_3,test_y_val_3)\n",
    "    \n",
    "    \n",
    "# 4) m_3 train & test prediction table + sensitivity / specificity\n",
    "m_3_tr_predictions = model_3.predict(x_val_3)\n",
    "labeled_m_3_tr_predictions = np.where(m_3_tr_predictions > 0.5, 1, 0).flatten()\n",
    "m_3_tr_sensitivity, m_3_tr_specificity = check_correct(labeled_m_3_tr_predictions, y_val_3)\n",
    "m_3_tr_predictions_flat = m_3_tr_predictions[:,0]\n",
    "\n",
    "m_3_test_predictions = model_3.predict(test_x_val_3)\n",
    "labeled_m_3_test_predictions = np.where(m_3_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_3_sensitivity, m_3_specificity = check_correct(labeled_m_3_test_predictions, test_y_val_3)\n",
    "m_3_test_predictions_flat = m_3_test_predictions[:,0]\n",
    "\n",
    "\n",
    "# 5) m_3 auc\n",
    "tr_df_3 = pd.DataFrame(data={\"patient\":list(train_data_3.index), \"hypothesis 1\": list(m_3_tr_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_3_tr_predictions), \"Platinum_Status\":list(y_val_3)})\n",
    "tr_df_3.to_csv(\"../result/prediction_result_m_3_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "ts_df_3 = pd.DataFrame(data={\"patient\":list(test_data_3.index), \"hypothesis 1\": list(m_3_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_3_test_predictions), \"Platinum_Status\":list(test_y_val_3)})\n",
    "ts_df_3.to_csv(\"../result/prediction_result_m_3_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "y_true = np.append(y_val_3, test_y_val_3)\n",
    "y_pred = np.append(labeled_m_3_tr_predictions, labeled_m_3_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_3, labeled_m_3_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_3, labeled_m_3_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_3_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_3_tr_sensitivity)+\", \"+str(m_3_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_3_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_3_sensitivity)+\", \"+str(m_3_specificity))\n",
    "\n",
    "# 6) save model\n",
    "model_3.save(\"../models/Ovary/m_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('######################################  '+types[3]+'  ######################################')\n",
    "\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "input_drop_out_4 = 0.1\n",
    "drop_out_4 = 0.3\n",
    "layers = [100, 50, 40, 10]\n",
    "m_4_tr_loss_best = 100 # for saving best loss value \n",
    "best_model_4=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_m_4 = Input(shape=(features_4,))\n",
    "m_4_dp = Dropout(input_drop_out_4)(input_m_4)\n",
    "for i in layers:\n",
    "    m_4 = Dense(i,activation='relu')(m_4_dp)\n",
    "    m_4_dp = Dropout(drop_out_4)(m_4)\n",
    "m_4_final = m_4_dp\n",
    "output_m_4 = Dense(1, activation=\"sigmoid\")(m_4_final)\n",
    "model_4 = Model(inputs=input_m_4,outputs=output_m_4)\n",
    "model_4.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    model_4.fit(x_val_4, y_val_4, batch_size=5, nb_epoch=1)\n",
    "    m_4_tr_loss=model_4.evaluate(x_val_4,y_val_4)[0]\n",
    "    if m_4_tr_loss < m_4_tr_loss_best: # new best model. count reset.\n",
    "        m_4_tr_loss_best = m_4_tr_loss\n",
    "        count=0\n",
    "        best_model_4 = model_4\n",
    "    if count>3: # no increase three time. stop.\n",
    "        model_4 = best_model_4\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_4_tr_loss,m_4_tr_accuracy=model_4.evaluate(x_val_4,y_val_4)\n",
    "m_4_loss,m_4_accuracy= model_4.evaluate(test_x_val_4,test_y_val_4)\n",
    "    \n",
    "    \n",
    "# 4) m_4 train & test prediction table + sensitivity / specificity\n",
    "m_4_tr_predictions = model_4.predict(x_val_4)\n",
    "labeled_m_4_tr_predictions = np.where(m_4_tr_predictions > 0.5, 1, 0).flatten()\n",
    "m_4_tr_sensitivity, m_4_tr_specificity = check_correct(labeled_m_4_tr_predictions, y_val_4)\n",
    "m_4_tr_predictions_flat = m_4_tr_predictions[:,0]\n",
    "\n",
    "m_4_test_predictions = model_4.predict(test_x_val_4)\n",
    "labeled_m_4_test_predictions = np.where(m_4_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_4_sensitivity, m_4_specificity = check_correct(labeled_m_4_test_predictions, test_y_val_4)\n",
    "m_4_test_predictions_flat = m_4_test_predictions[:,0]\n",
    "\n",
    "\n",
    "# 5) m_4 auc\n",
    "tr_df_4 = pd.DataFrame(data={\"patient\":list(train_data_4.index), \"hypothesis 1\": list(m_4_tr_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_4_tr_predictions), \"Platinum_Status\":list(y_val_4)})\n",
    "tr_df_4.to_csv(\"../result/prediction_result_m_4_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "ts_df_4 = pd.DataFrame(data={\"patient\":list(test_data_4.index), \"hypothesis 1\": list(m_4_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_4_test_predictions), \"Platinum_Status\":list(test_y_val_4)})\n",
    "ts_df_4.to_csv(\"../result/prediction_result_m_4_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "y_true = np.append(y_val_4, test_y_val_4)\n",
    "y_pred = np.append(labeled_m_4_tr_predictions, labeled_m_4_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_4, labeled_m_4_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_4, labeled_m_4_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_4_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_4_tr_sensitivity)+\", \"+str(m_4_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_4_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_4_sensitivity)+\", \"+str(m_4_specificity))\n",
    "\n",
    "# 6) save model\n",
    "model_4.save(\"../models/Ovary/m_4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[4]+'  ######################################')\n",
    "\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "input_drop_out_5 = 0.1\n",
    "drop_out_5 = 0.3\n",
    "layers = [100, 50, 40, 10]\n",
    "m_5_tr_loss_best = 100 # for saving best loss value \n",
    "best_model_5=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_m_5 = Input(shape=(features_5,))\n",
    "m_5_dp = Dropout(input_drop_out_5)(input_m_5)\n",
    "for i in layers:\n",
    "    m_5 = Dense(i,activation='relu')(m_5_dp)\n",
    "    m_5_dp = Dropout(drop_out_5)(m_5)\n",
    "m_5_final = m_5_dp\n",
    "output_m_5 = Dense(1, activation=\"sigmoid\")(m_5_final)\n",
    "model_5 = Model(inputs=input_m_5,outputs=output_m_5)\n",
    "model_5.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    model_5.fit(x_val_5, y_val_5, batch_size=5, nb_epoch=1)\n",
    "    m_5_tr_loss=model_5.evaluate(x_val_5,y_val_5)[0]\n",
    "    if m_5_tr_loss < m_5_tr_loss_best: # new best model. count reset.\n",
    "        m_5_tr_loss_best = m_5_tr_loss\n",
    "        count=0\n",
    "        best_model_5 = model_5\n",
    "    if count>3: # no increase three time. stop.\n",
    "        model_5 = best_model_5\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_5_tr_loss,m_5_tr_accuracy=model_5.evaluate(x_val_5,y_val_5)\n",
    "m_5_loss,m_5_accuracy= model_5.evaluate(test_x_val_5,test_y_val_5)\n",
    "    \n",
    "    \n",
    "# 4) m_5 train & test prediction table + sensitivity / specificity\n",
    "m_5_tr_predictions = model_5.predict(x_val_5)\n",
    "labeled_m_5_tr_predictions = np.where(m_5_tr_predictions > 0.5, 1, 0).flatten()\n",
    "m_5_tr_sensitivity, m_5_tr_specificity = check_correct(labeled_m_5_tr_predictions, y_val_5)\n",
    "m_5_tr_predictions_flat = m_5_tr_predictions[:,0]\n",
    "\n",
    "m_5_test_predictions = model_5.predict(test_x_val_5)\n",
    "labeled_m_5_test_predictions = np.where(m_5_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_5_sensitivity, m_5_specificity = check_correct(labeled_m_5_test_predictions, test_y_val_5)\n",
    "m_5_test_predictions_flat = m_5_test_predictions[:,0]\n",
    "\n",
    "\n",
    "# 5) m_5 auc\n",
    "tr_df_5 = pd.DataFrame(data={\"patient\":list(train_data_5.index), \"hypothesis 1\": list(m_5_tr_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_5_tr_predictions), \"Platinum_Status\":list(y_val_5)})\n",
    "tr_df_5.to_csv(\"../result/prediction_result_m_5_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "ts_df_5 = pd.DataFrame(data={\"patient\":list(test_data_5.index), \"hypothesis 1\": list(m_5_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_5_test_predictions), \"Platinum_Status\":list(test_y_val_5)})\n",
    "ts_df_5.to_csv(\"../result/prediction_result_m_5_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "y_true = np.append(y_val_5, test_y_val_5)\n",
    "y_pred = np.append(labeled_m_5_tr_predictions, labeled_m_5_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_5, labeled_m_5_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_5, labeled_m_5_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_5_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_5_tr_sensitivity)+\", \"+str(m_5_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_5_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_5_sensitivity)+\", \"+str(m_5_specificity))\n",
    "\n",
    "# 6) save model\n",
    "model_5.save(\"../models/Ovary/m_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('######################################  '+types[5]+'  ######################################')\n",
    "\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "input_drop_out_6 = 0.1\n",
    "drop_out_6 = 0.3\n",
    "layers = [100, 50, 40, 10]\n",
    "m_6_tr_loss_best = 100 # for saving best loss value \n",
    "best_model_6=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_m_6 = Input(shape=(features_6,))\n",
    "m_6_dp = Dropout(input_drop_out_6)(input_m_6)\n",
    "for i in layers:\n",
    "    m_6 = Dense(i,activation='relu')(m_6_dp)\n",
    "    m_6_dp = Dropout(drop_out_6)(m_6)\n",
    "m_6_final = m_6_dp\n",
    "output_m_6 = Dense(1, activation=\"sigmoid\")(m_6_final)\n",
    "model_6 = Model(inputs=input_m_6,outputs=output_m_6)\n",
    "model_6.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    model_6.fit(x_val_6, y_val_6, batch_size=5, nb_epoch=1)\n",
    "    m_6_tr_loss=model_6.evaluate(x_val_6,y_val_6)[0]\n",
    "    if m_6_tr_loss < m_6_tr_loss_best: # new best model. count reset.\n",
    "        m_6_tr_loss_best = m_6_tr_loss\n",
    "        count=0\n",
    "        best_model_6 = model_6\n",
    "    if count>3: # no increase three time. stop.\n",
    "        model_6 = best_model_6\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "m_6_tr_loss,m_6_tr_accuracy=model_6.evaluate(x_val_6,y_val_6)\n",
    "m_6_loss,m_6_accuracy= model_6.evaluate(test_x_val_6,test_y_val_6)\n",
    "    \n",
    "    \n",
    "# 4) m_6 train & test prediction table + sensitivity / specificity\n",
    "m_6_tr_predictions = model_6.predict(x_val_6)\n",
    "labeled_m_6_tr_predictions = np.where(m_6_tr_predictions > 0.5, 1, 0).flatten()\n",
    "m_6_tr_sensitivity, m_6_tr_specificity = check_correct(labeled_m_6_tr_predictions, y_val_6)\n",
    "m_6_tr_predictions_flat = m_6_tr_predictions[:,0]\n",
    "\n",
    "m_6_test_predictions = model_6.predict(test_x_val_6)\n",
    "labeled_m_6_test_predictions = np.where(m_6_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_6_sensitivity, m_6_specificity = check_correct(labeled_m_6_test_predictions, test_y_val_6)\n",
    "m_6_test_predictions_flat = m_6_test_predictions[:,0]\n",
    "\n",
    "\n",
    "# 5) m_6 auc\n",
    "tr_df_6 = pd.DataFrame(data={\"patient\":list(train_data_6.index), \"hypothesis 1\": list(m_6_tr_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_6_tr_predictions), \"Platinum_Status\":list(y_val_6)})\n",
    "tr_df_6.to_csv(\"../result/prediction_result_m_6_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "ts_df_6 = pd.DataFrame(data={\"patient\":list(test_data_6.index), \"hypothesis 1\": list(m_6_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_m_6_test_predictions), \"Platinum_Status\":list(test_y_val_6)})\n",
    "ts_df_6.to_csv(\"../result/prediction_result_m_6_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "y_true = np.append(y_val_6, test_y_val_6)\n",
    "y_pred = np.append(labeled_m_6_tr_predictions, labeled_m_6_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_6, labeled_m_6_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_6, labeled_m_6_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_6_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_6_tr_sensitivity)+\", \"+str(m_6_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_6_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_6_sensitivity)+\", \"+str(m_6_specificity))\n",
    "\n",
    "# 6) save model\n",
    "model_6.save(\"../models/Ovary/m_6.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seperate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"< \"+types[0]+\" > tr: \"+str(m_1_tr_accuracy)+\", ts: \"+str(m_1_accuracy))\n",
    "print(\"< \"+types[1]+\" > tr: \"+str(m_2_tr_accuracy)+\", ts: \"+str(m_2_accuracy))\n",
    "print(\"< \"+types[2]+\" > tr: \"+str(m_3_tr_accuracy)+\", ts: \"+str(m_3_accuracy))\n",
    "print(\"< \"+types[3]+\" > tr: \"+str(m_4_tr_accuracy)+\", ts: \"+str(m_4_accuracy))\n",
    "print(\"< \"+types[4]+\" > tr: \"+str(m_5_tr_accuracy)+\", ts: \"+str(m_5_accuracy))\n",
    "print(\"< \"+types[5]+\" > tr: \"+str(m_6_tr_accuracy)+\", ts: \"+str(m_6_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select models & using saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 4, 4, 4, 4]\n",
      "using saved model?: y\n",
      "######################################### Using saving models #########################################\n",
      "[1] file_name:  inter_OV_Annotation3000_1000 \n",
      "sample : 153  \n",
      "features : 1000\n",
      "[2] file_name:  inter_OV_CV_1000 \n",
      "sample : 153  \n",
      "features : 1000\n",
      "[3] file_name:  inter_OV_Var_1000 \n",
      "sample : 153  \n",
      "features : 1000\n",
      "[4] file_name:  inter_OV_new_Diff_1000 \n",
      "sample : 153  \n",
      "features : 1000\n",
      "[5] file_name:  inter_OV_Clin \n",
      "sample : 153  \n",
      "features : 35\n",
      "[6] file_name:  inter_OV_SNV_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "122/122 [==============================] - 1s 11ms/step\n",
      "31/31 [==============================] - 0s 506us/step\n",
      "Train Accuracy: 0.7213114724784601\n",
      "Test Accuracy: 0.7096773982048035\n",
      "122/122 [==============================] - 1s 10ms/step\n",
      "31/31 [==============================] - 0s 244us/step\n",
      "Train Accuracy: 0.6721311455867329\n",
      "Test Accuracy: 0.5806451439857483\n",
      "122/122 [==============================] - 1s 11ms/step\n",
      "31/31 [==============================] - 0s 285us/step\n",
      "Train Accuracy: 0.7213114724784601\n",
      "Test Accuracy: 0.7096773982048035\n",
      "122/122 [==============================] - 1s 11ms/step\n",
      "31/31 [==============================] - 0s 326us/step\n",
      "Train Accuracy: 0.7213114724784601\n",
      "Test Accuracy: 0.7096773982048035\n",
      "122/122 [==============================] - 2s 13ms/step\n",
      "31/31 [==============================] - 0s 187us/step\n",
      "Train Accuracy: 0.8278688475733897\n",
      "Test Accuracy: 0.6451612710952759\n",
      "122/122 [==============================] - 1s 12ms/step\n",
      "31/31 [==============================] - 0s 152us/step\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.6451612710952759\n"
     ]
    }
   ],
   "source": [
    "select = [1, 2, 3, 4, 4, 4, 4, 4]\n",
    "print(select)\n",
    "ch = input(\"using saved model?: \")\n",
    "\n",
    "if ch=='y':\n",
    "    print(\"######################################### Using saving models #########################################\")\n",
    "    \n",
    "    path = \"C://test/TC_intersect_subsamples/\"\n",
    "    types = [\"inter_OV_Annotation3000_1000\", \"inter_OV_CV_1000\", \n",
    "             \"inter_OV_Var_1000\", \"inter_OV_new_Diff_1000\",\n",
    "             \"inter_OV_Clin\", \n",
    "             \"inter_OV_SNV_400\" \n",
    "             ]\n",
    "\n",
    "    file_1 = path+types[0]+\".csv\"\n",
    "    file_2 = path+types[1]+\".csv\"\n",
    "    file_3 = path+types[2]+\".csv\"\n",
    "    file_4 = path+types[3]+\".csv\"\n",
    "    file_5 = path+types[4]+\".csv\"\n",
    "    file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "    idx_col = 0\n",
    "\n",
    "    data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "    data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "    data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "    data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "    data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "    data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "    sample_1,features_1 = data_1.shape\n",
    "    sample_2,features_2 = data_2.shape\n",
    "    sample_3,features_3 = data_3.shape\n",
    "    sample_4,features_4 = data_4.shape\n",
    "    sample_5,features_5 = data_5.shape\n",
    "    sample_6,features_6 = data_6.shape\n",
    "\n",
    "    # Data frame include index & Platinum_Status column, substract 2 to calculate real number of features \n",
    "    [features_1, features_2, features_3, features_4, features_5, features_6] = [features_1-2, features_2-2, features_3-2, features_4-2, features_5-2, features_6-2]\n",
    "\n",
    "    print(\"[1] file_name: \", types[0], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "    print(\"[2] file_name: \", types[1], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "    print(\"[3] file_name: \", types[2], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "    print(\"[4] file_name: \", types[3], \"\\nsample : {}  \\nfeatures : {}\".format(sample_4,features_4))\n",
    "    print(\"[5] file_name: \", types[4], \"\\nsample : {}  \\nfeatures : {}\".format(sample_5,features_5))\n",
    "    print(\"[6] file_name: \", types[5], \"\\nsample : {}  \\nfeatures : {}\".format(sample_6,features_6))\n",
    "    \n",
    "    train_data_1 = data_1.iloc[list(data_1.iloc[:,-1]!=1)]\n",
    "    test_data_1 = data_1.iloc[list(data_1.iloc[:,-1]==1)]\n",
    "\n",
    "    y_val_1 = train_data_1.Platinum_Status\n",
    "    x_val_1 = train_data_1.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val_1 = test_data_1.Platinum_Status\n",
    "    test_x_val_1 = test_data_1.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    train_data_2 = data_2.iloc[list(data_2.iloc[:,-1]!=1)]\n",
    "    test_data_2 = data_2.iloc[list(data_2.iloc[:,-1]==1)]\n",
    "\n",
    "    y_val_2 = train_data_2.Platinum_Status\n",
    "    x_val_2 = train_data_2.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val_2 = test_data_2.Platinum_Status\n",
    "    test_x_val_2 = test_data_2.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    train_data_3 = data_3.iloc[list(data_3.iloc[:,-1]!=1)]\n",
    "    test_data_3 = data_3.iloc[list(data_3.iloc[:,-1]==1)]\n",
    "\n",
    "    y_val_3 = train_data_3.Platinum_Status\n",
    "    x_val_3 = train_data_3.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val_3 = test_data_3.Platinum_Status\n",
    "    test_x_val_3 = test_data_3.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "\n",
    "    train_data_4 = data_4.iloc[list(data_4.iloc[:,-1]!=1)]\n",
    "    test_data_4 = data_4.iloc[list(data_4.iloc[:,-1]==1)]\n",
    "\n",
    "    y_val_4 = train_data_4.Platinum_Status\n",
    "    x_val_4 = train_data_4.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val_4 = test_data_4.Platinum_Status\n",
    "    test_x_val_4 = test_data_4.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    train_data_5 = data_5.iloc[list(data_5.iloc[:,-1]!=1)]\n",
    "    test_data_5 = data_5.iloc[list(data_5.iloc[:,-1]==1)]\n",
    "\n",
    "    y_val_5 = train_data_5.Platinum_Status\n",
    "    x_val_5 = train_data_5.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val_5 = test_data_5.Platinum_Status\n",
    "    test_x_val_5 = test_data_5.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    train_data_6 = data_6.iloc[list(data_6.iloc[:,-1]!=1)]\n",
    "    test_data_6 = data_6.iloc[list(data_6.iloc[:,-1]==1)]\n",
    "\n",
    "    y_val_6 = train_data_6.Platinum_Status\n",
    "    x_val_6 = train_data_6.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val_6 = test_data_6.Platinum_Status\n",
    "    test_x_val_6 = test_data_6.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    model_1_l = load_model(\"../models/Ovary/m_1.h5\")\n",
    "    m_1_l_tr_loss,m_1_l_tr_accuracy=model_1_l.evaluate(x_val_1,y_val_1)\n",
    "    m_1_l_loss,m_1_l_accuracy= model_1_l.evaluate(test_x_val_1,test_y_val_1)\n",
    "    model_1_l_new = Model(inputs = model_1_l.input, outputs=model_1_l.get_layer(model_1_l.layers[-2].name).output)\n",
    "    print(\"Train Accuracy: {}\".format(m_1_l_tr_accuracy))\n",
    "    print(\"Test Accuracy: {}\".format(m_1_l_accuracy))\n",
    "\n",
    "    model_2_l = load_model(\"../models/Ovary/m_2.h5\")\n",
    "    m_2_l_tr_loss,m_2_l_tr_accuracy=model_2_l.evaluate(x_val_2,y_val_2)\n",
    "    m_2_l_loss,m_2_l_accuracy= model_2_l.evaluate(test_x_val_2,test_y_val_2)\n",
    "    model_2_l_new = Model(inputs = model_2_l.input, outputs=model_2_l.get_layer(model_2_l.layers[-2].name).output)\n",
    "    print(\"Train Accuracy: {}\".format(m_2_l_tr_accuracy))\n",
    "    print(\"Test Accuracy: {}\".format(m_2_l_accuracy))\n",
    "\n",
    "    model_3_l = load_model(\"../models/Ovary/m_3.h5\")\n",
    "    m_3_l_tr_loss,m_3_l_tr_accuracy=model_3_l.evaluate(x_val_3,y_val_3)\n",
    "    m_3_l_loss,m_3_l_accuracy= model_3_l.evaluate(test_x_val_3,test_y_val_3)\n",
    "    model_3_l_new = Model(inputs = model_3_l.input, outputs=model_3_l.get_layer(model_3_l.layers[-2].name).output)\n",
    "    print(\"Train Accuracy: {}\".format(m_3_l_tr_accuracy))\n",
    "    print(\"Test Accuracy: {}\".format(m_3_l_accuracy))\n",
    "\n",
    "    model_4_l = load_model(\"../models/Ovary/m_4.h5\")\n",
    "    m_4_l_tr_loss,m_4_l_tr_accuracy=model_4_l.evaluate(x_val_4,y_val_4)\n",
    "    m_4_l_loss,m_4_l_accuracy= model_4_l.evaluate(test_x_val_4,test_y_val_4)\n",
    "    model_4_l_new = Model(inputs = model_4_l.input, outputs=model_4_l.get_layer(model_4_l.layers[-2].name).output)\n",
    "    print(\"Train Accuracy: {}\".format(m_4_l_tr_accuracy))\n",
    "    print(\"Test Accuracy: {}\".format(m_4_l_accuracy))\n",
    "\n",
    "    model_5_l = load_model(\"../models/Ovary/m_5.h5\")\n",
    "    m_5_l_tr_loss,m_5_l_tr_accuracy=model_5_l.evaluate(x_val_5,y_val_5)\n",
    "    m_5_l_loss,m_5_l_accuracy= model_5_l.evaluate(test_x_val_5,test_y_val_5)\n",
    "    model_5_l_new = Model(inputs = model_5_l.input, outputs=model_5_l.get_layer(model_5_l.layers[-2].name).output)\n",
    "    print(\"Train Accuracy: {}\".format(m_5_l_tr_accuracy))\n",
    "    print(\"Test Accuracy: {}\".format(m_5_l_accuracy))\n",
    "\n",
    "    model_6_l = load_model(\"../models/Ovary/m_6.h5\")\n",
    "    m_6_l_tr_loss,m_6_l_tr_accuracy=model_6_l.evaluate(x_val_6,y_val_6)\n",
    "    m_6_l_loss,m_6_l_accuracy= model_6_l.evaluate(test_x_val_6,test_y_val_6)\n",
    "    model_6_l_new = Model(inputs = model_6_l.input, outputs=model_6_l.get_layer(model_6_l.layers[-2].name).output)\n",
    "    print(\"Train Accuracy: {}\".format(m_6_l_tr_accuracy))\n",
    "    print(\"Test Accuracy: {}\".format(m_6_l_accuracy))\n",
    "    \n",
    "    m_1_predictions = model_1_l.predict(x_val_1)\n",
    "    m_2_predictions = model_2_l.predict(x_val_2)\n",
    "    m_3_predictions = model_3_l.predict(x_val_3)\n",
    "    m_4_predictions = model_4_l.predict(x_val_4)\n",
    "    m_5_predictions = model_5_l.predict(x_val_5)\n",
    "    m_6_predictions = model_6_l.predict(x_val_6)\n",
    "    \n",
    "    m_1_test_predictions = model_1_l.predict(test_x_val_1)\n",
    "    m_2_test_predictions = model_2_l.predict(test_x_val_2)\n",
    "    m_3_test_predictions = model_3_l.predict(test_x_val_3)\n",
    "    m_4_test_predictions = model_4_l.predict(test_x_val_4)\n",
    "    m_5_test_predictions = model_5_l.predict(test_x_val_5)\n",
    "    m_6_test_predictions = model_6_l.predict(test_x_val_6)\n",
    "    \n",
    "    \n",
    "elif ch=='n': \n",
    "    print(\"######################################### Using recently trained models #########################################\")\n",
    "    \n",
    "    m_1_predictions = model_1.predict(x_val_1)\n",
    "    m_2_predictions = model_2.predict(x_val_2)\n",
    "    m_3_predictions = model_3.predict(x_val_3)\n",
    "    m_4_predictions = model_4.predict(x_val_4)\n",
    "    m_5_predictions = model_5.predict(x_val_5)\n",
    "    m_6_predictions = model_6.predict(x_val_6)\n",
    "    \n",
    "    m_1_test_predictions = model_1.predict(test_x_val_1)\n",
    "    m_2_test_predictions = model_2.predict(test_x_val_2)\n",
    "    m_3_test_predictions = model_3.predict(test_x_val_3)\n",
    "    m_4_test_predictions = model_4.predict(test_x_val_4)\n",
    "    m_5_test_predictions = model_5.predict(test_x_val_5)\n",
    "    m_6_test_predictions = model_6.predict(test_x_val_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building original ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################## DNN Ensemble ##################################\n",
      "[1, 2, 3, 4, 4, 4, 4, 4]\n",
      "inter_OV_Annotation3000_1000\n",
      "inter_OV_CV_1000\n",
      "inter_OV_Var_1000\n",
      "inter_OV_new_Diff_1000\n",
      "inter_OV_new_Diff_1000\n",
      "inter_OV_new_Diff_1000\n",
      "inter_OV_new_Diff_1000\n",
      "inter_OV_new_Diff_1000\n",
      "#############################################################################################\n",
      "(122, 1)\n",
      "(122, 1)\n",
      "(122, 1)\n",
      "(122, 1)\n",
      "(122, 1)\n",
      "(122, 1)\n",
      "(122, 1)\n",
      "(122, 1)\n",
      "Epoch 1/5\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 0.6338 - acc: 0.7213\n",
      "Epoch 2/5\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6141 - acc: 0.7213\n",
      "Epoch 3/5\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6050 - acc: 0.7213\n",
      "Epoch 4/5\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5998 - acc: 0.7213\n",
      "Epoch 5/5\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5978 - acc: 0.7213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272709ecf60>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"################################## DNN Ensemble ##################################\")\n",
    "print(select)\n",
    "adam = optimizers.Adam()\n",
    "for type_i in select:\n",
    "    print(types[type_i-1])\n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "m_predictions = [m_1_predictions, m_2_predictions, m_3_predictions, m_4_predictions, m_5_predictions, m_6_predictions]\n",
    "m_predictions_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_predictions_select.append(m_predictions[select[i]-1])\n",
    "    print(m_predictions[select[i]-1].shape)\n",
    "    \n",
    "ensemble_x_val = np.concatenate(m_predictions_select, axis=1)\n",
    "\n",
    "ensemble_model_input = Input(shape = (len(select),))\n",
    "ensemble_m_h1 = Dense(2, activation=\"relu\")(ensemble_model_input)\n",
    "ensemble_model_output = Dense(1,activation='sigmoid')(ensemble_m_h1)\n",
    "ensemble_model = Model(inputs= ensemble_model_input, outputs = ensemble_model_output)\n",
    "\n",
    "ensemble_model.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "ensemble_model.fit(ensemble_x_val, y_val_1, epochs=5, batch_size= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _DNN Combiner_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 2s 17ms/step\n",
      "31/31 [==============================] - 0s 156us/step\n",
      "Overall AUC:  0.5\n",
      "train AUC:  0.5\n",
      "test AUC:  0.5\n",
      "Train Accuracy: 0.7213114724784601\n",
      "Train Sensitivities & Specificities : 0.0, 1.0\n",
      "Test Accuracy: 0.7096773982048035\n",
      "Test Sensitivities & Specificities : 0.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m_test_predictions = [m_1_test_predictions, m_2_test_predictions, m_3_test_predictions, m_4_test_predictions, m_5_test_predictions, m_6_test_predictions]\n",
    "m_test_predictions_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_test_predictions_select.append(m_test_predictions[select[i]-1])\n",
    "\n",
    "ensemble_test_x_val = np.concatenate(m_test_predictions_select, axis=1)\n",
    "\n",
    "em_tr_loss,em_tr_accuracy= ensemble_model.evaluate(ensemble_x_val,y_val_1)\n",
    "em_loss,em_accuracy= ensemble_model.evaluate(ensemble_test_x_val,test_y_val_1)\n",
    "\n",
    "ensemble_tr_predictions = ensemble_model.predict(ensemble_x_val)\n",
    "labeled_ensemble_tr_predictions = np.where(ensemble_tr_predictions > 0.5, 1, 0).flatten()\n",
    "ensemble_tr_sensitivity, ensemble_tr_specificity = check_correct(labeled_ensemble_tr_predictions, y_val_1)\n",
    "ensemble_tr_predictions_flat = ensemble_tr_predictions[:,0]\n",
    "\n",
    "ensemble_test_predictions = ensemble_model.predict(ensemble_test_x_val)\n",
    "labeled_ensemble_test_predictions = np.where(ensemble_test_predictions > 0.5, 1, 0).flatten()\n",
    "ensemble_sensitivity, ensemble_specificity = check_correct(labeled_ensemble_test_predictions, test_y_val_1)\n",
    "ensemble_test_predictions_flat = ensemble_test_predictions[:,0]\n",
    "\n",
    "# 5) m_em auc\n",
    "tr_df_em = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(ensemble_tr_predictions_flat), \n",
    "                        \"prediction\":list(labeled_ensemble_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_em.to_csv(\"../result/prediction_result_m_em_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "ts_df_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(ensemble_test_predictions_flat), \n",
    "                        \"prediction\":list(labeled_ensemble_test_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_em.to_csv(\"../result/prediction_result_m_em_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "y_true = np.append(y_val_1, test_y_val_1)\n",
    "y_pred = np.append(labeled_ensemble_tr_predictions, labeled_ensemble_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_1, labeled_ensemble_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_1, labeled_ensemble_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(ensemble_tr_sensitivity)+\", \"+str(ensemble_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(em_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(ensemble_sensitivity)+\", \"+str(ensemble_specificity))\n",
    "\n",
    "# 6) save model\n",
    "ensemble_model.save(\"../models/Ovary/m_em.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _mean_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for mean ensemble : 0.7377049180327869\n",
      "Train Sensitivities & Specificities : 0.20588235294117646, 0.9431818181818182\n",
      "Test Accuracy for mean ensemble : 0.6774193548387096\n",
      "Test Sensitivities & Specificities : 0.0, 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "mean_tr_predictions=sum(m_predictions_select)/len(select)\n",
    "mean_em_labeled_tr_predictions = np.where(mean_tr_predictions > 0.5, 1, 0).flatten()\n",
    "mean_em_tr_accuracy = sum(mean_em_labeled_tr_predictions==y_val_1.values)/len(y_val_1)\n",
    "mean_ensemble_tr_sensitivity, mean_ensemble_tr_specificity = check_correct(mean_em_labeled_tr_predictions, y_val_1)\n",
    "mean_ensemble_tr_predictions_flat = mean_tr_predictions[:,0]\n",
    "\n",
    "mean_predictions=sum(m_test_predictions_select)/len(select)\n",
    "mean_em_labeled_predictions = np.where(mean_predictions > 0.5, 1, 0).flatten()\n",
    "mean_em_accuracy = sum(mean_em_labeled_predictions==test_y_val_1.values)/len(test_y_val_1)\n",
    "mean_ensemble_sensitivity, mean_ensemble_specificity = check_correct(mean_em_labeled_predictions, test_y_val_1)\n",
    "mean_ensemble_test_predictions_flat = mean_predictions[:,0]\n",
    "\n",
    "ts_df_em_mean = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis_1\": list(mean_ensemble_test_predictions_flat), \n",
    "  \"prediction\":list(mean_em_labeled_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_em_mean.to_csv(\"../result/prediction_result_EM_mean.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "\n",
    "y_true = np.append(y_val_em, test_y_val_em)\n",
    "y_pred = np.append(labeled_m_em_tr_predictions, labeled_m_em_test_predictions)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "fpr_train, tpr_train, threshold = metrics.roc_curve(y_val_em, labeled_m_em_tr_predictions)\n",
    "fpr_test, tpr_test, threshold = metrics.roc_curve(test_y_val_em, labeled_m_em_test_predictions)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "\n",
    "print(\"Overall AUC: \", roc_auc)\n",
    "print(\"train AUC: \", roc_auc_train)\n",
    "print(\"test AUC: \", roc_auc_test)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(m_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(m_em_tr_sensitivity)+\", \"+str(m_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(m_em_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(m_em_sensitivity)+\", \"+str(m_em_specificity))\n",
    "\n",
    "\n",
    "print(\"Train Accuracy for mean ensemble : {}\".format(mean_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mean_ensemble_tr_sensitivity)+\", \"+str(mean_ensemble_tr_specificity))\n",
    "print(\"Test Accuracy for mean ensemble : {}\".format(mean_em_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mean_ensemble_sensitivity)+\", \"+str(mean_ensemble_specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferred Ensemble Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making new input data for t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=[input_m_1], outputs=[m_1_final])\n",
    "results_m_1 = model.predict([x_val_1])\n",
    "\n",
    "model = Model(inputs=[input_m_2], outputs=[m_2_final])\n",
    "results_m_2 = model.predict([x_val_2])\n",
    "\n",
    "model = Model(inputs=[input_m_3], outputs=[m_3_final])\n",
    "results_m_3 = model.predict([x_val_3])\n",
    "\n",
    "model = Model(inputs=[input_m_4], outputs=[m_4_final])\n",
    "results_m_4 = model.predict([x_val_4])\n",
    "\n",
    "model = Model(inputs=[input_m_5], outputs=[m_5_final])\n",
    "results_m_5 = model.predict([x_val_5])\n",
    "\n",
    "model = Model(inputs=[input_m_6], outputs=[m_6_final])\n",
    "results_m_6 = model.predict([x_val_6])\n",
    "\n",
    "results_m_sum = [results_m_1, results_m_2, results_m_3, results_m_4, results_m_5, results_m_6]\n",
    "results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    results_m_select.append(results_m_sum[select[i]-1])\n",
    "\n",
    "t_ensemble_x_val = np.concatenate(results_m_select, axis=1)\n",
    "print(t_ensemble_x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling t-ensemble  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ensemble_input = Input(shape=(t_ensemble_x_val.shape[1],))\n",
    "t_ensemble_h1 = Dense(20,activation='relu')(t_ensemble_input)\n",
    "t_ensemble_h2 = Dense(10,activation='relu')(t_ensemble_h1)\n",
    "t_ensemble_h3 = Dense(5,activation='relu')(t_ensemble_h2)\n",
    "t_ensemble_output = Dense(1,activation='sigmoid')(t_ensemble_h3)\n",
    "\n",
    "t_ensemble_model = Model(inputs=[t_ensemble_input],outputs=[t_ensemble_output])\n",
    "t_ensemble_model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "t_ensemble_model.fit(t_ensemble_x_val, y_val_1, epochs=2,batch_size=5)\n",
    "ensemble_model.save(\"../models/Ovary/t_EM_DNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_m_1], outputs=[m_1_final])\n",
    "test_results_m_1 = model.predict([test_x_val_1])\n",
    "\n",
    "model = Model(inputs=[input_m_2], outputs=[m_2_final])\n",
    "test_results_m_2 = model.predict([test_x_val_2])\n",
    "\n",
    "model = Model(inputs=[input_m_3], outputs=[m_3_final])\n",
    "test_results_m_3 = model.predict([test_x_val_3])\n",
    "\n",
    "model = Model(inputs=[input_m_4], outputs=[m_4_final])\n",
    "test_results_m_4 = model.predict([test_x_val_4])\n",
    "\n",
    "model = Model(inputs=[input_m_5], outputs=[m_5_final])\n",
    "test_results_m_5 = model.predict([test_x_val_5])\n",
    "\n",
    "model = Model(inputs=[input_m_6], outputs=[m_6_final])\n",
    "test_results_m_6 = model.predict([test_x_val_6])\n",
    "\n",
    "test_results_m_sum = [test_results_m_1, test_results_m_2, test_results_m_3, test_results_m_4, test_results_m_5, test_results_m_6]\n",
    "test_results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    test_results_m_select.append(test_results_m_sum[select[i]-1])\n",
    "\n",
    "t_ensemble_test_x_val = np.concatenate(test_results_m_select, axis=1)\n",
    "t_em_tr_accuracy = t_ensemble_model.evaluate(t_ensemble_x_val,y_val_1)[1]\n",
    "t_em_accuracy = t_ensemble_model.evaluate(t_ensemble_test_x_val,test_y_val_1)[1]\n",
    "\n",
    "t_ensemble_predictions = t_ensemble_model.predict(t_ensemble_x_val)\n",
    "labeled_t_ensemble_predictions = np.where(t_ensemble_predictions > 0.5, 1, 0).flatten()\n",
    "t_ensemble_tr_sensitivity, t_ensemble_tr_specificity = check_correct(labeled_t_ensemble_predictions, y_val_1)\n",
    "\n",
    "t_ensemble_test_predictions = t_ensemble_model.predict(t_ensemble_test_x_val)\n",
    "labeled_t_ensemble_test_predictions = np.where(t_ensemble_test_predictions > 0.5, 1, 0).flatten()\n",
    "t_ensemble_sensitivity, t_ensemble_specificity = check_correct(labeled_t_ensemble_test_predictions, test_y_val_1)\n",
    "\n",
    "t_ensemble_test_predictions_flat = t_ensemble_test_predictions[:,0]\n",
    "\n",
    "df_t_EM = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis_1\": list(t_ensemble_test_predictions_flat), \n",
    "  \"prediction\":list(labeled_t_ensemble_test_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "df_t_EM.to_csv(\"../result/prediction_result_EM_t.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis_1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(t_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(t_ensemble_tr_sensitivity)+\", \"+str(t_ensemble_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(t_em_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(t_ensemble_sensitivity)+\", \"+str(t_ensemble_specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "tr_accuracy_list = [m_1_tr_accuracy, m_2_tr_accuracy, m_3_tr_accuracy, m_4_tr_accuracy, m_5_tr_accuracy, m_6_tr_accuracy]\n",
    "ts_accuracy_list = [m_1_accuracy, m_2_accuracy, m_3_accuracy, m_4_accuracy, m_5_accuracy, m_6_accuracy]\n",
    "tr_accuracy_select = []\n",
    "ts_accuracy_select = []\n",
    "\n",
    "for i in select:\n",
    "    label.append(\"model\"+str(i))\n",
    "    tr_accuracy_select.append(tr_accuracy_list[i-1])\n",
    "    ts_accuracy_select.append(ts_accuracy_list[i-1])\n",
    "\n",
    "label = label+[\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "tr_accuracy_select= tr_accuracy_select + [mean_em_tr_accuracy, em_tr_accuracy, t_em_tr_accuracy]\n",
    "ts_accuracy_select= ts_accuracy_select + [mean_em_accuracy, em_accuracy, t_em_accuracy]\n",
    "\n",
    "for model_num in range(len(label)):\n",
    "    print(\"< \"+label[model_num]+\" > tr: \"+str(tr_accuracy_select[model_num])+\", ts: \"+str(ts_accuracy_select[model_num]))\n",
    "\n",
    "#label = [\"model1\",\"model2\",\"model3\",\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "#accuracy = [m1_accuracy,m2_accuracy,m3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy ]\n",
    "#print(\"model1: \"+str(accuracy[0])+\"\\nmodel2: \"+str(accuracy[1])+\"\\nmodel3: \"+str(accuracy[2])+\"\\nmean-em: \"+str(accuracy[3])+\"\\nd-comb em: \"+str(accuracy[4])+\"\\nt-em: \"+str(accuracy[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot_bar_x():\n",
    "    # this is for plotting purpose\n",
    "    plt.figure(figsize=(30,20))\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([min(m_1_accuracy,m_2_accuracy,m_3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy)-0.02,1])\n",
    "    index = np.arange(len(label))\n",
    "    plt.bar(index, accuracy,color=['red', 'orange', 'yellow', \"green\",'blue', 'purple'],alpha=0.5,width=0.3)\n",
    "    plt.xlabel('Method', fontsize=35)\n",
    "    plt.ylabel('Accuracy', fontsize=35)\n",
    "    plt.yticks(fontsize=30)    \n",
    "    plt.xticks(index, label, fontsize=30, rotation=90)\n",
    "    plt.title('Performance Comparison for each Ensemble Model',fontsize=40)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plot_bar_x()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
