{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model, Sequential \n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide raw data into train / test & x_val / y_val\n",
    "def data_split(raw_data, index_col, test_index):\n",
    "    \n",
    "    train_data = raw_data.iloc[list(raw_data.iloc[:,index_col]!=test_index)]\n",
    "    test_data = raw_data.iloc[list(raw_data.iloc[:,index_col]==test_index)]\n",
    "    \n",
    "    y_val = train_data.Platinum_Status\n",
    "    x_val = train_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val = test_data.Platinum_Status\n",
    "    test_x_val = test_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    return train_data, test_data, y_val, x_val, test_y_val, test_x_val\n",
    "\n",
    "    # raw_data: have gene_expressions(maybe multiple columns), index column, Platinum_Status column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all of model performance \n",
    "# - predictions(probability) / labeled predictions(0/1) / Loss / Accuracy / Sensitivity / Specificity / AUC values of Train / Test dataset.\n",
    "# using trained models, or you can put predictions(probability) passively(in this case, Loss & Accuracy do not provided.)\n",
    "def model_performance(information=False, Input_Prediction_Passively=False, using_model=None, tr_predictions=None, ts_predictions=None, tr_x_val=None, tr_y_val=None, ts_x_val=None, ts_y_val=None, output_list=None):\n",
    "    \n",
    "    if information == True:            \n",
    "        print(\"options model_performance:\\n1) using_model: keras models that you want to check performance. \\\"Input_Prediction_Passive\\\" option for input prediction list instead using models.\\n3) tr_predictions & ts_predictions: prediction input passively. put this data only when not using keras model.\\n4) tr_x_val & ts_x_val: input samples of train/test samples.\\n4) tr_y_val & ts_y_val: results of train/test samples.\\n5) output_list: return values that you want to recieve.\\n CAUTION: Essential variable.\\n\\t tr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\\n\\n* CAUTION: if 'None' value is returned, please check your input tr inputs(None value for tr outputs) or ts inputs(None value for ts outputs).\") \n",
    "        return 0\n",
    "    elif information != False:\n",
    "        print(\"for using information options, please set 'information' variable for 'True'\")\n",
    "        return -1\n",
    "    \n",
    "    if using_model is None:\n",
    "        if Input_Prediction_Passively == False:\n",
    "            print(\"ERROR: There are no models for using.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\") \n",
    "            return -1\n",
    "        elif (tr_predictions is None) and (ts_predictions is None): # No model/prediction input. no performance should be calculated.\n",
    "                print(\"ERROR: Input prediction list instead using saved model.\")\n",
    "                return -1\n",
    "        else: # No model input, but Input_Prediction_Passively is True & input prediction is valid.\n",
    "            tr_loss,tr_accuracy= None, None\n",
    "            ts_loss,ts_accuracy= None, None\n",
    "            \n",
    "    elif Input_Prediction_Passively == True: # both of model/prediction putted, could cause confusing.\n",
    "        ch = input(\"You put both model and prediction. Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "        while 1:\n",
    "            if ch == 'p':\n",
    "                using_model = None\n",
    "                break\n",
    "            elif ch == 'm':\n",
    "                tr_predictions = None\n",
    "                ts_predictions = None\n",
    "                break\n",
    "            elif ch == 'e':\n",
    "                return 0\n",
    "            else:\n",
    "                print(\"you put worng option: \"+str(ch))\n",
    "            ch = input(\"Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "                \n",
    "    if output_list is None:\n",
    "        print(\"ERROR: There are no output_list for return.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\")\n",
    "        return -1\n",
    "    \n",
    "    if not(tr_x_val is None) and not(tr_y_val is None):\n",
    "        # predict tr result only when no tr_prediction input\n",
    "        if tr_predictions is None:\n",
    "            tr_loss,tr_accuracy= using_model.evaluate(tr_x_val,tr_y_val)\n",
    "            tr_predictions = using_model.predict(tr_x_val)\n",
    "        # tr sensitivity / specificity\n",
    "        labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "        tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, tr_y_val)\n",
    "        tr_predictions_flat = tr_predictions[:,0]   \n",
    "        # roc(tr)\n",
    "        fpr_tr, tpr_tr, threshold_tr = metrics.roc_curve(tr_y_val, tr_predictions)\n",
    "        roc_auc_tr = metrics.auc(fpr_tr, tpr_tr)\n",
    "    \n",
    "    if not(ts_x_val is None) and not(ts_y_val is None):\n",
    "        # predict ts result only when no ts_prediction input\n",
    "        if ts_predictions is None:\n",
    "            ts_loss,ts_accuracy= using_model.evaluate(ts_x_val,ts_y_val)\n",
    "            ts_predictions = using_model.predict(ts_x_val)\n",
    "        labeled_ts_predictions = np.where(ts_predictions > 0.5, 1, 0).flatten()\n",
    "        ts_sensitivity, ts_specificity = check_correct(labeled_ts_predictions, ts_y_val)\n",
    "        ts_predictions_flat = ts_predictions[:,0]   \n",
    "        # roc(ts)\n",
    "        fpr_ts, tpr_ts, threshold_ts = metrics.roc_curve(ts_y_val, ts_predictions)\n",
    "        roc_auc_ts = metrics.auc(fpr_ts, tpr_ts)    \n",
    "    \n",
    "    if (not(tr_x_val is None) and not(tr_y_val is None)) and (not(ts_x_val is None) and not(ts_y_val is None)):\n",
    "        y_true = np.append(tr_y_val, ts_y_val)\n",
    "        y_pred = np.append(tr_predictions, ts_predictions)\n",
    "        fpr_total, tpr_total, threshold_total = metrics.roc_curve(y_true, y_pred)\n",
    "        roc_auc_total = metrics.auc(fpr_total, tpr_total)\n",
    "        \n",
    "        \n",
    "    return_list = []\n",
    "    \n",
    "    for output in output_list:\n",
    "        \n",
    "        if(output == \"tr_loss\"):\n",
    "            return_list.append(tr_loss)\n",
    "                               \n",
    "        elif(output == \"tr_accuracy\"):\n",
    "            return_list.append(tr_accuracy)\n",
    "                               \n",
    "        elif(output == \"tr_sensitivity\"):\n",
    "            return_list.append(tr_sensitivity)\n",
    "                               \n",
    "        elif(output == \"tr_specificity\"):\n",
    "            return_list.append(tr_specificity)\n",
    "                               \n",
    "        elif(output == \"tr_predictions\"):\n",
    "            return_list.append(tr_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_tr_predictions\"):\n",
    "            return_list.append(labeled_tr_predictions)\n",
    "                               \n",
    "        elif(output == \"tr_predictions_flat\"):\n",
    "            return_list.append(tr_predictions_flat)\n",
    "            \n",
    "        elif(output == \"roc_auc_tr\"):\n",
    "            return_list.append(roc_auc_tr)\n",
    "\n",
    "        elif(output == \"ts_loss\"):\n",
    "            return_list.append(ts_loss)\n",
    "                               \n",
    "        elif(output == \"ts_accuracy\"):\n",
    "            return_list.append(ts_accuracy)\n",
    "                               \n",
    "        elif(output == \"ts_sensitivity\"):\n",
    "            return_list.append(ts_sensitivity)\n",
    "                               \n",
    "        elif(output == \"ts_specificity\"):\n",
    "            return_list.append(ts_specificity)\n",
    "                               \n",
    "        elif(output == \"ts_predictions\"):\n",
    "            return_list.append(ts_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_ts_predictions\"):\n",
    "            return_list.append(labeled_ts_predictions)\n",
    "                               \n",
    "        elif(output == \"ts_predictions_flat\"):\n",
    "            return_list.append(ts_predictions_flat)\n",
    "        \n",
    "        elif(output == \"roc_auc_ts\"):\n",
    "            return_list.append(roc_auc_ts)\n",
    "            \n",
    "        elif(output == \"roc_auc_total\"):\n",
    "            return_list.append(roc_auc_total)\n",
    "                               \n",
    "        else:\n",
    "            print(\"There are no options <\"+str(output)+\">. Please refer these output options:\\ntr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\")\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation: import & preprocessing data + import module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input path & name of models / raw data for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model_path & each model_name.\n",
    "# Caution: If you want to change input models, you also have to change selected data types.\n",
    "\n",
    "# ex) if you want to put these models: two CV, one Annot_3000,  one Var, one new_Diff, one Clin.\n",
    "\n",
    "'''\n",
    "\n",
    "m_1_name = CV_400_1.h5\n",
    "m_2_name = CV_400_2.h5\n",
    "m_3_name = Annot_3000_400_1.h5\n",
    "m_4_name = Var_400_0.h5\n",
    "m_5_name = new_Diff_400_2.h5\n",
    "m_6_name = Clin_400_1.h5\n",
    "--> if you change this part,\n",
    "\n",
    "select_types = [types[1], # \"inter_by_names_CV_400\"\n",
    "                types[1], # \"inter_by_names_CV_400\"\n",
    "                types[0], # \"inter_by_names_Annotation3000_400\"\n",
    "                types[2], # \"inter_by_names_Var_400\"\n",
    "                types[3], # \"inter_by_names_new_Diff_400\"\n",
    "                types[4]] # \"inter_by_names_Clin\"\n",
    "--> you also have to change this part.\n",
    "\n",
    "'''\n",
    "\n",
    "types = [\"inter_by_names_Annotation3000_400\", \"inter_by_names_CV_400\", \n",
    "         \"inter_by_names_Var_400\", \"inter_by_names_new_Diff_400\",\n",
    "         \"inter_by_names_Clin\", \n",
    "         \"inter_by_names_SNV\" \n",
    "         ]\n",
    "\n",
    "# input model path & ensemble data(Transcriptome, Cinical Information, Somatic Mutation data)\n",
    "# data path(server): /home/tjahn/TCGA_Ovary/01.Data/DNN/TC_intersect_subsamples_by_names \n",
    "model_path = \"C:/test/best_models/\"\n",
    "path = \"C:/test/TC_intersect_subsamples_by_names/\"\n",
    "save_model_path = \"../models/Ovary\"\n",
    "save_prediction_path = \"../result/\"\n",
    "\n",
    "# change 'types' and 'load_model' part for using another models.\n",
    "m_1_name = \"new_Diff_400_2.h5\"\n",
    "m_2_name = \"CV_400_0.h5\"\n",
    "m_3_name = \"Var_400_0.h5\"\n",
    "m_4_name = \"new_Diff_400_0.h5\"\n",
    "m_5_name = \"Clin_2.h5\"\n",
    "m_6_name = \"SNV_1.h5\"\n",
    "select_types = [types[3],\n",
    "                types[1],\n",
    "                types[2],\n",
    "                types[3],\n",
    "                types[4],\n",
    "                types[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "select = [1, 3, 4]\n",
    "print(select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] file_name:  inter_by_names_new_Diff_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[2] file_name:  inter_by_names_CV_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[3] file_name:  inter_by_names_Var_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[4] file_name:  inter_by_names_new_Diff_400 \n",
      "sample : 153  \n",
      "features : 400\n",
      "[5] file_name:  inter_by_names_Clin \n",
      "sample : 153  \n",
      "features : 35\n",
      "[6] file_name:  inter_by_names_SNV \n",
      "sample : 153  \n",
      "features : 6970\n"
     ]
    }
   ],
   "source": [
    "file_1 = path+select_types[0]+\".csv\"\n",
    "file_2 = path+select_types[1]+\".csv\"\n",
    "file_3 = path+select_types[2]+\".csv\"\n",
    "file_4 = path+select_types[3]+\".csv\"\n",
    "file_5 = path+select_types[4]+\".csv\"\n",
    "file_6 = path+select_types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "sample_1,features_1 = data_1.shape\n",
    "sample_2,features_2 = data_2.shape\n",
    "sample_3,features_3 = data_3.shape\n",
    "sample_4,features_4 = data_4.shape\n",
    "sample_5,features_5 = data_5.shape\n",
    "sample_6,features_6 = data_6.shape\n",
    "\n",
    "# Data frame include index & Platinum_Status column, substract 2 to calculate real number of features \n",
    "[features_1, features_2, features_3, features_4, features_5, features_6] = [features_1-2, features_2-2, features_3-2, features_4-2, features_5-2, features_6-2]\n",
    "\n",
    "print(\"[1] file_name: \", select_types[0], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "print(\"[2] file_name: \", select_types[1], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "print(\"[3] file_name: \", select_types[2], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "print(\"[4] file_name: \", select_types[3], \"\\nsample : {}  \\nfeatures : {}\".format(sample_4,features_4))\n",
    "print(\"[5] file_name: \", select_types[4], \"\\nsample : {}  \\nfeatures : {}\".format(sample_5,features_5))\n",
    "print(\"[6] file_name: \", select_types[5], \"\\nsample : {}  \\nfeatures : {}\".format(sample_6,features_6))\n",
    "\n",
    "# Split Train Test Data\n",
    "\n",
    "train_data_1, test_data_1, y_val_1, x_val_1, test_y_val_1, test_x_val_1 = data_split(raw_data = data_1, index_col = -1, test_index = 1)\n",
    "train_data_2, test_data_2, y_val_2, x_val_2, test_y_val_2, test_x_val_2 = data_split(raw_data = data_2, index_col = -1, test_index = 1)\n",
    "train_data_3, test_data_3, y_val_3, x_val_3, test_y_val_3, test_x_val_3 = data_split(raw_data = data_3, index_col = -1, test_index = 1)\n",
    "train_data_4, test_data_4, y_val_4, x_val_4, test_y_val_4, test_x_val_4 = data_split(raw_data = data_4, index_col = -1, test_index = 1)\n",
    "train_data_5, test_data_5, y_val_5, x_val_5, test_y_val_5, test_x_val_5 = data_split(raw_data = data_5, index_col = -1, test_index = 1)\n",
    "train_data_6, test_data_6, y_val_6, x_val_6, test_y_val_6, test_x_val_6 = data_split(raw_data = data_6, index_col = -1, test_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 96us/step\n",
      "31/31 [==============================] - 0s 97us/step\n",
      "31/31 [==============================] - 0s 129us/step\n",
      "0.8387096774193549\n",
      "Increased Coverage : model added!\n",
      "0.8709677419354839\n",
      "Increased Coverage : model added!\n",
      "0.967741935483871\n",
      "Increased Coverage : model added!\n",
      "number of final models for ensemble:  3\n"
     ]
    }
   ],
   "source": [
    "def ensemble_coverage(inputModels,x,y):\n",
    "    outputModels = []\n",
    "    modelInfo = []\n",
    "    coverageTotal= [False]*len(y)\n",
    "    \n",
    "    for m in inputModels:\n",
    "        yHat = m.predict(x)\n",
    "        yHat = [round(i) for [i] in yHat]\n",
    "        \n",
    "        loss, acc = m.evaluate(x,y)\n",
    "        modelInfo.append((m,yHat,acc))\n",
    "    \n",
    "    modelInfo.sort(key=lambda x : x[2],reverse=True)\n",
    "    \n",
    "    for m,yHat,acc in modelInfo:\n",
    "        beforeCoverage = sum(coverageTotal)\n",
    "        coverage = [a == b for a,b in zip(y,yHat)]\n",
    "        coverageTotal = [a or b for a,b in zip(coverageTotal,coverage)]\n",
    "        afterCoverage = sum(coverageTotal)\n",
    "        \n",
    "        print(afterCoverage/len(y))\n",
    "        \n",
    "        if afterCoverage > beforeCoverage:\n",
    "            outputModels.append(m)\n",
    "            print(\"Increased Coverage : model added!\")\n",
    "        else:\n",
    "            print(\"Same Coverage : model not added\")\n",
    "        if afterCoverage == len(y):\n",
    "            print(\"Fully Covered!\")\n",
    "            break\n",
    "        \n",
    "                \n",
    "    return outputModels\n",
    "\n",
    "inputModels = [model_1_l,model_3_l,model_4_l]\n",
    "x = test_x_val_1\n",
    "y = test_y_val_1\n",
    "\n",
    "ensemble_models = ensemble_coverage(inputModels,x,y)\n",
    "print(\"number of final models for ensemble: \",len(ensemble_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import separate models & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 4ms/step\n",
      "31/31 [==============================] - 0s 96us/step\n",
      "inter_by_names_new_Diff_400\n",
      "Train Accuracy: 0.9918032786885246\n",
      "Test Accuracy: 0.8387096524238586\n",
      "122/122 [==============================] - 1s 4ms/step\n",
      "31/31 [==============================] - 0s 161us/step\n",
      "inter_by_names_CV_400\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.6451612710952759\n",
      "122/122 [==============================] - 1s 5ms/step\n",
      "31/31 [==============================] - 0s 161us/step\n",
      "inter_by_names_Var_400\n",
      "Train Accuracy: 0.9426229449569202\n",
      "Test Accuracy: 0.7096773982048035\n",
      "122/122 [==============================] - 1s 4ms/step\n",
      "31/31 [==============================] - 0s 129us/step\n",
      "inter_by_names_new_Diff_400\n",
      "Train Accuracy: 0.9508196662683956\n",
      "Test Accuracy: 0.8387096524238586\n",
      "122/122 [==============================] - 1s 5ms/step\n",
      "31/31 [==============================] - 0s 257us/step\n",
      "inter_by_names_Clin\n",
      "Train Accuracy: 0.7704918062100645\n",
      "Test Accuracy: 0.7419354915618896\n",
      "122/122 [==============================] - 1s 5ms/step\n",
      "31/31 [==============================] - 0s 225us/step\n",
      "inter_by_names_SNV\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.774193525314331\n"
     ]
    }
   ],
   "source": [
    "# model load & evaluation. <model_n_l> is full-layer model, <model_n_l_new> is without-sigmoid-layer model.\n",
    "'''\n",
    "Each model's tr_accuracy can be differ to original model, but ts_accuracy should be same to original tested models.\n",
    "Because we using full-size data(about 200 patients data used Transcriptome, Clinical, SNV models.) for train each models.\n",
    "In contrast, in this code, we using ensemble-input data(intersected 153 patients).\n",
    "For-training-patients may be different in ensemble data and whole size data, but for-test-patients are the same.\n",
    "'''\n",
    "\n",
    "model_1_l = load_model(model_path+m_1_name)\n",
    "output_list_1 = model_performance(Input_Prediction_Passively=False, using_model=model_1_l, tr_x_val=x_val_1, tr_y_val=y_val_1, ts_x_val = test_x_val_1, ts_y_val= test_y_val_1, output_list= [\"tr_accuracy\", \"ts_accuracy\", \"\"])\n",
    "\n",
    "m_1_l_tr_loss,m_1_l_tr_accuracy=model_1_l.evaluate(x_val_1,y_val_1)\n",
    "m_1_l_loss,m_1_l_accuracy= model_1_l.evaluate(test_x_val_1,test_y_val_1)\n",
    "model_1_l_new = Model(inputs = model_1_l.input, outputs=model_1_l.get_layer(model_1_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[0])\n",
    "print(\"Train Accuracy: {}\".format(m_1_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_1_l_accuracy))\n",
    "\n",
    "model_2_l = load_model(model_path+m_2_name)\n",
    "m_2_l_tr_loss,m_2_l_tr_accuracy=model_2_l.evaluate(x_val_2,y_val_2)\n",
    "m_2_l_loss,m_2_l_accuracy= model_2_l.evaluate(test_x_val_2,test_y_val_2)\n",
    "model_2_l_new = Model(inputs = model_2_l.input, outputs=model_2_l.get_layer(model_2_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[1])\n",
    "print(\"Train Accuracy: {}\".format(m_2_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_2_l_accuracy))\n",
    "\n",
    "model_3_l = load_model(model_path+m_3_name)\n",
    "m_3_l_tr_loss,m_3_l_tr_accuracy=model_3_l.evaluate(x_val_3,y_val_3)\n",
    "m_3_l_loss,m_3_l_accuracy= model_3_l.evaluate(test_x_val_3,test_y_val_3)\n",
    "model_3_l_new = Model(inputs = model_3_l.input, outputs=model_3_l.get_layer(model_3_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[2])\n",
    "print(\"Train Accuracy: {}\".format(m_3_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_3_l_accuracy))\n",
    "\n",
    "model_4_l = load_model(model_path+m_4_name)\n",
    "m_4_l_tr_loss,m_4_l_tr_accuracy=model_4_l.evaluate(x_val_4,y_val_4)\n",
    "m_4_l_loss,m_4_l_accuracy= model_4_l.evaluate(test_x_val_4,test_y_val_4)\n",
    "model_4_l_new = Model(inputs = model_4_l.input, outputs=model_4_l.get_layer(model_4_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[3])\n",
    "print(\"Train Accuracy: {}\".format(m_4_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_4_l_accuracy))\n",
    "\n",
    "model_5_l = load_model(model_path+m_5_name)\n",
    "m_5_l_tr_loss,m_5_l_tr_accuracy=model_5_l.evaluate(x_val_5,y_val_5)\n",
    "m_5_l_loss,m_5_l_accuracy= model_5_l.evaluate(test_x_val_5,test_y_val_5)\n",
    "model_5_l_new = Model(inputs = model_5_l.input, outputs=model_5_l.get_layer(model_5_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[4])\n",
    "print(\"Train Accuracy: {}\".format(m_5_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_5_l_accuracy))\n",
    "\n",
    "model_6_l = load_model(model_path+m_6_name)\n",
    "m_6_l_tr_loss,m_6_l_tr_accuracy=model_6_l.evaluate(x_val_6,y_val_6)\n",
    "m_6_l_loss,m_6_l_accuracy= model_6_l.evaluate(test_x_val_6,test_y_val_6)\n",
    "model_6_l_new = Model(inputs = model_6_l.input, outputs=model_6_l.get_layer(model_6_l.layers[-2].name).output)\n",
    "\n",
    "print(select_types[5])\n",
    "print(\"Train Accuracy: {}\".format(m_6_l_tr_accuracy))\n",
    "print(\"Test Accuracy: {}\".format(m_6_l_accuracy))\n",
    "\n",
    "m_1_predictions = model_1_l.predict(x_val_1)\n",
    "m_2_predictions = model_2_l.predict(x_val_2)\n",
    "m_3_predictions = model_3_l.predict(x_val_3)\n",
    "m_4_predictions = model_4_l.predict(x_val_4)\n",
    "m_5_predictions = model_5_l.predict(x_val_5)\n",
    "m_6_predictions = model_6_l.predict(x_val_6)\n",
    "\n",
    "m_1_test_predictions = model_1_l.predict(test_x_val_1)\n",
    "m_2_test_predictions = model_2_l.predict(test_x_val_2)\n",
    "m_3_test_predictions = model_3_l.predict(test_x_val_3)\n",
    "m_4_test_predictions = model_4_l.predict(test_x_val_4)\n",
    "m_5_test_predictions = model_5_l.predict(test_x_val_5)\n",
    "m_6_test_predictions = model_6_l.predict(test_x_val_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seperate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< inter_by_names_new_Diff_400 > tr: 0.9918032786885246, ts: 0.8387096524238586\n",
      "< inter_by_names_CV_400 > tr: 1.0, ts: 0.6451612710952759\n",
      "< inter_by_names_Var_400 > tr: 0.9426229449569202, ts: 0.7096773982048035\n",
      "< inter_by_names_new_Diff_400 > tr: 0.9508196662683956, ts: 0.8387096524238586\n",
      "< inter_by_names_Clin > tr: 0.7704918062100645, ts: 0.7419354915618896\n",
      "< inter_by_names_SNV > tr: 1.0, ts: 0.774193525314331\n"
     ]
    }
   ],
   "source": [
    "print(\"< \"+select_types[0]+\" > tr: \"+str(m_1_l_tr_accuracy)+\", ts: \"+str(m_1_l_accuracy))\n",
    "print(\"< \"+select_types[1]+\" > tr: \"+str(m_2_l_tr_accuracy)+\", ts: \"+str(m_2_l_accuracy))\n",
    "print(\"< \"+select_types[2]+\" > tr: \"+str(m_3_l_tr_accuracy)+\", ts: \"+str(m_3_l_accuracy))\n",
    "print(\"< \"+select_types[3]+\" > tr: \"+str(m_4_l_tr_accuracy)+\", ts: \"+str(m_4_l_accuracy))\n",
    "print(\"< \"+select_types[4]+\" > tr: \"+str(m_5_l_tr_accuracy)+\", ts: \"+str(m_5_l_accuracy))\n",
    "print(\"< \"+select_types[5]+\" > tr: \"+str(m_6_l_tr_accuracy)+\", ts: \"+str(m_6_l_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DNN-Combiner Ensmeble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building original ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################## DNN em ##################################\n",
      "select: [1, 3, 4]\n",
      "inter_by_names_new_Diff_400\n",
      "inter_by_names_Var_400\n",
      "inter_by_names_new_Diff_400\n",
      "#############################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "122/122 [==============================] - 2s 14ms/step - loss: 0.6857 - acc: 0.6148\n",
      "122/122 [==============================] - 1s 5ms/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 131us/step - loss: 0.7059 - acc: 0.5984\n",
      "122/122 [==============================] - 0s 139us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.6662 - acc: 0.6230\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 155us/step - loss: 0.6685 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 221us/step - loss: 0.6437 - acc: 0.7377\n",
      "122/122 [==============================] - 0s 123us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 172us/step - loss: 0.6261 - acc: 0.7951\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 155us/step - loss: 0.5914 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 139us/step - loss: 0.5846 - acc: 0.8443\n",
      "122/122 [==============================] - 0s 90us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.5873 - acc: 0.8033\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 221us/step - loss: 0.5553 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 155us/step - loss: 0.5833 - acc: 0.8033\n",
      "122/122 [==============================] - 0s 131us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.5748 - acc: 0.7787\n",
      "122/122 [==============================] - 0s 139us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 163us/step - loss: 0.5324 - acc: 0.8443\n",
      "122/122 [==============================] - 0s 114us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 221us/step - loss: 0.5520 - acc: 0.8197\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 155us/step - loss: 0.5390 - acc: 0.8443\n",
      "122/122 [==============================] - 0s 131us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 139us/step - loss: 0.5434 - acc: 0.8279\n",
      "122/122 [==============================] - 0s 90us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.5134 - acc: 0.8443\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.5061 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 139us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 139us/step - loss: 0.5133 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 204us/step - loss: 0.4756 - acc: 0.8852\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.4976 - acc: 0.8279\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.4926 - acc: 0.8279\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 180us/step - loss: 0.4834 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.4487 - acc: 0.8770\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4627 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 123us/step - loss: 0.4260 - acc: 0.8852\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 180us/step - loss: 0.4036 - acc: 0.8934\n",
      "122/122 [==============================] - 0s 90us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 123us/step - loss: 0.4057 - acc: 0.8607\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 188us/step - loss: 0.4264 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.4665 - acc: 0.7951\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 188us/step - loss: 0.4108 - acc: 0.8770\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 131us/step - loss: 0.4017 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 90us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 180us/step - loss: 0.4018 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 114us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 213us/step - loss: 0.4082 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 164us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.3834 - acc: 0.8934\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 180us/step - loss: 0.3711 - acc: 0.9016\n",
      "122/122 [==============================] - 0s 123us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 163us/step - loss: 0.4286 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.3557 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.3131 - acc: 0.9180\n",
      "122/122 [==============================] - 0s 90us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 188us/step - loss: 0.3689 - acc: 0.8607\n",
      "122/122 [==============================] - 0s 115us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 221us/step - loss: 0.4173 - acc: 0.8443\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 155us/step - loss: 0.3595 - acc: 0.8770\n",
      "122/122 [==============================] - 0s 164us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 163us/step - loss: 0.3437 - acc: 0.9262\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 155us/step - loss: 0.3721 - acc: 0.9016\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 139us/step - loss: 0.3378 - acc: 0.9016\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 196us/step - loss: 0.3893 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 98us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.4296 - acc: 0.8443\n",
      "122/122 [==============================] - 0s 123us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 163us/step - loss: 0.3124 - acc: 0.9016\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 147us/step - loss: 0.2728 - acc: 0.9344\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 196us/step - loss: 0.3134 - acc: 0.9098\n",
      "122/122 [==============================] - 0s 106us/step\n",
      "##################### DNN ensemble model trained. #####################\n",
      "##################### DNN ensemble model saved. #####################\n"
     ]
    }
   ],
   "source": [
    "print(\"################################## DNN em ##################################\")\n",
    "print(\"select: \"+str(select))\n",
    "for select_type_i in select:\n",
    "    print(select_types[select_type_i-1])\n",
    "print(\"#############################################################################################\")\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "input_drop_out_em = 0.3\n",
    "drop_out_em = 0.5\n",
    "layers = [5]\n",
    "em_tr_loss_best = 100 # for saving best loss value \n",
    "best_em_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "m_tr_predictions = [m_1_predictions, m_2_predictions, m_3_predictions, m_4_predictions, m_5_predictions, m_6_predictions]\n",
    "m_tr_predictions_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    m_tr_predictions_select.append(m_tr_predictions[select[i]-1])\n",
    "    #print(m_tr_predictions[select[i]-1].shape)\n",
    "    \n",
    "em_x_val = np.concatenate(m_tr_predictions_select, axis=1)\n",
    "\n",
    "input_em = Input(shape=(len(select),))\n",
    "em_m_dp = Dropout(input_drop_out_em)(input_em)\n",
    "for i in layers:\n",
    "    em_m = Dense(i,activation='relu')(em_m_dp)\n",
    "    em_m_dp = Dropout(drop_out_em)(em_m)\n",
    "em_m_final = em_m_dp\n",
    "output_em = Dense(1, activation=\"sigmoid\")(em_m_final)\n",
    "em_model = Model(inputs=input_em,outputs=output_em)\n",
    "em_model.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    em_model.fit(em_x_val, y_val_1, nb_epoch=1)\n",
    "    em_tr_loss=em_model.evaluate(em_x_val,y_val_1)[0]\n",
    "    if em_tr_loss < em_tr_loss_best: # new best model. count reset.\n",
    "        em_tr_loss_best = em_tr_loss\n",
    "        count=0\n",
    "        best_em_model = em_model\n",
    "    if count>3: # no increase three time. stop.\n",
    "        em_model = best_em_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "print(\"##################### DNN ensemble model trained. #####################\")\n",
    "# 4) save model\n",
    "\n",
    "em_model.save(save_model_path+\"/m_em.h5\")\n",
    "print(\"##################### DNN ensemble model saved. #####################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _DNN Combiner_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 114us/step\n",
      "31/31 [==============================] - 0s 64us/step\n",
      "Overall AUC:  0.9737843551797041\n",
      "Train AUC:  0.9996657754010695\n",
      "Test AUC:  0.8080808080808081\n",
      "Train Accuracy: 0.9754098302028218\n",
      "Train Sensitivities & Specificities : 0.9117647058823529, 1.0\n",
      "Test Accuracy: 0.7419354915618896\n",
      "Test Sensitivities & Specificities : 0.4444444444444444, 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "m_test_predictions = [m_1_test_predictions, m_2_test_predictions, m_3_test_predictions, m_4_test_predictions, m_5_test_predictions, m_6_test_predictions]\n",
    "m_test_predictions_select = []   \n",
    "#def prediction_result(pathway, tr_x_val, tr_y_val, ts_x_val, ts_y_val, tr_patients, ts_patients)\n",
    "for i in range(len(select)):\n",
    "    m_test_predictions_select.append(m_test_predictions[select[i]-1])\n",
    "    \n",
    "em_test_x_val = np.concatenate(m_test_predictions_select, axis=1)\n",
    "\n",
    "em_output_list = model_performance(\n",
    "    information = False, using_model=em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=em_x_val, tr_y_val=y_val_1, ts_x_val=em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "em_tr_loss, em_tr_accuracy, em_tr_sensitivity, em_tr_specificity, em_tr_predictions, em_labeled_tr_predictions, em_tr_predictions_flat, em_roc_auc_tr, em_ts_loss, em_ts_accuracy, em_ts_sensitivity, em_ts_specificity, em_ts_predictions,em_labeled_ts_predictions, em_ts_predictions_flat, em_roc_auc_ts, em_roc_auc_total = em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", em_roc_auc_total)\n",
    "print(\"Train AUC: \", em_roc_auc_tr)\n",
    "print(\"Test AUC: \", em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(em_tr_sensitivity)+\", \"+str(em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(em_ts_sensitivity)+\", \"+str(em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_em = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(em_tr_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_em.to_csv(save_prediction_path+\"prediction_result_DNN_em_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(em_ts_predictions_flat), \n",
    "                        \"prediction\":list(em_labeled_ts_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_em.to_csv(save_prediction_path+\"prediction_result_DNN_em_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mean Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _mean_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC:  0.9706131078224102\n",
      "Train AUC:  0.9996657754010695\n",
      "Test AUC:  0.7979797979797979\n",
      "Train Accuracy: 0.9836065573770492\n",
      "Train Sensitivities & Specificities : 0.9411764705882353, 1.0\n",
      "Test Accuracy: 0.8064516129032258\n",
      "Test Sensitivities & Specificities : 0.6666666666666666, 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "mean_em_tr_predictions=sum(m_tr_predictions_select)/len(select)\n",
    "mean_em_ts_predictions=sum(m_test_predictions_select)/len(select)\n",
    "\n",
    "mean_em_output_list = model_performance(\n",
    "    information = False, using_model=None,Input_Prediction_Passively = True, \n",
    "    tr_predictions=mean_em_tr_predictions, ts_predictions=mean_em_ts_predictions, \n",
    "    tr_x_val=em_x_val, tr_y_val=y_val_1, ts_x_val=em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_sensitivity\", \"tr_specificity\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_sensitivity\", \"ts_specificity\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "mean_em_tr_sensitivity, mean_em_tr_specificity,  mean_em_labeled_tr_predictions, mean_em_tr_predictions_flat, mean_em_roc_auc_tr, mean_em_ts_sensitivity, mean_em_ts_specificity, mean_em_labeled_ts_predictions, mean_em_ts_predictions_flat, mean_em_roc_auc_ts, mean_em_roc_auc_total = mean_em_output_list\n",
    "\n",
    "mean_em_tr_accuracy = sum(mean_em_labeled_tr_predictions==y_val_1.values)/len(y_val_1)\n",
    "mean_em_ts_accuracy = sum(mean_em_labeled_ts_predictions==test_y_val_1.values)/len(test_y_val_1)\n",
    "\n",
    "print(\"Overall AUC: \", mean_em_roc_auc_total)\n",
    "print(\"Train AUC: \", mean_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", mean_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(mean_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(mean_em_tr_sensitivity)+\", \"+str(mean_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(mean_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(mean_em_ts_sensitivity)+\", \"+str(mean_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_mean_em = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(mean_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_mean_em.to_csv(save_prediction_path+\"prediction_result_mean_em_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_mean_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(mean_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(mean_em_labeled_ts_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_mean_em.to_csv(save_prediction_path+\"prediction_result_mean_em_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 4s 31ms/step\n",
      "31/31 [==============================] - 0s 421us/step\n",
      "Overall AUC:  0.9809725158562368\n",
      "Train AUC:  1.0\n",
      "Test AUC:  0.8686868686868686\n",
      "Train Accuracy: 1.0\n",
      "Train Sensitivities & Specificities : 1.0, 1.0\n",
      "Test Accuracy: 0.9032257795333862\n",
      "Test Sensitivities & Specificities : 0.8888888888888888, 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "model_test = load_model(\"G:/ /Class/6 7 (hell)/Lab/TCGA /Best_Models/18.09.15/best_models/Full_5+5.h5\")\n",
    "full_em_output_list = model_performance(\n",
    "    information = False, using_model=model_test,Input_Prediction_Passively = False, \n",
    "    tr_x_val=full_em_x_val, tr_y_val=y_val_1, ts_x_val=full_em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "full_em_tr_loss, full_em_tr_accuracy, full_em_tr_sensitivity, full_em_tr_specificity, full_em_tr_predictions, full_em_labeled_tr_predictions, full_em_tr_predictions_flat, full_em_roc_auc_tr, full_em_ts_loss, full_em_ts_accuracy, full_em_ts_sensitivity, full_em_ts_specificity, full_em_ts_predictions,full_em_labeled_ts_predictions, full_em_ts_predictions_flat, full_em_roc_auc_ts, full_em_roc_auc_total = full_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", full_em_roc_auc_total)\n",
    "print(\"Train AUC: \", full_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", full_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(full_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(full_em_tr_sensitivity)+\", \"+str(full_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(full_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(full_em_ts_sensitivity)+\", \"+str(full_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Transferred Ensemble Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making new input data for t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 5)\n",
      "(122, 200)\n",
      "(122, 200)\n",
      "(31, 5)\n",
      "(31, 200)\n",
      "(31, 200)\n",
      "(122, 405)\n"
     ]
    }
   ],
   "source": [
    "results_m_1 = model_1_l_new.predict([x_val_1])\n",
    "results_m_2 = model_2_l_new.predict([x_val_2])\n",
    "results_m_3 = model_3_l_new.predict([x_val_3])\n",
    "results_m_4 = model_4_l_new.predict([x_val_4])\n",
    "results_m_5 = model_5_l_new.predict([x_val_5])\n",
    "results_m_6 = model_6_l_new.predict([x_val_6])\n",
    "\n",
    "results_m_sum = [results_m_1, results_m_2, results_m_3, results_m_4, results_m_5, results_m_6]\n",
    "results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    print(results_m_sum[select[i]-1].shape)\n",
    "    results_m_select.append(results_m_sum[select[i]-1])\n",
    "\n",
    "test_results_m_1 = model_1_l_new.predict([test_x_val_1])\n",
    "test_results_m_2 = model_2_l_new.predict([test_x_val_2])\n",
    "test_results_m_3 = model_3_l_new.predict([test_x_val_3])\n",
    "test_results_m_4 = model_4_l_new.predict([test_x_val_4])\n",
    "test_results_m_5 = model_5_l_new.predict([test_x_val_5])\n",
    "test_results_m_6 = model_6_l_new.predict([test_x_val_6])\n",
    "\n",
    "test_results_m_sum = [test_results_m_1, test_results_m_2, test_results_m_3, test_results_m_4, test_results_m_5, test_results_m_6]\n",
    "test_results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    test_results_m_select.append(test_results_m_sum[select[i]-1])\n",
    "    print(test_results_m_sum[select[i]-1].shape)\n",
    "\n",
    "t_em_test_x_val = np.concatenate(test_results_m_select, axis=1)\n",
    "t_em_x_val = np.concatenate(results_m_select, axis=1)\n",
    "print(t_em_x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 2005      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,031\n",
      "Trainable params: 2,021\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_em_x_val = np.concatenate([x_val_1, em_x_val], axis = 1)\n",
    "full_em_test_x_val = np.concatenate([test_x_val_1, em_test_x_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 400)\n",
      "(122, 405)\n",
      "(122, 3)\n",
      "(31, 3)\n",
      "403\n",
      "(31, 403)\n"
     ]
    }
   ],
   "source": [
    "print(x_val_1.shape)\n",
    "print(t_em_x_val.shape)\n",
    "print(em_x_val.shape)\n",
    "print(em_test_x_val.shape)\n",
    "print(full_em_x_val.shape[1])\n",
    "print(full_em_test_x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed_model\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 6s 52ms/step - loss: 0.6239 - acc: 0.6721\n",
      "122/122 [==============================] - 3s 21ms/step\n",
      "best model: 0.8032786836389636\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4844 - acc: 0.7541\n",
      "122/122 [==============================] - 0s 319us/step\n",
      "best model: 0.8442622901963406\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.4727 - acc: 0.7869\n",
      "122/122 [==============================] - 0s 319us/step\n",
      "best model: 0.8852458967537177\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4483 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 343us/step\n",
      "best model: 0.9180327878623712\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.4619 - acc: 0.7869\n",
      "122/122 [==============================] - 0s 335us/step\n",
      "best model: 0.9508196662683956\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.4164 - acc: 0.8115\n",
      "122/122 [==============================] - 0s 311us/step\n",
      "best model: 0.9672131147540983\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4226 - acc: 0.8033\n",
      "122/122 [==============================] - 0s 294us/step\n",
      "best model: 0.9672131147540983\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.3446 - acc: 0.8607\n",
      "122/122 [==============================] - 0s 311us/step\n",
      "best model: 0.959016387579871\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.3905 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 294us/step\n",
      "best model: 0.9672131147540983\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3725 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 327us/step\n",
      "best model: 0.9754098360655737\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4232 - acc: 0.8033\n",
      "122/122 [==============================] - 0s 311us/step\n",
      "best model: 0.9754098360655737\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.3989 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 253us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3376 - acc: 0.8197\n",
      "122/122 [==============================] - 0s 319us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3781 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 311us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3931 - acc: 0.8197\n",
      "122/122 [==============================] - 0s 311us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3656 - acc: 0.8525\n",
      "122/122 [==============================] - 0s 286us/step\n",
      "best model: 0.9918032786885246\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3632 - acc: 0.8361\n",
      "122/122 [==============================] - 0s 302us/step\n",
      "best model: 0.9918032786885246\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3172 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 286us/step\n",
      "best model: 0.9918032786885246\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3764 - acc: 0.8279\n",
      "122/122 [==============================] - 0s 327us/step\n",
      "best model: 1.0\n",
      "mixed_model trained.\n",
      "mixed_model saved.\n",
      "122/122 [==============================] - 0s 327us/step\n",
      "31/31 [==============================] - 0s 193us/step\n",
      "Overall AUC:  0.9767441860465116\n",
      "Train AUC:  1.0\n",
      "Test AUC:  0.8181818181818181\n",
      "Train Accuracy: 1.0\n",
      "Train Sensitivities & Specificities : 1.0, 1.0\n",
      "Test Accuracy: 0.8387096524238586\n",
      "Test Sensitivities & Specificities : 0.7777777777777778, 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "print(\"mixed_model\")\n",
    "\n",
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "input_drop_out_full_em = 0.5\n",
    "drop_out_full_em = 0\n",
    "layers = [100]\n",
    "full_em_tr_loss_best = 100 # for saving best loss value \n",
    "best_full_em_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "# 2) model build\n",
    "input_full_em = Input(shape=(full_em_x_val.shape[1],))\n",
    "full_em_m_bn = Dropout(input_drop_out_full_em)(input_full_em)\n",
    "for i in layers:\n",
    "    full_em_m = Dense(i)(full_em_m_bn)\n",
    "    full_em_m_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(full_em_m)\n",
    "    full_em_m_ac = Activation(\"relu\")(full_em_m_bn)\n",
    "\n",
    "output_full_em = Dense(1, activation=\"sigmoid\")(full_em_m_ac)\n",
    "full_em_model = Model(inputs=input_full_em,outputs=output_full_em)\n",
    "full_em_model.compile(optimizer=optimizers.Adam(), \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "while 1:\n",
    "    full_em_model.fit(full_em_x_val, y_val_1, batch_size=5,epochs=1)\n",
    "    full_em_tr_loss, full_em_tr_accuracy =full_em_model.evaluate(full_em_x_val, y_val_1)\n",
    "    if full_em_tr_loss < full_em_tr_loss_best: # new best model. count reset.\n",
    "        full_em_tr_loss_best = full_em_tr_loss\n",
    "        count=0\n",
    "        best_full_em_model = full_em_model\n",
    "        best_full_em_tr_accuracy = full_em_tr_accuracy\n",
    "        print(\"best model: \"+str(full_em_tr_accuracy))\n",
    "    if count>20 or (best_full_em_tr_accuracy == 1): # no increase three time. stop.\n",
    "        full_em_model = best_full_em_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "print(\"mixed_model trained.\")\n",
    "\n",
    "# 4) save model\n",
    "full_em_model.save(save_model_path+\"/full_em.h5\")\n",
    "print(\"mixed_model saved.\")\n",
    "\n",
    "# 5) evaluate model\n",
    "full_em_output_list = model_performance(\n",
    "    information = False, using_model=full_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=full_em_x_val, tr_y_val=y_val_1, ts_x_val=full_em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "full_em_tr_loss, full_em_tr_accuracy, full_em_tr_sensitivity, full_em_tr_specificity, full_em_tr_predictions, full_em_labeled_tr_predictions, full_em_tr_predictions_flat, full_em_roc_auc_tr, full_em_ts_loss, full_em_ts_accuracy, full_em_ts_sensitivity, full_em_ts_specificity, full_em_ts_predictions,full_em_labeled_ts_predictions, full_em_ts_predictions_flat, full_em_roc_auc_ts, full_em_roc_auc_total = full_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", full_em_roc_auc_total)\n",
    "print(\"Train AUC: \", full_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", full_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(full_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(full_em_tr_sensitivity)+\", \"+str(full_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(full_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(full_em_ts_sensitivity)+\", \"+str(full_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        (None, 403)               0         \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 403)               0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 100)               40400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,301\n",
      "Trainable params: 40,901\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_em_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "122/122 [==============================] - 3s 21ms/step - loss: 1.2949 - acc: 0.4836\n",
      "122/122 [==============================] - 1s 7ms/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 278us/step - loss: 0.9962 - acc: 0.6967\n",
      "122/122 [==============================] - 0s 245us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.8265 - acc: 0.6148\n",
      "122/122 [==============================] - 0s 245us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.8878 - acc: 0.5164\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 294us/step - loss: 0.7530 - acc: 0.6230\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 286us/step - loss: 0.7712 - acc: 0.6639\n",
      "122/122 [==============================] - 0s 204us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.7654 - acc: 0.6475\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.6718 - acc: 0.6639\n",
      "122/122 [==============================] - 0s 221us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.6495 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 237us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.6556 - acc: 0.6393\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.6676 - acc: 0.6557\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.6493 - acc: 0.6639\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.6053 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 261us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.6301 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 196us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.7385 - acc: 0.7377\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 311us/step - loss: 0.6713 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 335us/step - loss: 0.5726 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.6073 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 164us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.5904 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 302us/step - loss: 0.5810 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.6221 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 229us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 376us/step - loss: 0.5764 - acc: 0.7131\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.5964 - acc: 0.7131\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 401us/step - loss: 0.6053 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.6501 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 213us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.5600 - acc: 0.7213\n",
      "122/122 [==============================] - ETA:  - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 302us/step - loss: 0.5399 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 196us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.5623 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.5851 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.6035 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 196us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 262us/step - loss: 0.4966 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 294us/step - loss: 0.4861 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 229us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.5594 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 302us/step - loss: 0.5737 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.4954 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 253us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 262us/step - loss: 0.5243 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 286us/step - loss: 0.5661 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.5201 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 311us/step - loss: 0.5270 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 278us/step - loss: 0.5025 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 164us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.5611 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.5863 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 213us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 213us/step - loss: 0.4970 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 204us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.5169 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 196us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.4930 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.4831 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 164us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4659 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 278us/step - loss: 0.4727 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.4297 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.4853 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 237us/step - loss: 0.4324 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 221us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4452 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 294us/step - loss: 0.5256 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 221us/step - loss: 0.7232 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.5211 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 286us/step - loss: 0.4916 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 212us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 278us/step - loss: 0.5316 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 213us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 294us/step - loss: 0.4526 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 221us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5565 - acc: 0.710 - 0s 286us/step - loss: 0.5366 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 229us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.5266 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 237us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 262us/step - loss: 0.4750 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 237us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 278us/step - loss: 0.4629 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 221us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 311us/step - loss: 0.4055 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 237us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 278us/step - loss: 0.4622 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 229us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.5315 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4652 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 229us/step - loss: 0.5157 - acc: 0.7131\n",
      "122/122 [==============================] - 0s 237us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4495 - acc: 0.7623\n",
      "122/122 [==============================] - 0s 221us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 286us/step - loss: 0.4228 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4847 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 196us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 278us/step - loss: 0.4349 - acc: 0.7131\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 294us/step - loss: 0.4446 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 286us/step - loss: 0.4105 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 311us/step - loss: 0.4693 - acc: 0.7049\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 294us/step - loss: 0.4306 - acc: 0.7377\n",
      "122/122 [==============================] - 0s 204us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 294us/step - loss: 0.4491 - acc: 0.7377\n",
      "122/122 [==============================] - 0s 196us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 221us/step - loss: 0.5502 - acc: 0.7131\n",
      "122/122 [==============================] - 0s 229us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4519 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 302us/step - loss: 0.4250 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.4935 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 311us/step - loss: 0.4596 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.5130 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 302us/step - loss: 0.5158 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 196us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 262us/step - loss: 0.4665 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 221us/step - loss: 0.4176 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.5113 - acc: 0.7213\n",
      "122/122 [==============================] - ETA:  - 0s 204us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 262us/step - loss: 0.4844 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.6202 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 213us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 286us/step - loss: 0.5020 - acc: 0.7131\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 335us/step - loss: 0.4723 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 163us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 245us/step - loss: 0.4971 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 262us/step - loss: 0.4996 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.4013 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 302us/step - loss: 0.4285 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.4547 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 270us/step - loss: 0.5787 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 213us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 253us/step - loss: 0.5458 - acc: 0.7213\n",
      "122/122 [==============================] - 0s 147us/step\n",
      "full ensemble model trained.\n"
     ]
    }
   ],
   "source": [
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "input_drop_out_t_em = 0.5\n",
    "drop_out_t_em = 0.5\n",
    "layers = [100, 100, 100, 100]\n",
    "full_em_tr_loss_best = 100 # for saving best loss value \n",
    "best_full_em_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_t_em = Input(shape=(full_em_x_val.shape[1],))\n",
    "full_em_m_dp = Dropout(input_drop_out_t_em)(input_t_em)\n",
    "for i in layers:\n",
    "    full_em_m = Dense(i,activation='relu')(full_em_m_dp)\n",
    "    full_em_m_dp = Dropout(drop_out_em)(full_em_m)\n",
    "full_em_m_final = full_em_m_dp\n",
    "output_t_em = Dense(1, activation=\"sigmoid\")(full_em_m_final)\n",
    "full_em_model = Model(inputs=input_t_em,outputs=output_t_em)\n",
    "full_em_model.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "\n",
    "#full_em_model.fit(full_em_x_val, y_val_1, batch_size=5, epochs = 100, validation_split = 0.1, callbacks=[early_stopping])\n",
    "while 1:\n",
    "    full_em_model.fit(full_em_x_val, y_val_1, batch_size= 100, nb_epoch=1)\n",
    "    full_em_tr_loss=full_em_model.evaluate(full_em_x_val,y_val_1)[0]\n",
    "    if full_em_tr_loss < full_em_tr_loss_best: # new best model. count reset.\n",
    "        full_em_tr_loss_best = full_em_tr_loss\n",
    "        count=0\n",
    "        best_full_em_model = full_em_model\n",
    "    if count>20: # no increase three time. stop.\n",
    "        full_em_model = best_full_em_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "print(\"full ensemble model trained.\")\n",
    "full_em_model.save(save_model_path+\"f_em.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_em_output_list = model_performance(\n",
    "    information = False, using_model=full_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=full_em_x_val, tr_y_val=y_val_1, ts_x_val=full_em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "full_em_tr_loss, full_em_tr_accuracy, full_em_tr_sensitivity, full_em_tr_specificity, full_em_tr_predictions, full_em_labeled_tr_predictions, full_em_tr_predictions_flat, full_em_roc_auc_tr, full_em_ts_loss, full_em_ts_accuracy, full_em_ts_sensitivity, full_em_ts_specificity, full_em_ts_predictions,full_em_labeled_ts_predictions, full_em_ts_predictions_flat, full_em_roc_auc_ts, full_em_roc_auc_total = full_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", full_em_roc_auc_total)\n",
    "print(\"Train AUC: \", full_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", full_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(full_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(full_em_tr_sensitivity)+\", \"+str(full_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(full_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(full_em_ts_sensitivity)+\", \"+str(full_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling t-ensemble  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "122/122 [==============================] - 2s 15ms/step - loss: 1.0244 - acc: 0.6721\n",
      "122/122 [==============================] - 1s 6ms/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.5489 - acc: 0.7295\n",
      "122/122 [==============================] - 0s 245us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.4067 - acc: 0.7951\n",
      "122/122 [==============================] - 0s 188us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.3687 - acc: 0.8689\n",
      "122/122 [==============================] - 0s 221us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2550 - acc: 0.9016\n",
      "122/122 [==============================] - 0s 139us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2671 - acc: 0.8934\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1336 - acc: 0.9426\n",
      "122/122 [==============================] - 0s 172us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1351 - acc: 0.9262\n",
      "122/122 [==============================] - 0s 131us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2324 - acc: 0.9262\n",
      "122/122 [==============================] - 0s 213us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.2143 - acc: 0.9426\n",
      "122/122 [==============================] - 0s 270us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.2142 - acc: 0.9344\n",
      "122/122 [==============================] - 0s 155us/step\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.2979 - acc: 0.9426\n",
      "122/122 [==============================] - 0s 180us/step\n",
      "transffered ensemble model trained.\n"
     ]
    }
   ],
   "source": [
    "# 1) parameter setting\n",
    "adam = optimizers.Adam(lr=0.005)\n",
    "input_drop_out_t_em = 0.5\n",
    "drop_out_t_em = 0.5\n",
    "layers = [100, 100, 100, 100]\n",
    "t_em_tr_loss_best = 100 # for saving best loss value \n",
    "best_t_em_model=[] #for saving best model\n",
    "count=0 # for early stopping\n",
    "\n",
    "\n",
    "# 2) model build\n",
    "input_t_em = Input(shape=(t_em_x_val.shape[1],))\n",
    "t_em_m_dp = Dropout(input_drop_out_t_em)(input_t_em)\n",
    "for i in layers:\n",
    "    t_em_m = Dense(i,activation='relu')(t_em_m_dp)\n",
    "    t_em_m_dp = Dropout(drop_out_em)(t_em_m)\n",
    "t_em_m_final = t_em_m_dp\n",
    "output_t_em = Dense(1, activation=\"sigmoid\")(t_em_m_final)\n",
    "t_em_model = Model(inputs=input_t_em,outputs=output_t_em)\n",
    "t_em_model.compile(optimizer=adam, \n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 3) Training: if no increase of tr_loss three times, stop training.\n",
    "\n",
    "#t_em_model.fit(t_em_x_val, y_val_1, batch_size=5, epochs = 100, validation_split = 0.1, callbacks=[early_stopping])\n",
    "while 1:\n",
    "    t_em_model.fit(t_em_x_val, y_val_1, batch_size=5, nb_epoch=1)\n",
    "    t_em_tr_loss=t_em_model.evaluate(t_em_x_val,y_val_1)[0]\n",
    "    if t_em_tr_loss < t_em_tr_loss_best: # new best model. count reset.\n",
    "        t_em_tr_loss_best = t_em_tr_loss\n",
    "        count=0\n",
    "        best_t_em_model = t_em_model\n",
    "    if count>3: # no increase three time. stop.\n",
    "        t_em_model = best_t_em_model\n",
    "        break\n",
    "    else: count=count+1\n",
    "\n",
    "print(\"transffered ensemble model trained.\")\n",
    "t_em_model.save(save_model_path+\"t_em.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 163us/step\n",
      "31/31 [==============================] - 0s 96us/step\n",
      "Overall AUC:  0.9649048625792812\n",
      "Train AUC:  1.0\n",
      "Test AUC:  0.7626262626262627\n",
      "Train Accuracy: 1.0\n",
      "Train Sensitivities & Specificities : 1.0, 1.0\n",
      "Test Accuracy: 0.774193525314331\n",
      "Test Sensitivities & Specificities : 0.6666666666666666, 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "test_results_m_1 = model_1_l_new.predict([test_x_val_1])\n",
    "test_results_m_2 = model_2_l_new.predict([test_x_val_2])\n",
    "test_results_m_3 = model_3_l_new.predict([test_x_val_3])\n",
    "test_results_m_4 = model_4_l_new.predict([test_x_val_4])\n",
    "test_results_m_5 = model_5_l_new.predict([test_x_val_5])\n",
    "test_results_m_6 = model_6_l_new.predict([test_x_val_6])\n",
    "\n",
    "test_results_m_sum = [test_results_m_1, test_results_m_2, test_results_m_3, test_results_m_4, test_results_m_5, test_results_m_6]\n",
    "test_results_m_select = []\n",
    "\n",
    "for i in range(len(select)):\n",
    "    test_results_m_select.append(test_results_m_sum[select[i]-1])\n",
    "\n",
    "t_em_test_x_val = np.concatenate(test_results_m_select, axis=1)\n",
    "\n",
    "t_em_output_list = model_performance(\n",
    "    information = False, using_model=t_em_model,Input_Prediction_Passively = False, \n",
    "    tr_x_val=t_em_x_val, tr_y_val=y_val_1, ts_x_val=t_em_test_x_val, ts_y_val=test_y_val_1,\n",
    "    output_list=[\"tr_loss\", \"tr_accuracy\", \"tr_sensitivity\", \"tr_specificity\", \"tr_predictions\",\n",
    "                 \"labeled_tr_predictions\", \"tr_predictions_flat\", \"roc_auc_tr\", \n",
    "                 \"ts_loss\", \"ts_accuracy\", \"ts_sensitivity\", \"ts_specificity\", \"ts_predictions\",\n",
    "                 \"labeled_ts_predictions\", \"ts_predictions_flat\", \"roc_auc_ts\", \n",
    "                 \"roc_auc_total\"])\n",
    "\n",
    "t_em_tr_loss, t_em_tr_accuracy, t_em_tr_sensitivity, t_em_tr_specificity, t_em_tr_predictions, t_em_labeled_tr_predictions, t_em_tr_predictions_flat, t_em_roc_auc_tr, t_em_ts_loss, t_em_ts_accuracy, t_em_ts_sensitivity, t_em_ts_specificity, t_em_ts_predictions,t_em_labeled_ts_predictions, t_em_ts_predictions_flat, t_em_roc_auc_ts, t_em_roc_auc_total = t_em_output_list\n",
    "\n",
    "print(\"Overall AUC: \", t_em_roc_auc_total)\n",
    "print(\"Train AUC: \", t_em_roc_auc_tr)\n",
    "print(\"Test AUC: \", t_em_roc_auc_ts)\n",
    "\n",
    "print(\"Train Accuracy: {}\".format(t_em_tr_accuracy))\n",
    "print(\"Train Sensitivities & Specificities : \"+str(t_em_tr_sensitivity)+\", \"+str(t_em_tr_specificity))\n",
    "print(\"Test Accuracy: {}\".format(t_em_ts_accuracy))\n",
    "print(\"Test Sensitivities & Specificities : \"+str(t_em_ts_sensitivity)+\", \"+str(t_em_ts_specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction result.\n",
    "\n",
    "tr_df_t_em = pd.DataFrame(data={\"patient\":list(train_data_1.index), \"hypothesis 1\": list(t_em_tr_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_tr_predictions), \"Platinum_Status\":list(y_val_1)})\n",
    "tr_df_t_em.to_csv(save_prediction_path+\"prediction_result_t_em_tr.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])\n",
    "\n",
    "ts_df_t_em = pd.DataFrame(data={\"patient\":list(test_data_1.index), \"hypothesis 1\": list(t_em_ts_predictions_flat), \n",
    "                        \"prediction\":list(t_em_labeled_ts_predictions), \"Platinum_Status\":list(test_y_val_1)})\n",
    "ts_df_t_em.to_csv(save_prediction_path+\"prediction_result_t_em_ts.csv\", index=False, header=True, columns = [\"patient\", \"hypothesis 1\", \"prediction\", \"Platinum_Status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< model1 > tr: 0.9918032786885246, ts: 0.8387096524238586\n",
      "< model2 > tr: 1.0, ts: 0.6451612710952759\n",
      "< model3 > tr: 0.9426229449569202, ts: 0.7096773982048035\n",
      "< model4 > tr: 0.9508196662683956, ts: 0.8387096524238586\n",
      "< mean-em > tr: 1.0, ts: 0.7096774193548387\n",
      "< d-comb em > tr: 1.0, ts: 0.6451612710952759\n",
      "< t-em > tr: 1.0, ts: 0.6774193644523621\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "tr_accuracy_list = [m_1_l_tr_accuracy, m_2_l_tr_accuracy, m_3_l_tr_accuracy, m_4_l_tr_accuracy, m_5_l_tr_accuracy, m_6_l_tr_accuracy]\n",
    "ts_accuracy_list = [m_1_l_accuracy, m_2_l_accuracy, m_3_l_accuracy, m_4_l_accuracy, m_5_l_accuracy, m_6_l_accuracy]\n",
    "tr_accuracy_select = []\n",
    "ts_accuracy_select = []\n",
    "\n",
    "for i in select:\n",
    "    label.append(\"model\"+str(i))\n",
    "    tr_accuracy_select.append(tr_accuracy_list[i-1])\n",
    "    ts_accuracy_select.append(ts_accuracy_list[i-1])\n",
    "\n",
    "label = label+[\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "tr_accuracy_select= tr_accuracy_select + [mean_em_tr_accuracy, em_tr_accuracy, t_em_tr_accuracy]\n",
    "ts_accuracy_select= ts_accuracy_select + [mean_em_ts_accuracy, em_ts_accuracy, t_em_ts_accuracy]\n",
    "\n",
    "for model_num in range(len(label)):\n",
    "    print(\"< \"+label[model_num]+\" > tr: \"+str(tr_accuracy_select[model_num])+\", ts: \"+str(ts_accuracy_select[model_num]))\n",
    "\n",
    "#label = [\"model1\",\"model2\",\"model3\",\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "#accuracy = [m1_accuracy,m2_accuracy,m3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy ]\n",
    "#print(\"model1: \"+str(accuracy[0])+\"\\nmodel2: \"+str(accuracy[1])+\"\\nmodel3: \"+str(accuracy[2])+\"\\nmean-em: \"+str(accuracy[3])+\"\\nd-comb em: \"+str(accuracy[4])+\"\\nt-em: \"+str(accuracy[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_x():\n",
    "    # this is for plotting purpose\n",
    "    plt.figure(figsize=(30,20))\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([min(m_1_ts_accuracy,m_2_ts_accuracy,m_3_ts_accuracy,m_4_ts_accuracy, mean_em_ts_accuracy,em_ts_accuracy,t_em_ts_accuracy)-0.02,1])\n",
    "    index = np.arange(len(label))\n",
    "    plt.bar(index, accuracy,color=['red', 'orange', 'yellow', \"green\",'blue', 'purple'],alpha=0.5,width=0.3)\n",
    "    plt.xlabel('Method', fontsize=35)\n",
    "    plt.ylabel('Accuracy', fontsize=35)\n",
    "    plt.yticks(fontsize=30)    \n",
    "    plt.xticks(index, label, fontsize=30, rotation=90)\n",
    "    plt.title('Performance Comparison for each Ensemble Model',fontsize=40)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_1_ts_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-a28903a7e4e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_bar_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-361019c0cd9e>\u001b[0m in \u001b[0;36mplot_bar_x\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_1_ts_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_2_ts_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_3_ts_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_4_ts_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_em_ts_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mem_ts_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_em_ts_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'orange'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yellow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"green\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'purple'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm_1_ts_accuracy' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrcAAARiCAYAAAAOZ6xTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3c+rpmUdx/HPNycJhHLhBKEGLkbMXXWQwI0RwehCd6EQUYRuslUEBlHhnxBYMURIQcqsyoXgSggiwTNEkiPCYJQHAyeTNi1MuFrMCMfj0fM4Pv74MK8XHDjX/Vz3/Xz3b+7rmbVWAAAAAAAAoMHHPuwBAAAAAAAAYFPiFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFDjyLg1M7+amZdn5q9v8/nMzE9n5tzMPDMzX9j+mAAAAAAAALDZm1sPJzn5Dp/fnuTExb/7kvz8vY8FAAAAAAAAb3Vk3Fpr/SHJv99hy11Jfr0ueCrJ1TPzmW0NCAAAAAAAAG/Yxm9uXZvkxX3rvYvXAAAAAAAAYKuObeEZc8i1dejGmfty4ejCXHXVVV+86aabtvD1AAAAAAAANDlz5sy/1lrHL+XebcStvSTX71tfl+SlwzautU4lOZUkOzs7a3d3dwtfDwAAAAAAQJOZ+ful3ruNYwkfS/KNueBLSf6z1vrnFp4LAAAAAAAAb3Lkm1sz80iS25JcMzN7SX6c5ONJstb6RZLHk9yR5FyS/yb51vs1LAAAAAAAAJe3I+PWWuueIz5fSb6ztYkAAAAAAADgbWzjWEIAAAAAAAD4QIhbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqbBS3ZubkzDw/M+dm5oFDPv/szDw5M3+emWdm5o7tjwoAAAAAAMDl7si4NTNXJHkoye1Jbk5yz8zcfGDbD5OcXmt9PsndSX627UEBAAAAAABgkze3bklybq31wlrrtSSPJrnrwJ6V5JMX//9Ukpe2NyIAAAAAAABcsEncujbJi/vWexev7feTJF+fmb0kjyf57mEPmpn7ZmZ3ZnbPnz9/CeMCAAAAAABwOdskbs0h19aB9T1JHl5rXZfkjiS/mZm3PHutdWqttbPW2jl+/Pi7nxYAAAAAAIDL2iZxay/J9fvW1+Wtxw5+O8npJFlr/SnJJ5Jcs40BAQAAAAAA4A2bxK2nk5yYmRtm5sokdyd57MCefyT5SpLMzOdyIW45dxAAAAAAAICtOjJurbVeT3J/kieSPJfk9Frr2Zl5cGbuvLjte0nunZm/JHkkyTfXWgePLgQAAAAAAID35Ngmm9Zajyd5/MC1H+37/2ySW7c7GgAAAAAAALzZJscSAgAAAAAAwEeCuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUGOjuDUzJ2fm+Zk5NzMPvM2er83M2Zl5dmZ+u90xAQAAAAAAIDl21IaZuSLJQ0m+mmQvydMz89ha6+y+PSeS/CDJrWutV2fm0+/XwAAAAAAAAFy+Nnlz65Yk59ZaL6y1XkvyaJK7Duy5N8lDa61Xk2St9fJ2xwQAAAAAAIDN4ta1SV7ct967eG2/G5PcODN/nJmnZubkYQ+amftmZndmds+fP39pEwMAAAAAAHDZ2iRuzSHX1oH1sSQnktyW5J4kv5yZq99y01qn1lo7a62d48ePv9tZAQAAAAAAuMxtErf2kly/b31dkpcO2fP7tdb/1lp/S/J8LsQuAAAAAAAA2JpN4tbTSU7MzA0zc2WSu5M8dmDP75J8OUlm5ppcOKbwhW0OCgAAAAAAAEfGrbXW60nuT/JEkueSnF5rPTszD87MnRe3PZHklZk5m+TJJN9fa73yfg0NAAAAAADA5WnWOvjzWR+MnZ2dtbu7+6F8NwAAAAAAAB+emTmz1tq5lHs3OZYQAAAAAAAAPhLELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQD+z979hWh213cc/3zNkvbC1ILJRUlSEzCWbkVIWYKlF1qUkvRic5OWBKQWQnOVSrEUUixS4pVKEQppMaBohRJjLtpFVnLhH1pKE7JgEaMEllTMkoLbanMjmqb99WLGOGwmu8+sM7Pz6b5esPCcc37zzPfqxzP7nnMGAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUGOjuDUzd87MczNzdmYeusi6e2ZmzcyJ/RsRAAAAAAAAtlwybs3MNUkeSXJXkuNJ7puZ47usuy7JB5I8vd9DAgAAAAAAQLLZnVt3JDm71np+rfVykseS3L3Luo8k+ViSH+3jfAAAAAAAAPCqTeLWjUle2HF8bvvcq2bm9iQ3r7W+eLE3mpkHZubMzJw5f/78nocFAAAAAADg6rZJ3Jpdzq1XL868IcknkvzJpd5orfXoWuvEWuvEDTfcsPmUAAAAAAAAkM3i1rkkN+84vinJizuOr0vy9iRfm5nvJHlnklMzc2K/hgQAAAAAAIBks7j1TJLbZubWmbk2yb1JTv3k4lrrpbXW9WutW9ZatyR5KsnJtdaZA5kYAAAAAACAq9Yl49Za65UkDyZ5Msm3kzy+1np2Zh6emZMHPSAAAAAAAAD8xLFNFq21Tic5fcG5D7/O2nf/7GMBAAAAAADAa23yWEIAAAAAAAA4EsQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgHMLKecAAATc0lEQVQAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAaG8WtmblzZp6bmbMz89Au1z84M9+amW/MzJdn5i37PyoAAAAAAABXu0vGrZm5JskjSe5KcjzJfTNz/IJlX09yYq31jiRPJPnYfg8KAAAAAAAAm9y5dUeSs2ut59daLyd5LMndOxestb661vrh9uFTSW7a3zEBAAAAAABgs7h1Y5IXdhyf2z73eu5P8qXdLszMAzNzZmbOnD9/fvMpAQAAAAAAIJvFrdnl3Np14cz7kpxI8vHdrq+1Hl1rnVhrnbjhhhs2nxIAAAAAAACSHNtgzbkkN+84vinJixcumpn3JvlQknettX68P+MBAAAAAADAT21y59YzSW6bmVtn5tok9yY5tXPBzNye5JNJTq61vrf/YwIAAAAAAMAGcWut9UqSB5M8meTbSR5faz07Mw/PzMntZR9P8sYkX5iZf52ZU6/zdgAAAAAAAHDZNnksYdZap5OcvuDch3e8fu8+zwUAAAAAAACvscljCQEAAAAAAOBIELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGqIWwAAAAAAANQQtwAAAAAAAKghbgEAAAAAAFBD3AIAAAAAAKCGuAUAAAAAAEANcQsAAAAAAIAa4hYAAAAAAAA1xC0AAAAAAABqiFsAAAAAAADUELcAAAAAAACoIW4BAAAAAABQQ9wCAAAAAACghrgFAAAAAABADXELAAAAAACAGuIWAAAAAAAANcQtAAAAAAAAaohbAAAAAAAA1BC3AAAAAAAAqCFuAQAAAAAAUEPcAgAAAAAAoIa4BQAAAAAAQA1xCwAAAAAAgBriFgAAAAAAADXELQAAAAAAAGpsFLdm5s6ZeW5mzs7MQ7tc/7mZ+fz29adn5pb9HhQAAAAAAAAuGbdm5pokjyS5K8nxJPfNzPELlt2f5Adrrbcm+USSj+73oAAAAAAAALDJnVt3JDm71np+rfVykseS3H3BmruTfHb79RNJ3jMzs39jAgAAAAAAwGZx68YkL+w4Prd9btc1a61XkryU5M37MSAAAAAAAAD8xLEN1ux2B9a6jDWZmQeSPLB9+OOZ+eYG3x+An7o+yX9c6SEAytg7AfbGvgmwd/ZOgL37lcv9wk3i1rkkN+84vinJi6+z5tzMHEvypiTfv/CN1lqPJnk0SWbmzFrrxOUMDXC1sncC7J29E2Bv7JsAe2fvBNi7mTlzuV+7yWMJn0ly28zcOjPXJrk3yakL1pxK8v7t1/ck+cpa6zV3bgEAAAAAAMDP4pJ3bq21XpmZB5M8meSaJJ9eaz07Mw8nObPWOpXkU0k+NzNns3XH1r0HOTQAAAAAAABXp00eS5i11ukkpy849+Edr3+U5Hf3+L0f3eN6AOydAJfD3gmwN/ZNgL2zdwLs3WXvnePpgQAAAAAAALTY5G9uAQAAAAAAwJFw4HFrZu6cmedm5uzMPLTL9Z+bmc9vX396Zm456JkAjroN9s4Pzsy3ZuYbM/PlmXnLlZgT4Ki41L65Y909M7Nm5sRhzgdwFG2yd87M721/7nx2Zv7usGcEOGo2+Hn9l2fmqzPz9e2f2X/nSswJcFTMzKdn5nsz883XuT4z81fb++o3ZubXN3nfA41bM3NNkkeS3JXkeJL7Zub4BcvuT/KDtdZbk3wiyUcPciaAo27DvfPrSU6std6R5IkkHzvcKQGOjg33zczMdUk+kOTpw50Q4OjZZO+cmduS/FmS31xr/VqSPz70QQGOkA0/d/55ksfXWrcnuTfJXx/ulABHzmeS3HmR63cluW373wNJ/maTNz3oO7fuSHJ2rfX8WuvlJI8lufuCNXcn+ez26yeSvGdm5oDnAjjKLrl3rrW+utb64fbhU0luOuQZAY6STT5zJslHsvXLAD86zOEAjqhN9s4/TPLIWusHSbLW+t4hzwhw1Gyyd64kv7D9+k1JXjzE+QCOnLXWPyb5/kWW3J3kb9eWp5L84sz80qXe96Dj1o1JXthxfG773K5r1lqvJHkpyZsPeC6Ao2yTvXOn+5N86UAnAjjaLrlvzsztSW5ea33xMAcDOMI2+cz5tiRvm5l/npmnZuZiv3ELcDXYZO/8iyTvm5lzSU4n+aPDGQ2g1l7/LzRJcuzAxtmy2x1Y6zLWAFxNNt4XZ+Z9SU4kedeBTgRwtF1035yZN2Tr8dd/cFgDARTY5DPnsWw9Hubd2XpSwD/NzNvXWv91wLMBHFWb7J33JfnMWusvZ+Y3knxue+/834MfD6DSZTWig75z61ySm3cc35TX3or76pqZOZat23UvdosawP93m+ydmZn3JvlQkpNrrR8f0mwAR9Gl9s3rkrw9yddm5jtJ3pnk1MycOLQJAY6eTX9e/4e11n+vtf4tyXPZil0AV6tN9s77kzyeJGutf0ny80muP5TpADpt9H+hFzrouPVMkttm5taZuTZbf0Tx1AVrTiV5//bre5J8Za3lzi3ganbJvXP78VqfzFbY8rcPgKvdRffNtdZLa63r11q3rLVuydbfKjy51jpzZcYFOBI2+Xn975P8VpLMzPXZekzh84c6JcDRssne+d0k70mSmfnVbMWt84c6JUCXU0l+f7a8M8lLa61/v9QXHehjCddar8zMg0meTHJNkk+vtZ6dmYeTnFlrnUryqWzdnns2W3ds3XuQMwEcdRvunR9P8sYkX5iZJPnuWuvkFRsa4AracN8EYIcN984nk/z2zHwryf8k+dO11n9euan/r707pkEgCMIw+k+Bg1OFCpBATgA2SBCAKXTQ7hWLgCsgZMJ7Arac5tvMAPzWztm5JrlX1SVzrdbJR37gn1XVI3PN9fK+R3hNckiSMcYt8z7hMckzySvJede7ZisAAAAAAABdfHstIQAAAAAAAHyMuAUAAAAAAEAb4hYAAAAAAABtiFsAAAAAAAC0IW4BAAAAAADQhrgFAAAAAABAG+IWAAAAAAAAbYhbAAAAAAAAtLEBl13E0wD9DxsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 2160x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_bar_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
