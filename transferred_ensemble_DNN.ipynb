{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['cancer-correct'] = 0\n",
    "    result['cancer-wrong'] = 0\n",
    "    result['normal-correct'] = 0\n",
    "    result['normal-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['normal-correct'] += 1\n",
    "            else :\n",
    "                result['cancer-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['normal-wrong'] += 1\n",
    "            else :\n",
    "                result['cancer-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "        #print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['cancer-correct']/(result['cancer-correct']+result['cancer-wrong'])\n",
    "    specificity=result['normal-correct']/(result['normal-correct']+result['normal-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num=1\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C://test/TCGA_with_GEO/\"\n",
    "types = [\"TCGA_with_GEO_input_ensemble_CV_3000\", \"TCGA_with_GEO_input_ensemble_foundation_308\",\n",
    "         \"TCGA_with_GEO_input_ensemble_foundation_2267\", \"TCGA_with_GEO_input_ensemble_Mean_3000\",\n",
    "         \"TCGA_with_GEO_input_ensemble_VAR_3000\"\n",
    "         ]\n",
    "select = [4, 0, 1]\n",
    "idx_col = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] file_name:  TCGA_with_GEO_input_ensemble_VAR_3000 \n",
      "sample : 26248  \n",
      "features : 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XIST</th>\n",
       "      <th>CMPK2</th>\n",
       "      <th>MMP1</th>\n",
       "      <th>EGFR</th>\n",
       "      <th>RPS4Y1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM1000612</th>\n",
       "      <td>1.390</td>\n",
       "      <td>-1.111</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000613</th>\n",
       "      <td>-1.500</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-2.664</td>\n",
       "      <td>-0.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000614</th>\n",
       "      <td>0.485</td>\n",
       "      <td>1.006</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000615</th>\n",
       "      <td>0.702</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.298</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000616</th>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             XIST  CMPK2   MMP1   EGFR  RPS4Y1\n",
       "patient                                       \n",
       "GSM1000612  1.390 -1.111 -0.231 -0.107   0.072\n",
       "GSM1000613 -1.500  0.131 -0.145 -2.664  -0.053\n",
       "GSM1000614  0.485  1.006 -0.259 -0.039   0.163\n",
       "GSM1000615  0.702 -0.035  0.298  1.075   0.101\n",
       "GSM1000616 -0.375 -0.094  0.192  0.339  -0.009"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_1 = path+types[select[0]]+\".csv\"\n",
    "data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "sample_1,features_1 = data_1.shape\n",
    "features_1 = features_1-3\n",
    "print(\"[1] file_name: \", types[select[0]], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))\n",
    "data_1.iloc[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] file_name:  TCGA_with_GEO_input_ensemble_CV_3000 \n",
      "sample : 26248  \n",
      "features : 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPDC7</th>\n",
       "      <th>SGK2</th>\n",
       "      <th>RHEBL1</th>\n",
       "      <th>PLK4</th>\n",
       "      <th>IGKC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM1000612</th>\n",
       "      <td>0.442</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000613</th>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.498</td>\n",
       "      <td>0.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000614</th>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-1.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000615</th>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000616</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.773</td>\n",
       "      <td>1.061</td>\n",
       "      <td>0.769</td>\n",
       "      <td>-0.270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DEPDC7   SGK2  RHEBL1   PLK4   IGKC\n",
       "patient                                        \n",
       "GSM1000612   0.442 -1.856   0.096  0.666  0.343\n",
       "GSM1000613   0.381 -0.129  -0.808 -1.498  0.624\n",
       "GSM1000614  -0.271 -0.719   0.216 -0.363 -1.160\n",
       "GSM1000615  -0.525  0.048   0.127 -0.556  0.644\n",
       "GSM1000616   0.285  0.773   1.061  0.769 -0.270"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_2 = path+types[select[1]]+\".csv\"\n",
    "data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "sample_2,features_2 = data_2.shape\n",
    "features_2 = features_2-3\n",
    "print(\"[2] file_name: \", types[select[1]], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))\n",
    "data_2.iloc[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] file_name:  TCGA_with_GEO_input_ensemble_foundation_308 \n",
      "sample : 26248  \n",
      "features : 308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XIST</th>\n",
       "      <th>CMPK2</th>\n",
       "      <th>MMP1</th>\n",
       "      <th>EGFR</th>\n",
       "      <th>RPS4Y1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM1000612</th>\n",
       "      <td>1.390</td>\n",
       "      <td>-1.111</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000613</th>\n",
       "      <td>-1.500</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-2.664</td>\n",
       "      <td>-0.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000614</th>\n",
       "      <td>0.485</td>\n",
       "      <td>1.006</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000615</th>\n",
       "      <td>0.702</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.298</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM1000616</th>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             XIST  CMPK2   MMP1   EGFR  RPS4Y1\n",
       "patient                                       \n",
       "GSM1000612  1.390 -1.111 -0.231 -0.107   0.072\n",
       "GSM1000613 -1.500  0.131 -0.145 -2.664  -0.053\n",
       "GSM1000614  0.485  1.006 -0.259 -0.039   0.163\n",
       "GSM1000615  0.702 -0.035  0.298  1.075   0.101\n",
       "GSM1000616 -0.375 -0.094  0.192  0.339  -0.009"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_3 = path+types[select[2]]+\".csv\"\n",
    "data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "sample_3,features_3 = data_3.shape\n",
    "features_3 = features_3-3\n",
    "print(\"[3] file_name: \", types[select[2]], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))\n",
    "data_1.iloc[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_ids = list(pd.read_csv(path+\"Train_14_index.csv\").x)\n",
    "test_p_ids = list(pd.read_csv(path+\"Test_14_index.csv\").x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = data_1.iloc[train_p_ids]\n",
    "test_data_1 = data_1.iloc[test_p_ids]\n",
    "y_val_1 = train_data_1.result\n",
    "x_val_1 = train_data_1.drop([\"cancer_code\",\"result\",\"index\"],axis=1)\n",
    "test_y_val_1 = test_data_1.result\n",
    "test_x_val_1 = test_data_1.drop([\"cancer_code\",\"result\",\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2 = data_2.iloc[train_p_ids]\n",
    "test_data_2 = data_2.iloc[test_p_ids]\n",
    "y_val_2 = train_data_2.result\n",
    "x_val_2 = train_data_2.drop([\"cancer_code\",\"result\",\"index\"],axis=1)\n",
    "test_y_val_2 = test_data_2.result\n",
    "test_x_val_2 = test_data_2.drop([\"cancer_code\",\"result\",\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_3 = data_3.iloc[train_p_ids]\n",
    "test_data_3 = data_3.iloc[test_p_ids]\n",
    "y_val_3 = train_data_3.result\n",
    "x_val_3 = train_data_3.drop([\"cancer_code\",\"result\",\"index\"],axis=1)\n",
    "test_y_val_3 = test_data_3.result\n",
    "test_x_val_3 = test_data_3.drop([\"cancer_code\",\"result\",\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Seperate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building seperate model for ensemble(model 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "Drop_out = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11810 samples, validate on 1313 samples\n",
      "Epoch 1/50\n",
      "11810/11810 [==============================] - 9s 732us/step - loss: 0.8098 - acc: 0.5324 - val_loss: 0.7128 - val_acc: 7.6161e-04\n",
      "Epoch 2/50\n",
      "11810/11810 [==============================] - 2s 203us/step - loss: 0.4232 - acc: 0.8008 - val_loss: 0.1956 - val_acc: 0.9474\n",
      "Epoch 3/50\n",
      "11810/11810 [==============================] - 3s 222us/step - loss: 0.2652 - acc: 0.8955 - val_loss: 0.0163 - val_acc: 0.9954\n",
      "Epoch 4/50\n",
      "11810/11810 [==============================] - 3s 216us/step - loss: 0.2265 - acc: 0.9130 - val_loss: 0.0990 - val_acc: 0.9619\n",
      "Epoch 5/50\n",
      "11810/11810 [==============================] - 2s 211us/step - loss: 0.2017 - acc: 0.9222 - val_loss: 0.0360 - val_acc: 0.9901\n",
      "Epoch 6/50\n",
      "11810/11810 [==============================] - 3s 219us/step - loss: 0.1942 - acc: 0.9257 - val_loss: 0.0395 - val_acc: 0.9886\n",
      "Epoch 7/50\n",
      "11810/11810 [==============================] - 3s 217us/step - loss: 0.1882 - acc: 0.9287 - val_loss: 0.0244 - val_acc: 0.9909\n",
      "Epoch 8/50\n",
      "11810/11810 [==============================] - 3s 272us/step - loss: 0.1714 - acc: 0.9352 - val_loss: 0.0504 - val_acc: 0.9840\n",
      "Epoch 9/50\n",
      "11810/11810 [==============================] - 3s 235us/step - loss: 0.1640 - acc: 0.9390 - val_loss: 0.0272 - val_acc: 0.9909\n",
      "Epoch 10/50\n",
      "11810/11810 [==============================] - 3s 227us/step - loss: 0.1489 - acc: 0.9443 - val_loss: 0.0122 - val_acc: 0.9924\n",
      "Epoch 11/50\n",
      "11810/11810 [==============================] - 3s 219us/step - loss: 0.1596 - acc: 0.9390 - val_loss: 0.0154 - val_acc: 0.9947\n",
      "Epoch 12/50\n",
      "11810/11810 [==============================] - 2s 211us/step - loss: 0.1505 - acc: 0.9413 - val_loss: 0.0305 - val_acc: 0.9931\n",
      "Epoch 13/50\n",
      "11810/11810 [==============================] - 3s 217us/step - loss: 0.1474 - acc: 0.9420 - val_loss: 0.0705 - val_acc: 0.9787\n",
      "Epoch 14/50\n",
      "11810/11810 [==============================] - 3s 214us/step - loss: 0.1442 - acc: 0.9447 - val_loss: 0.0283 - val_acc: 0.9878\n",
      "Epoch 15/50\n",
      " 1150/11810 [=>............................] - ETA: 2s - loss: 0.1412 - acc: 0.9522"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-28ee888dfc9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../models/DNN/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_DNN_model_1.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hgh97\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_m_1 = Input(shape=(features_1,))\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "input_drop_m_1 = Dropout(Drop_out)(input_m_1)\n",
    "h1_m_1 = Dense(500,activation='relu')(input_drop_m_1)\n",
    "h1_m_1 = Dropout(Drop_out)(h1_m_1)\n",
    "h2_m_1 = Dense(500,activation='relu')(h1_m_1)\n",
    "h2_m_1 = Dropout(Drop_out)(h2_m_1)\n",
    "h3_m_1 = Dense(200,activation='relu')(h2_m_1) \n",
    "h3_m_1 = Dropout(Drop_out)(h3_m_1)\n",
    "h4_m_1 = Dense(300,activation='relu')(h3_m_1) \n",
    "h4_m_1 = Dropout(Drop_out)(h4_m_1)\n",
    "h5_m_1 = Dense(200,activation='relu')(h4_m_1) \n",
    "h5_m_1 = Dropout(Drop_out)(h5_m_1)\n",
    "h6_m_1 = Dense(100,activation='relu')(h5_m_1) \n",
    "h6_m_1 = Dropout(Drop_out)(h6_m_1)\n",
    "output_m_1 = Dense(1,activation=\"sigmoid\")(h6_m_1) \n",
    "model_1 = Model(inputs=input_m_1,outputs=output_m_1)\n",
    "\n",
    "model_1.compile(optimizer=adam, \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_1.fit(x_val_1, y_val_1, epochs=50, callbacks=[early_stopping], validation_split=0.1, batch_size=50)\n",
    "\n",
    "model_1.save(\"../models/DNN/\"+str(num)+\"_DNN_model_1.h5\")\n",
    "print(\"model_1 saved model to disk\")\n",
    "print(\"[1] file_name: \", types[select[0]], \"\\nsample : {}  \\nfeatures : {}\".format(sample_1,features_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_1_ = load_model(\"DNN_model_1.h5\")\n",
    "model_1[0]\n",
    "m1_acc = model_1_.evaluate(test_x_val_1, test_y_val_1)\n",
    "print(m1_acc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_m_2 = Input(shape=(features_2,))\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "input_drop_m_2 = Dropout(Drop_out)(input_m_2)\n",
    "h1_m_2 = Dense(500,activation='relu')(input_drop_m_2)\n",
    "h1_m_2 = Dropout(Drop_out)(h1_m_2)\n",
    "h2_m_2 = Dense(500,activation='relu')(h1_m_2)\n",
    "h2_m_2 = Dropout(Drop_out)(h2_m_2)\n",
    "h3_m_2 = Dense(200,activation='relu')(h2_m_2) \n",
    "h3_m_2 = Dropout(Drop_out)(h3_m_2)\n",
    "h4_m_2 = Dense(300,activation='relu')(h3_m_2) \n",
    "h4_m_2 = Dropout(Drop_out)(h4_m_2)\n",
    "h5_m_2 = Dense(200,activation='relu')(h4_m_2) \n",
    "h5_m_2 = Dropout(Drop_out)(h5_m_2)\n",
    "h6_m_2 = Dense(100,activation='relu')(h5_m_2) \n",
    "h6_m_2 = Dropout(Drop_out)(h6_m_2)\n",
    "output_m_2 = Dense(1,activation=\"sigmoid\")(h6_m_2) \n",
    "model_2 = Model(inputs=input_m_2,outputs=output_m_2)\n",
    "\n",
    "model_2.compile(optimizer=adam, \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(x_val_2, y_val_2, epochs=50, callbacks=[early_stopping], validation_split=0.1, batch_size=50)\n",
    "\n",
    "model_2.save(\"../models/DNN/\"+str(num)+\"_DNN_model_2.h5\")\n",
    "print(\"model_2 saved model to disk\")\n",
    "print(\"[2] file_name: \", types[select[1]], \"\\nsample : {}  \\nfeatures : {}\".format(sample_2,features_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_m_3 = Input(shape=(features_3,))\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "input_drop_m_3 = Dropout(Drop_out)(input_m_3)\n",
    "h1_m_3 = Dense(500,activation='relu')(input_drop_m_3)\n",
    "h1_m_3 = Dropout(Drop_out)(h1_m_3)\n",
    "h2_m_3 = Dense(500,activation='relu')(h1_m_3)\n",
    "h2_m_3 = Dropout(Drop_out)(h2_m_3)\n",
    "h3_m_3 = Dense(200,activation='relu')(h2_m_3) \n",
    "h3_m_3 = Dropout(Drop_out)(h3_m_3)\n",
    "h4_m_3 = Dense(300,activation='relu')(h3_m_3) \n",
    "h4_m_3 = Dropout(Drop_out)(h4_m_3)\n",
    "h5_m_3 = Dense(200,activation='relu')(h4_m_3) \n",
    "h5_m_3 = Dropout(Drop_out)(h5_m_3)\n",
    "h6_m_3 = Dense(100,activation='relu')(h5_m_3) \n",
    "h6_m_3 = Dropout(Drop_out)(h6_m_3)\n",
    "output_m_3 = Dense(1,activation=\"sigmoid\")(h6_m_3) \n",
    "model_3 = Model(inputs=input_m_3,outputs=output_m_3)\n",
    "\n",
    "model_3.compile(optimizer=adam, \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3.fit(x_val_3, y_val_3, epochs=50, callbacks=[early_stopping], validation_split=0.1, batch_size=50)\n",
    "\n",
    "model_3.save(\"../models/DNN/\"+str(num)+\"_DNN_model_3.h5\")\n",
    "print(\"model_3 saved model to disk\")\n",
    "print(\"[1] file_name: \", types[select[2]], \"\\nsample : {}  \\nfeatures : {}\".format(sample_3,features_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating seperate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1_tr_loss,m_1_tr_accuracy=model_1.evaluate(x_val_1,y_val_1)\n",
    "m_2_tr_loss,m_2_tr_accuracy=model_2.evaluate(x_val_2,y_val_2)\n",
    "m_3_tr_loss,m_3_tr_accuracy=model_3.evaluate(x_val_3,y_val_3)\n",
    "\n",
    "m_1_loss,m_1_accuracy= model_1.evaluate(test_x_val_1,test_y_val_1)\n",
    "m_2_loss,m_2_accuracy= model_2.evaluate(test_x_val_2,test_y_val_2)\n",
    "m_3_loss,m_3_accuracy= model_3.evaluate(test_x_val_3,test_y_val_3)\n",
    "\n",
    "m_1_predictions = model_1.predict(x_val_1)\n",
    "labeled_m_1_predictions = np.where(m_1_predictions > 0.5, 1, 0).flatten()\n",
    "m_1_tr_sensitivity, m_1_tr_specificity = check_correct(labeled_m_1_predictions, y_val_1)\n",
    "m_1_test_predictions = model_1.predict(test_x_val_1)\n",
    "labeled_m_1_test_predictions = np.where(m_1_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_1_sensitivity, m_1_specificity = check_correct(labeled_m_1_test_predictions, test_y_val_1)\n",
    "\n",
    "m_2_predictions = model_2.predict(x_val_2)\n",
    "labeled_m_2_predictions = np.where(m_2_predictions > 0.5, 1, 0).flatten()\n",
    "m_2_tr_sensitivity, m_2_tr_specificity = check_correct(labeled_m_2_predictions, y_val_2)\n",
    "m_2_test_predictions = model_2.predict(test_x_val_2)\n",
    "labeled_m_2_test_predictions = np.where(m_2_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_2_sensitivity, m_2_specificity = check_correct(labeled_m_2_test_predictions, test_y_val_2)\n",
    "\n",
    "m_3_predictions = model_3.predict(x_val_3)\n",
    "labeled_m_3_predictions = np.where(m_3_predictions > 0.5, 1, 0).flatten()\n",
    "m_3_tr_sensitivity, m_3_tr_specificity = check_correct(labeled_m_3_predictions, y_val_3)\n",
    "m_3_test_predictions = model_3.predict(test_x_val_3)\n",
    "labeled_m_3_test_predictions = np.where(m_3_test_predictions > 0.5, 1, 0).flatten()\n",
    "m_3_sensitivity, m_3_specificity = check_correct(labeled_m_3_test_predictions, test_y_val_3)\n",
    "\n",
    "\n",
    "print(types[select[0]],types[select[1]],types[select[2]])\n",
    "print(\"../models/DNN/\"+str(m_1_tr_accuracy)+\", \"+str(m_1_accuracy))\n",
    "print(\"\\nTrain Accuracy for model 1, 2, 3 : {},{},{}\".format(m_1_tr_accuracy,m_2_tr_accuracy,m_3_tr_accuracy))\n",
    "print(\"\\nTest Accuracy for model 1, 2, 3 : {},{},{}\".format(m_1_accuracy,m_2_accuracy,m_3_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building original ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1_predictions = model_1.predict(x_val_1)\n",
    "m_2_predictions = model_2.predict(x_val_2)\n",
    "m_3_predictions = model_3.predict(x_val_3)\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "ensemble_x_val = np.concatenate([m_1_predictions, m_2_predictions, m_3_predictions], axis=1)\n",
    "\n",
    "ensemble_input = Input(shape=(3,))\n",
    "h1_ensemble = Dense(2,activation=\"relu\")(ensemble_input)\n",
    "ensemble_output = Dense(1,activation='sigmoid')(h1_ensemble)\n",
    "ensemble_model = Model(inputs=ensemble_input, outputs=ensemble_output)\n",
    "\n",
    "ensemble_model.compile(optimizer=adam, \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ensemble_model.fit(ensemble_x_val, y_val_1, epochs=10, batch_size = 1, validation_split = 0.1, callbacks=[early_stopping])\n",
    "\n",
    "ensemble_model.save(\"../models/DNN/\"+str(num)+\"_ensemble_DNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _DNN Combiner_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1_tr_predictions = model_1.predict(x_val_1)\n",
    "m_2_tr_predictions = model_2.predict(x_val_2)\n",
    "m_3_tr_predictions = model_3.predict(x_val_3)\n",
    "\n",
    "m_1_test_predictions = model_1.predict(test_x_val_1)\n",
    "m_2_test_predictions = model_2.predict(test_x_val_2)\n",
    "m_3_test_predictions = model_3.predict(test_x_val_3)\n",
    "\n",
    "ensemble_x_val = np.concatenate([m_1_tr_predictions, m_2_tr_predictions, m_3_tr_predictions], axis=1)\n",
    "ensemble_test_x_val = np.concatenate([m_1_test_predictions, m_2_test_predictions, m_3_test_predictions], axis=1)\n",
    "\n",
    "em_loss,em_accuracy= ensemble_model.evaluate(ensemble_test_x_val,test_y_val_1)\n",
    "\n",
    "ensemble_predictions = ensemble_model.predict(ensemble_x_val)\n",
    "labeled_ensemble_predictions = np.where(ensemble_predictions > 0.5, 1, 0).flatten()\n",
    "em_tr_sensitivity, em_tr_specificity = check_correct(labeled_ensemble_predictions, y_val_1)\n",
    "ensemble_test_predictions = ensemble_model.predict(ensemble_test_x_val)\n",
    "labeled_ensemble_test_predictions = np.where(ensemble_test_predictions > 0.5, 1, 0).flatten()\n",
    "em_sensitivity, em_specificity = check_correct(labeled_ensemble_test_predictions, test_y_val_1)\n",
    "\n",
    "print(\"Accuracy for DNN combiner ensemble : {}\".format(em_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating _mean_ ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tr_predictions=(m_1_tr_predictions+m_2_tr_predictions+m_3_tr_predictions)/3\n",
    "mean_predictions=(m_1_test_predictions+m_2_test_predictions+m_3_test_predictions)/3\n",
    "labled_predictions = np.where(mean_predictions > 0.5, 1, 0).flatten()\n",
    "mean_em_accuracy = sum(labled_predictions==test_y_val_1.values)/len(test_y_val_1)\n",
    "\n",
    "labeled_mean_em_predictions = np.where(mean_tr_predictions > 0.5, 1, 0).flatten()\n",
    "mean_em_tr_sensitivity, mean_em_tr_specificity = check_correct(labeled_mean_em_predictions, y_val_1)\n",
    "labeled_mean_em_test_predictions = np.where(mean_predictions > 0.5, 1, 0).flatten()\n",
    "mean_em_sensitivity, mean_em_specificity = check_correct(labeled_mean_em_test_predictions, test_y_val_1)\n",
    "mean_em_tr_accuracy = sum(labeled_mean_em_predictions==y_val_1.values)/len(y_val_1)\n",
    "\n",
    "print(\"Accuracy for mean ensemble : {}\".format(mean_em_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferred Ensemble Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making new input data for t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model = Model(inputs=[input_m_1], outputs=[h6_m_1])\n",
    "results_m_1 = model.predict([x_val_1])\n",
    "\n",
    "model = Model(inputs=[input_m_2], outputs=[h6_m_2])\n",
    "results_m_2 = model.predict([x_val_2])\n",
    "\n",
    "model = Model(inputs=[input_m_3], outputs=[h6_m_3])\n",
    "results_m_3 = model.predict([x_val_3])\n",
    "\n",
    "t_ensemble_x_val = np.concatenate([results_m_1, results_m_2, results_m_3], axis=1)\n",
    "print(t_ensemble_x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling t-ensemble  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ensemble_input = Input(shape=(t_ensemble_x_val.shape[1],))\n",
    "t_ensemble_h1 = Dense(300,activation='relu')(t_ensemble_input)\n",
    "t_ensemble_h1 = Dropout(Drop_out)(t_ensemble_h1)\n",
    "t_ensemble_h2 = Dense(150,activation='relu')(t_ensemble_h1)\n",
    "t_ensemble_h2 = Dropout(Drop_out)(t_ensemble_h2)\n",
    "t_ensemble_h3 = Dense(100,activation='relu')(t_ensemble_h2)\n",
    "t_ensemble_h3 = Dropout(Drop_out)(t_ensemble_h3)\n",
    "t_ensemble_h4 = Dense(50,activation='relu')(t_ensemble_h3)\n",
    "t_ensemble_h4 = Dropout(Drop_out)(t_ensemble_h4)\n",
    "t_ensemble_output = Dense(1,activation='sigmoid')(t_ensemble_h4)\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "t_ensemble_model = Model(inputs=[t_ensemble_input],outputs=[t_ensemble_output])\n",
    "t_ensemble_model.compile(optimizer=adam, \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "t_ensemble_model.fit(t_ensemble_x_val, y_val_1, epochs=100, batch_size=20, validation_split = 0.1, callbacks=[early_stopping])\n",
    "\n",
    "ensemble_model.save(\"../models/DNN/\"+str(num)+\"_t_ensemble_DNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating t-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_m_1], outputs=[h6_m_1])\n",
    "test_results_m_1 = model.predict([test_x_val_1])\n",
    "model = Model(inputs=[input_m_2], outputs=[h6_m_2])\n",
    "test_results_m_2 = model.predict([test_x_val_2])\n",
    "model = Model(inputs=[input_m_3], outputs=[h6_m_3])\n",
    "test_results_m_3 = model.predict([test_x_val_3])\n",
    "\n",
    "t_ensemble_test_x_val = np.concatenate([test_results_m_1, test_results_m_2, test_results_m_3], axis=1)\n",
    "t_em_accuracy = t_ensemble_model.evaluate(t_ensemble_test_x_val,test_y_val_1)[1]\n",
    "t_em_tr_accuracy = t_ensemble_model.evaluate(t_ensemble_x_val,y_val_1)[1]\n",
    "\n",
    "t_em_predictions = t_ensemble_model.predict(t_ensemble_x_val)\n",
    "labeled_t_em_predictions = np.where(t_em_predictions > 0.5, 1, 0).flatten()\n",
    "t_em_tr_sensitivity, t_em_tr_specificity = check_correct(labeled_t_em_predictions, y_val_1)\n",
    "t_em_test_predictions = t_ensemble_model.predict(t_ensemble_test_x_val)\n",
    "labeled_t_em_test_predictions = np.where(t_em_test_predictions > 0.5, 1, 0).flatten()\n",
    "t_em_sensitivity, t_em_specificity = check_correct(labeled_t_em_test_predictions, test_y_val_1)\n",
    "\n",
    "\n",
    "print(\"\\nAccuracy for t-ensemble: \",t_em_accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"model_1\",\"model_2\",\"model_3\",\"mean-em\",\"d-comb em\",\"t-em\"]\n",
    "accuracy = [m_1_accuracy,m_2_accuracy,m_3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy ]\n",
    "print(\"model_1: \"+str(accuracy[0])+\"\\nmodel_2: \"+str(accuracy[1])+\"\\nmodel_3: \"+str(accuracy[2])+\"\\nmean-em: \"+str(accuracy[3])+\"\\nd-comb em: \"+str(accuracy[4])+\"\\nt-em: \"+str(accuracy[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_x():\n",
    "    # this is for plotting purpose\n",
    "    plt.figure(figsize=(30,20))\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([min(m_1_accuracy,m_2_accuracy,m_3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy)-0.02,1])\n",
    "    index = np.arange(len(label))\n",
    "    plt.bar(index, accuracy,color=['red', 'orange', 'yellow', \"green\",'blue', 'purple',],alpha=0.5,width=0.3)\n",
    "    plt.xlabel('Method', fontsize=35)\n",
    "    plt.ylabel('Accuracy', fontsize=35)\n",
    "    plt.yticks(fontsize=30)    \n",
    "    plt.xticks(index, label, fontsize=30, rotation=90)\n",
    "    plt.title('Performance Comparison for each Ensemble Model',fontsize=40)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_bar_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_box = [types[select[0]], types[select[1]], types[select[2]], \"mean_em\",\"DNN_em\",\"t_em\"]\n",
    "tr_accuracy_box = [m_1_tr_accuracy,m_2_tr_accuracy,m_3_tr_accuracy,mean_em_tr_accuracy,em_accuracy,t_em_tr_accuracy]\n",
    "ts_accuracy_box = [m_1_accuracy,m_2_accuracy,m_3_accuracy,mean_em_accuracy,em_accuracy,t_em_accuracy]\n",
    "tr_sensitivity_box = [m_1_tr_sensitivity,m_2_tr_sensitivity,m_3_tr_sensitivity,mean_em_tr_sensitivity,em_sensitivity,t_em_tr_sensitivity]\n",
    "ts_sensitivity_box = [m_1_sensitivity,m_2_sensitivity,m_3_sensitivity,mean_em_sensitivity,em_sensitivity,t_em_sensitivity]\n",
    "tr_specificity_box = [m_1_tr_specificity,m_2_tr_specificity,m_3_tr_specificity,mean_em_tr_specificity,em_specificity,t_em_tr_specificity]\n",
    "ts_specificity_box = [m_1_specificity,m_2_specificity,m_3_specificity,mean_em_specificity,em_specificity,t_em_specificity]\n",
    "df = pd.DataFrame(data={\"model\":model_box, \"tr_accuracy\":tr_accuracy_box, \"tr_sensitivity\":tr_sensitivity_box, \"tr_specificity\":tr_specificity_box,\"ts_accuracy\":ts_accuracy_box, \"ts_sensitivity\":ts_sensitivity_box, \"ts_specificity\":ts_specificity_box})\n",
    "df.to_csv(\"../result/\"+str(num)+\"_result.csv\", index=False, header=True, columns= [\"model\", \"tr_accuracy\",  \"tr_sensitivity\", \"tr_specificity\",\"ts_accuracy\",  \"ts_sensitivity\", \"ts_specificity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=num+1\n",
    "print(num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
