{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Packages & Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model, Sequential \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity & specificity using Predicted Y & Real Y\n",
    "def check_correct(predict, y):\n",
    "    result = {}\n",
    "    result['resistant-correct'] = 0\n",
    "    result['resistant-wrong'] = 0\n",
    "    result['sensitive-correct'] = 0\n",
    "    result['sensitive-wrong'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if predict[i] == y[i] :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-correct'] += 1\n",
    "            else :\n",
    "                result['resistant-correct'] += 1\n",
    "        else :\n",
    "            if y[i] == 0 :\n",
    "                result['sensitive-wrong'] += 1\n",
    "            else :\n",
    "                result['resistant-wrong'] += 1\n",
    "\n",
    "    #for result_k, result_v in result.items():\n",
    "    #    print(result_k +\" : \"+ str(result_v))\n",
    "    sensitivity=result['resistant-correct']/(result['resistant-correct']+result['resistant-wrong'])\n",
    "    specificity=result['sensitive-correct']/(result['sensitive-correct']+result['sensitive-wrong'])\n",
    "    #print(\"Sensitivity :\", sensitivity)\n",
    "    #print(\"Specificity :\", specificity)\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide raw data into train / test & x_val / y_val\n",
    "def data_split(raw_data, index_col, test_index):\n",
    "    \n",
    "    train_data = raw_data.iloc[list(raw_data.iloc[:,index_col]!=test_index)]\n",
    "    test_data = raw_data.iloc[list(raw_data.iloc[:,index_col]==test_index)]\n",
    "    \n",
    "    y_val = train_data.Platinum_Status\n",
    "    x_val = train_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    test_y_val = test_data.Platinum_Status\n",
    "    test_x_val = test_data.drop([\"Platinum_Status\",\"index\"],axis=1)\n",
    "    \n",
    "    return train_data, test_data, y_val, x_val, test_y_val, test_x_val\n",
    "\n",
    "    # raw_data: have gene_expressions(maybe multiple columns), index column, Platinum_Status column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all of model performance \n",
    "# - predictions(probability) / labeled predictions(0/1) / Loss / Accuracy / Sensitivity / Specificity / AUC values of Train / Test dataset.\n",
    "# using trained models, or you can put predictions(probability) passively(in this case, Loss & Accuracy do not provided.)\n",
    "def model_performance(information=False, Input_Prediction_Passively=False, using_model=None, tr_predictions=None, ts_predictions=None, tr_x_val=None, tr_y_val=None, ts_x_val=None, ts_y_val=None, output_list=None):\n",
    "    \n",
    "    if information == True:            \n",
    "        print(\"options model_performance:\\n1) using_model: keras models that you want to check performance. \\\"Input_Prediction_Passive\\\" option for input prediction list instead using models.\\n3) tr_predictions & ts_predictions: prediction input passively. put this data only when not using keras model.\\n4) tr_x_val & ts_x_val: input samples of train/test samples.\\n4) tr_y_val & ts_y_val: results of train/test samples.\\n5) output_list: return values that you want to recieve.\\n CAUTION: Essential variable.\\n\\t tr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\\n\\n* CAUTION: if 'None' value is returned, please check your input tr inputs(None value for tr outputs) or ts inputs(None value for ts outputs).\") \n",
    "        return 0\n",
    "    elif information != False:\n",
    "        print(\"for using information options, please set 'information' variable for 'True'\")\n",
    "        return -1\n",
    "    \n",
    "    if using_model is None:\n",
    "        if Input_Prediction_Passively == False:\n",
    "            print(\"ERROR: There are no models for using.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\") \n",
    "            return -1\n",
    "        elif (tr_predictions is None) and (ts_predictions is None): # No model/prediction input. no performance should be calculated.\n",
    "                print(\"ERROR: Input prediction list instead using saved model.\")\n",
    "                return -1\n",
    "        else: # No model input, but Input_Prediction_Passively is True & input prediction is valid.\n",
    "            tr_loss,tr_accuracy= None, None\n",
    "            ts_loss,ts_accuracy= None, None\n",
    "            \n",
    "    elif Input_Prediction_Passively == True: # both of model/prediction putted, could cause confusing.\n",
    "        ch = input(\"You put both model and prediction. Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "        while 1:\n",
    "            if ch == 'p':\n",
    "                using_model = None\n",
    "                break\n",
    "            elif ch == 'm':\n",
    "                tr_predictions = None\n",
    "                ts_predictions = None\n",
    "                break\n",
    "            elif ch == 'e':\n",
    "                return 0\n",
    "            else:\n",
    "                print(\"you put worng option: \"+str(ch))\n",
    "            ch = input(\"Select one method:\\n'p' for using prediction only, 'm' using models only, 'n' for quit the function.\")\n",
    "                \n",
    "    if output_list is None:\n",
    "        print(\"ERROR: There are no output_list for return.\\nusing \\\"model_performance(information = True)\\\" for getting informations of this function.\")\n",
    "        return -1\n",
    "    \n",
    "    if not(tr_x_val is None) and not(tr_y_val is None):\n",
    "        # predict tr result only when no tr_prediction input\n",
    "        if tr_predictions is None:\n",
    "            tr_loss,tr_accuracy= using_model.evaluate(tr_x_val,tr_y_val,verbose=0)\n",
    "            tr_predictions = using_model.predict(tr_x_val,verbose=0)\n",
    "        # tr sensitivity / specificity\n",
    "        labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "        tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, tr_y_val)\n",
    "        tr_predictions_flat = tr_predictions[:,0]   \n",
    "        # roc(tr)\n",
    "        fpr_tr, tpr_tr, threshold_tr = metrics.roc_curve(tr_y_val, tr_predictions)\n",
    "        roc_auc_tr = metrics.auc(fpr_tr, tpr_tr)\n",
    "    \n",
    "    if not(ts_x_val is None) and not(ts_y_val is None):\n",
    "        # predict ts result only when no ts_prediction input\n",
    "        if ts_predictions is None:\n",
    "            ts_loss,ts_accuracy= using_model.evaluate(ts_x_val,ts_y_val,verbose=0)\n",
    "            ts_predictions = using_model.predict(ts_x_val,verbose=0)\n",
    "        labeled_ts_predictions = np.where(ts_predictions > 0.5, 1, 0).flatten()\n",
    "        ts_sensitivity, ts_specificity = check_correct(labeled_ts_predictions, ts_y_val)\n",
    "        ts_predictions_flat = ts_predictions[:,0]   \n",
    "        # roc(ts)\n",
    "        fpr_ts, tpr_ts, threshold_ts = metrics.roc_curve(ts_y_val, ts_predictions)\n",
    "        roc_auc_ts = metrics.auc(fpr_ts, tpr_ts)    \n",
    "    \n",
    "    if (not(tr_x_val is None) and not(tr_y_val is None)) and (not(ts_x_val is None) and not(ts_y_val is None)):\n",
    "        y_true = np.append(tr_y_val, ts_y_val)\n",
    "        y_pred = np.append(tr_predictions, ts_predictions)\n",
    "        fpr_total, tpr_total, threshold_total = metrics.roc_curve(y_true, y_pred)\n",
    "        roc_auc_total = metrics.auc(fpr_total, tpr_total)\n",
    "        \n",
    "        \n",
    "    return_list = []\n",
    "    \n",
    "    for output in output_list:\n",
    "        \n",
    "        if(output == \"tr_loss\"):\n",
    "            return_list.append(tr_loss)\n",
    "                               \n",
    "        elif(output == \"tr_accuracy\"):\n",
    "            return_list.append(tr_accuracy)\n",
    "                               \n",
    "        elif(output == \"tr_sensitivity\"):\n",
    "            return_list.append(tr_sensitivity)\n",
    "                               \n",
    "        elif(output == \"tr_specificity\"):\n",
    "            return_list.append(tr_specificity)\n",
    "                               \n",
    "        elif(output == \"tr_predictions\"):\n",
    "            return_list.append(tr_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_tr_predictions\"):\n",
    "            return_list.append(labeled_tr_predictions)\n",
    "                               \n",
    "        elif(output == \"tr_predictions_flat\"):\n",
    "            return_list.append(tr_predictions_flat)\n",
    "            \n",
    "        elif(output == \"roc_auc_tr\"):\n",
    "            return_list.append(roc_auc_tr)\n",
    "\n",
    "        elif(output == \"ts_loss\"):\n",
    "            return_list.append(ts_loss)\n",
    "                               \n",
    "        elif(output == \"ts_accuracy\"):\n",
    "            return_list.append(ts_accuracy)\n",
    "                               \n",
    "        elif(output == \"ts_sensitivity\"):\n",
    "            return_list.append(ts_sensitivity)\n",
    "                               \n",
    "        elif(output == \"ts_specificity\"):\n",
    "            return_list.append(ts_specificity)\n",
    "                               \n",
    "        elif(output == \"ts_predictions\"):\n",
    "            return_list.append(ts_predictions)\n",
    "                               \n",
    "        elif(output == \"labeled_ts_predictions\"):\n",
    "            return_list.append(labeled_ts_predictions)\n",
    "                               \n",
    "        elif(output == \"ts_predictions_flat\"):\n",
    "            return_list.append(ts_predictions_flat)\n",
    "        \n",
    "        elif(output == \"roc_auc_ts\"):\n",
    "            return_list.append(roc_auc_ts)\n",
    "            \n",
    "        elif(output == \"roc_auc_total\"):\n",
    "            return_list.append(roc_auc_total)\n",
    "                               \n",
    "        else:\n",
    "            print(\"There are no options <\"+str(output)+\">. Please refer these output options:\\ntr_loss, tr_accuracy, tr_sensitivity, tr_specificity, tr_predictions, labeled_tr_predictions, tr_predictions_flat, roc_auc_tr,\\nts_loss, ts_accuracy, ts_sensitivity, ts_specificity, ts_predictions, labeled_ts_predictions, ts_predictions_flat, roc_auc_ts,\\nroc_auc_total\")\n",
    "            break\n",
    "    \n",
    "    if len(return_list)==1:\n",
    "        return_list = return_list[0]\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training one NN model using train X & Y values\n",
    "# Returns trained NN model\n",
    "def train_NN_model(tr_x_val=-1, tr_y_val=-1, val_x_val=-1, val_y_val=-1, n_epoch=-1, n_layers=-1, train_method=-1):\n",
    "    \n",
    "    # 1) parameter setting\n",
    "    lr=0.01\n",
    "    input_drop_out = 0\n",
    "    drop_out = 0.5\n",
    "    layers = n_layers\n",
    "    BN = True\n",
    "    batch_size = 5\n",
    "    m_tr_loss_best = 100\n",
    "    \n",
    "    m_adam = optimizers.Adam(lr=lr)\n",
    "    # 2) model build\n",
    "    #m_input = Input(shape=(input_dim[1],))\n",
    "    m_input = Input(shape=(tr_x_val.shape[1],))\n",
    "    m_dp = Dropout(input_drop_out)(m_input)\n",
    "    if BN == True:\n",
    "        for i in layers:\n",
    "            m_h = Dense(i)(m_dp)\n",
    "            m_bn = BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')(m_h)\n",
    "            m_dp = Activation(\"relu\")(m_bn)\n",
    "    else:        \n",
    "        for i in m_layers:\n",
    "            m_h = Dense(i,activation='relu')(m_dp)\n",
    "            m_dp = Dropout(drop_out)(m_h)\n",
    "    m_final = m_dp\n",
    "    m_output = Dense(1, activation=\"sigmoid\")(m_final)\n",
    "    m_model = Model(inputs=m_input,outputs=m_output)\n",
    "    m_model.compile(optimizer=m_adam, \n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    # Training method: \n",
    "    # Training method that maximize train accuracy does not fit into AdatBoost: because it makes too strong classifier\n",
    "    \n",
    "    if train_method == \"t\":\n",
    "        while 1:\n",
    "            m_model.fit(tr_x_val, tr_y_val, batch_size=batch_size, epochs=1, verbose = 0)\n",
    "            m_tr_loss=m_model.evaluate(tr_x_val, tr_y_val, verbose = 0)[0]\n",
    "            if m_tr_loss < m_tr_loss_best: # new best model. count reset.\n",
    "                m_tr_loss_best = m_tr_loss\n",
    "                count=0\n",
    "                best_m_model = m_model\n",
    "            if count>3: # no increase three time. stop.\n",
    "                m_model = best_m_model\n",
    "                break\n",
    "            else: count=count+1    \n",
    "                \n",
    "    elif train_method == \"e\":\n",
    "        if n_epoch == -1:\n",
    "            print(\"number of epoch is needed.\")\n",
    "        m_model.fit(tr_x_val, tr_y_val, batch_size=batch_size, epochs=n_epoch, verbose = 0)\n",
    "        \n",
    "    elif train_method == \"v\":\n",
    "        if n_epoch == -1:\n",
    "            print(\"number of epoch is needed.\")\n",
    "        if val_x_val == -1 or val_y_val == -1:\n",
    "            print(\"validation x or validation y is needed\")\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "        m_model.fit(tr_x_val, tr_y_val, batch_size=batch_size, epochs=n_epoch, verbose = 0, validation_data=(val_x_val, val_y_val), callbacks=[early_stopping])\n",
    "        \n",
    "    else:\n",
    "        print(\"insufficient option for training.\\n\\t\\\"t\\\" for fitting until train loss is not increased\\n\\t\\\"e\\\" for fitting number of epochs\\n\\t\\\"v\\\" for fitting number of epochs and early stopping using validationset.\")\n",
    "        return -1\n",
    "    return m_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x2ef8d5ee9b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input Data & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Declaration of path name & type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [\"OV_six_fold_Annotation3000_400\", \n",
    "         \"OV_six_fold_CV_400\", \n",
    "         \"OV_six_fold_Var_400\", \"OV_six_fold_new_Diff_400\",\n",
    "         \"OV_six_fold_Clin\", \n",
    "         \"OV_six_fold_SNV\" \n",
    "         ]\n",
    "\n",
    "path = \"../../../TC_six_fold_subsamples/\"\n",
    "save_model_path = \"../best_models/model/\"\n",
    "save_prediction_path = \"../best_models/predictions/\"\n",
    "save_result_path = \"../best_models/results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Split data into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] file_name:  OV_six_fold_Annotation3000_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[2] file_name:  OV_six_fold_CV_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[3] file_name:  OV_six_fold_Var_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[4] file_name:  OV_six_fold_new_Diff_400 \n",
      "sample : 217  \n",
      "features : 400\n",
      "[5] file_name:  OV_six_fold_Clin \n",
      "sample : 287  \n",
      "features : 35\n",
      "[6] file_name:  OV_six_fold_SNV \n",
      "sample : 213  \n",
      "features : 13814\n"
     ]
    }
   ],
   "source": [
    "file_1 = path+types[0]+\".csv\"\n",
    "file_2 = path+types[1]+\".csv\"\n",
    "file_3 = path+types[2]+\".csv\"\n",
    "file_4 = path+types[3]+\".csv\"\n",
    "file_5 = path+types[4]+\".csv\"\n",
    "file_6 = path+types[5]+\".csv\"\n",
    "\n",
    "idx_col = 0\n",
    "\n",
    "full_data_1 = pd.read_csv(file_1,index_col=idx_col)\n",
    "full_data_2 = pd.read_csv(file_2,index_col=idx_col)\n",
    "full_data_3 = pd.read_csv(file_3,index_col=idx_col)\n",
    "full_data_4 = pd.read_csv(file_4,index_col=idx_col)\n",
    "full_data_5 = pd.read_csv(file_5,index_col=idx_col)\n",
    "full_data_6 = pd.read_csv(file_6,index_col=idx_col)\n",
    "\n",
    "inter_data_1 = full_data_1.iloc[list(full_data_1.iloc[:,-1]!=6)]\n",
    "inter_data_2 = full_data_2.iloc[list(full_data_2.iloc[:,-1]!=6)]\n",
    "inter_data_3 = full_data_3.iloc[list(full_data_3.iloc[:,-1]!=6)]\n",
    "inter_data_4 = full_data_4.iloc[list(full_data_4.iloc[:,-1]!=6)]\n",
    "inter_data_5 = full_data_5.iloc[list(full_data_5.iloc[:,-1]!=6)]\n",
    "inter_data_6 = full_data_6.iloc[list(full_data_6.iloc[:,-1]!=6)]\n",
    "\n",
    "print(\"[1] file_name: \", types[0], \"\\nsample : {}  \\nfeatures : {}\".format(full_data_1.shape[0],full_data_1.shape[1]-2))\n",
    "print(\"[2] file_name: \", types[1], \"\\nsample : {}  \\nfeatures : {}\".format(full_data_2.shape[0],full_data_2.shape[1]-2))\n",
    "print(\"[3] file_name: \", types[2], \"\\nsample : {}  \\nfeatures : {}\".format(full_data_3.shape[0],full_data_3.shape[1]-2))\n",
    "print(\"[4] file_name: \", types[3], \"\\nsample : {}  \\nfeatures : {}\".format(full_data_4.shape[0],full_data_4.shape[1]-2))\n",
    "print(\"[5] file_name: \", types[4], \"\\nsample : {}  \\nfeatures : {}\".format(full_data_5.shape[0],full_data_5.shape[1]-2))\n",
    "print(\"[6] file_name: \", types[5], \"\\nsample : {}  \\nfeatures : {}\".format(full_data_6.shape[0],full_data_6.shape[1]-2))\n",
    "\n",
    "\n",
    "# Split Train Test Data\n",
    "\n",
    "tr_data_1, ts_data_1, tr_y_1, tr_x_1, ts_y_1, ts_x_1 = data_split(raw_data = full_data_1, index_col = -1, test_index = 1)\n",
    "tr_data_2, ts_data_2, tr_y_2, tr_x_2, ts_y_2, ts_x_2 = data_split(raw_data = full_data_2, index_col = -1, test_index = 1)\n",
    "tr_data_3, ts_data_3, tr_y_3, tr_x_3, ts_y_3, ts_x_3 = data_split(raw_data = full_data_3, index_col = -1, test_index = 1)\n",
    "tr_data_4, ts_data_4, tr_y_4, tr_x_4, ts_y_4, ts_x_4 = data_split(raw_data = full_data_4, index_col = -1, test_index = 1)\n",
    "tr_data_5, ts_data_5, tr_y_5, tr_x_5, ts_y_5, ts_x_5 = data_split(raw_data = full_data_5, index_col = -1, test_index = 1)\n",
    "tr_data_6, ts_data_6, tr_y_6, tr_x_6, ts_y_6, ts_x_6 = data_split(raw_data = full_data_6, index_col = -1, test_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_1 = tr_x_1.iloc[:int(tr_x_1.shape[0]/10),:]\n",
    "tr_v_x_1 = tr_x_1.iloc[int(tr_x_1.shape[0]/10):,:]\n",
    "val_y_1 = tr_y_1.iloc[:int(tr_y_1.shape[0]/10)]\n",
    "tr_v_y_1 = tr_y_1.iloc[int(tr_y_1.shape[0]/10):]\n",
    "\n",
    "val_x_2 = tr_x_2.iloc[:int(tr_x_2.shape[0]/10),:]\n",
    "tr_v_x_2 = tr_x_2.iloc[int(tr_x_2.shape[0]/10):,:]\n",
    "val_y_2 = tr_y_2.iloc[:int(tr_y_2.shape[0]/10)]\n",
    "tr_v_y_2 = tr_y_2.iloc[int(tr_y_2.shape[0]/10):]\n",
    "\n",
    "val_x_3 = tr_x_3.iloc[:int(tr_x_3.shape[0]/10),:]\n",
    "tr_v_x_3 = tr_x_3.iloc[int(tr_x_3.shape[0]/10):,:]\n",
    "val_y_3 = tr_y_3.iloc[:int(tr_y_3.shape[0]/10)]\n",
    "tr_v_y_3 = tr_y_3.iloc[int(tr_y_3.shape[0]/10):]\n",
    "\n",
    "val_x_4 = tr_x_4.iloc[:int(tr_x_4.shape[0]/10),:]\n",
    "tr_v_x_4 = tr_x_4.iloc[int(tr_x_4.shape[0]/10):,:]\n",
    "val_y_4 = tr_y_4.iloc[:int(tr_y_4.shape[0]/10)]\n",
    "tr_v_y_4 = tr_y_4.iloc[int(tr_y_4.shape[0]/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_v_x_list = [tr_v_x_1, tr_v_x_2, tr_v_x_3, tr_v_x_4]\n",
    "tr_v_y_list = [tr_v_y_1, tr_v_y_2, tr_v_y_3, tr_v_y_4]\n",
    "val_x_list = [val_x_1, val_x_2, val_x_3, val_x_4]\n",
    "val_y_list = [val_y_1, val_y_2, val_y_3, val_y_4]\n",
    "tr_x_list = [tr_x_1, tr_x_2, tr_x_3, tr_x_4]\n",
    "tr_y_list = [tr_y_1, tr_y_2, tr_y_3, tr_y_4]\n",
    "ts_x_list = [ts_x_1, ts_x_2, ts_x_3, ts_x_4]\n",
    "ts_y_list = [ts_y_1, ts_y_2, ts_y_3, ts_y_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training Models & Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Training models N stage - using \"t\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.113 (error: 0.113)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.054 (error: 0.054)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.005 (error: 0.005)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 2/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.005 (error: 0.005)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.0 (error: 0.0)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Annotation3000_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 3/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.134 (error: 0.134)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.0 (error: 0.0)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 4/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.081 (error: 0.081)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.016 (error: 0.016)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 5/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.022 (error: 0.022)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.0 (error: 0.0)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 6/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.075 (error: 0.075)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.0 (error: 0.0)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 7/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.258 (error: 0.258)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.005 (error: 0.005)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.005 (error: 0.005)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.005 (error: 0.005)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 0.995\n",
      "\t   Weighted error: 0.005, Alpha: 2.61\n",
      "\n",
      "\n",
      "# 8/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.043 (error: 0.043)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.027 (error: 0.027)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.0 (error: 0.0)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 9/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.151 (error: 0.151)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.011 (error: 0.011)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.0 (error: 0.0)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Var_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n",
      "# 10/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.222 (error: 0.226)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.0 (error: 0.0)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.0 (error: 0.0)\n",
      "\n",
      "\t-> Selected: OV_six_fold_CV_400\n",
      "\t   tr_acc: 1.0\n",
      "\t   Weighted error: 0.0, Alpha: 8.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.array([1/tr_x_1.shape[0]]*tr_x_1.shape[0])\n",
    "model_list = []\n",
    "alpha_list = []\n",
    "type_num_list = []\n",
    "sample_weight_list = []\n",
    "error_list = []\n",
    "\n",
    "for step in range(max_model_num):\n",
    "    print(\"# \"+str(step+1)+\"/\"+str(max_model_num)+\"th step\\n\")\n",
    "    best_model = 0\n",
    "    best_weighted_errors = 0\n",
    "    best_weighted_error_sum = -1\n",
    "    best_alpha = 0\n",
    "    best_type_num = 0\n",
    "    best_tr_acc = 0\n",
    "    best_error = 0\n",
    "    \n",
    "    for t in range(4):\n",
    "        \n",
    "        tr_x = tr_x_list[t]\n",
    "        tr_y = tr_y_list[t]\n",
    "        val_x = val_x_list[t]\n",
    "        val_y = val_y_list[t]\n",
    "        #print(tr_x.iloc[:2][:2])\n",
    "\n",
    "        model_t = train_NN_model(tr_x_val=tr_x, tr_y_val=tr_y, n_layers=[20], train_method=\"t\")\n",
    "        pred_Y = model_performance(using_model = model_t, tr_x_val=tr_x, tr_y_val=tr_y, output_list=[\"labeled_tr_predictions\"])\n",
    "        #if pred_Y is 0 or 1, all weighted predict Y of 0 samples will be just 0\n",
    "        pred_Y_proc = pred_Y*2-1\n",
    "        Y_proc = np.array(tr_y)*2-1\n",
    "        error = abs(Y_proc - pred_Y_proc)/2\n",
    "        error_sum = np.sum(error)\n",
    "        weighted_errors = sample_weight*error.T\n",
    "        weighted_error_sum = np.sum(weighted_errors)\n",
    "        print(\"Weighted error of \"+types[t]+\":\\n\\t\"+str(weighted_error_sum.round(3))+\" (error: \"+str((error_sum/tr_x.shape[0]).round(3))+\")\")\n",
    "        \n",
    "        if best_weighted_error_sum == -1 or best_weighted_error_sum > weighted_error_sum:\n",
    "            #print(error)\n",
    "            #print(weighted_errors)\n",
    "            \n",
    "            best_model = model_t\n",
    "            best_error = error\n",
    "            best_weighted_errors = weighted_errors\n",
    "            best_weighted_error_sum = np.sum(weighted_errors)\n",
    "            best_alpha = math.log((1-min(best_weighted_error_sum, (1-math.exp(-16))))/max(best_weighted_error_sum, math.exp(-16)))/2\n",
    "            best_type_num = t\n",
    "            best_tr_acc = model_t.evaluate(tr_x, tr_y, verbose = 0)[1]\n",
    "\n",
    "    print(\"\\n\\t-> Selected: \"+types[best_type_num])\n",
    "    print(\"\\t   tr_acc: \"+str(best_tr_acc.round(3)))\n",
    "    print(\"\\t   Weighted error: \"+str(best_weighted_error_sum.round(3))+\", Alpha: \"+str(np.float64(best_alpha).round(3))+\"\\n\\n\")\n",
    "    error_term = (best_error*2)-1\n",
    "    updated_weight = sample_weight*np.exp((-1)*best_alpha*error_term).T\n",
    "    sample_weight = updated_weight / np.sum(updated_weight)\n",
    "    \n",
    "    model_list.append(best_model)\n",
    "    alpha_list.append(best_alpha)\n",
    "    type_num_list.append(best_type_num)\n",
    "    sample_weight_list.append(sample_weight)\n",
    "    error_list.append(best_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th best model is OV_six_fold_CV_400\n",
      "# 1 th model: OV_six_fold_CV_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.4838709533214569\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.4838709677419355\n",
      "\n",
      "2th best model is OV_six_fold_Annotation3000_400\n",
      "# 2 th model: OV_six_fold_Annotation3000_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.6774193644523621\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.6774193548387097\n",
      "\n",
      "3th best model is OV_six_fold_CV_400\n",
      "# 3 th model: OV_six_fold_CV_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.4838709533214569\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5483870967741935\n",
      "\n",
      "4th best model is OV_six_fold_CV_400\n",
      "# 4 th model: OV_six_fold_CV_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.5161290168762207\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5483870967741935\n",
      "\n",
      "5th best model is OV_six_fold_CV_400\n",
      "# 5 th model: OV_six_fold_CV_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.4193548262119293\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5483870967741935\n",
      "\n",
      "6th best model is OV_six_fold_CV_400\n",
      "# 6 th model: OV_six_fold_CV_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.5806451439857483\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5483870967741935\n",
      "\n",
      "7th best model is OV_six_fold_CV_400\n",
      "# 7 th model: OV_six_fold_CV_400\n",
      "tr_acc: 0.9946236559139785\n",
      "ts_acc: 0.4193548262119293\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5161290322580645\n",
      "\n",
      "8th best model is OV_six_fold_CV_400\n",
      "# 8 th model: OV_six_fold_CV_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.5806451439857483\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5483870967741935\n",
      "\n",
      "9th best model is OV_six_fold_Var_400\n",
      "# 9 th model: OV_six_fold_Var_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.6774193644523621\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5161290322580645\n",
      "\n",
      "10th best model is OV_six_fold_CV_400\n",
      "# 10 th model: OV_six_fold_CV_400\n",
      "tr_acc: 1.0\n",
      "ts_acc: 0.4838709533214569\n",
      "tr_acc_tot: 1.0\n",
      "ts_acc_tot: 0.5483870967741935\n",
      "\n",
      "####################### Final acc: 1.0, 0.5483870967741935\n"
     ]
    }
   ],
   "source": [
    "tr_sum = 0\n",
    "ts_sum = 0\n",
    "alpha_sum = 0\n",
    "\n",
    "tr_acc_list = []\n",
    "ts_acc_list = []\n",
    "tr_acc_tot_list = []\n",
    "ts_acc_tot_list = []\n",
    "tr_sum_tot_list = []\n",
    "ts_sum_tot_list = []\n",
    "\n",
    "for m in range(len(model_list)):\n",
    "    b = type_num_list[m]\n",
    "    best_type = types[b]\n",
    "    print(str(m+1)+\"th best model is \"+best_type)\n",
    "    [tr_x, tr_y, ts_x, ts_y] = [tr_x_list[b], tr_y_list[b], ts_x_list[b], ts_y_list[b]]\n",
    "    print(\"# \"+str(m+1)+\" th model: \"+best_type)\n",
    "    tr_pred_Y = np.array(model_performance(using_model = model_list[m], tr_x_val=tr_x, tr_y_val=tr_y, ts_x_val=ts_x, ts_y_val=ts_y, output_list=[\"labeled_tr_predictions\"]))\n",
    "    ts_pred_Y = np.array(model_performance(using_model = model_list[m], tr_x_val=tr_x, tr_y_val=tr_y, ts_x_val=ts_x, ts_y_val=ts_y, output_list=[\"labeled_ts_predictions\"]))\n",
    "    tr_acc, ts_acc = np.array(model_performance(using_model = model_list[m], tr_x_val=tr_x, tr_y_val=tr_y, ts_x_val=ts_x, ts_y_val=ts_y, output_list=[\"tr_accuracy\", \"ts_accuracy\"]))\n",
    "    best_val_acc = model_t.evaluate(val_x, val_y, verbose = 0)[1]\n",
    "    \n",
    "    print(\"tr_acc: \"+str(tr_acc))\n",
    "    print(\"ts_acc: \"+str(ts_acc))\n",
    "\n",
    "    tr_pred_Y_proc = tr_pred_Y*2 -1 \n",
    "    ts_pred_Y_proc = ts_pred_Y*2 -1\n",
    "    \n",
    "    #print(tr_pred_Y)\n",
    "    #print(tr_pred_Y_proc)\n",
    "    \n",
    "    tr_sum = tr_sum + alpha_list[m]*tr_pred_Y_proc\n",
    "    ts_sum = ts_sum + alpha_list[m]*ts_pred_Y_proc\n",
    "    alpha_sum = alpha_sum + alpha_list[m]\n",
    "    \n",
    "    tr_sum_tot = np.where(tr_sum / alpha_sum > 0, 1, 0).flatten()\n",
    "    ts_sum_tot = np.where(ts_sum / alpha_sum > 0, 1, 0).flatten()\n",
    "    \n",
    "    tr_acc_tot = 1 - np.sum(np.abs(tr_sum_tot - np.asarray(tr_y))) / tr_sum_tot.shape[0]\n",
    "    ts_acc_tot = 1 - np.sum(np.abs(ts_sum_tot - np.asarray(ts_y))) / ts_sum_tot.shape[0]\n",
    "\n",
    "    print(\"tr_acc_tot: \"+str(tr_acc_tot))\n",
    "    print(\"ts_acc_tot: \"+str(ts_acc_tot)+\"\\n\")\n",
    "    \n",
    "    tr_acc_list.append(tr_acc)\n",
    "    ts_acc_list.append(ts_acc)\n",
    "    tr_acc_tot_list.append(tr_acc_tot)\n",
    "    ts_acc_tot_list.append(ts_acc_tot)\n",
    "    tr_sum_tot_list.append(tr_sum_tot)\n",
    "    ts_sum_tot_list.append(ts_sum_tot)\n",
    "    \n",
    "    \n",
    "print(\"####################### Final acc: \"+str(tr_acc_tot)+\", \"+str(ts_acc_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,len(tr_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_acc_table = pd.DataFrame(data={\"tr_acc\":tr_acc_list, \"ts_acc\":ts_acc_list})\n",
    "inv_acc_table = inv_acc_table[[\"tr_acc\", \"ts_acc\"]]\n",
    "tot_acc_table = pd.DataFrame(data={\"tr_acc\":tr_acc_tot_list, \"ts_acc\":ts_acc_tot_list})\n",
    "tot_acc_table = tot_acc_table[[\"tr_acc\", \"ts_acc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *AdaBoost Using Weight of Validation Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.444 (error: 0.444)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.556 (error: 0.556)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.611 (error: 0.611)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.611 (error: 0.611)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Annotation3000_400\n",
      "\t   tr_acc: 0.946\n",
      "\t   val_acc: 0.556\n",
      "\t   Weighted error: 0.444, Alpha: 0.112\n",
      "\n",
      "\n",
      "# 2/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.415 (error: 0.444)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.415 (error: 0.444)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.354 (error: 0.389)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.476 (error: 0.5)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Var_400\n",
      "\t   tr_acc: 0.988\n",
      "\t   val_acc: 0.611\n",
      "\t   Weighted error: 0.354, Alpha: 0.301\n",
      "\n",
      "\n",
      "# 3/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.475 (error: 0.556)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.403 (error: 0.444)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.321 (error: 0.444)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.321 (error: 0.444)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Var_400\n",
      "\t   tr_acc: 0.952\n",
      "\t   val_acc: 0.556\n",
      "\t   Weighted error: 0.321, Alpha: 0.374\n",
      "\n",
      "\n",
      "# 4/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.485 (error: 0.389)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.281 (error: 0.444)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.271 (error: 0.5)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.591 (error: 0.667)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Var_400\n",
      "\t   tr_acc: 0.982\n",
      "\t   val_acc: 0.5\n",
      "\t   Weighted error: 0.271, Alpha: 0.496\n",
      "\n",
      "\n",
      "# 5/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.517 (error: 0.611)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.245 (error: 0.444)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.158 (error: 0.444)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.486 (error: 0.556)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Var_400\n",
      "\t   tr_acc: 0.982\n",
      "\t   val_acc: 0.556\n",
      "\t   Weighted error: 0.158, Alpha: 0.836\n",
      "\n",
      "\n",
      "# 6/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.022 (error: 0.333)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.401 (error: 0.556)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.23 (error: 0.611)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.281 (error: 0.556)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Annotation3000_400\n",
      "\t   tr_acc: 0.833\n",
      "\t   val_acc: 0.667\n",
      "\t   Weighted error: 0.022, Alpha: 1.904\n",
      "\n",
      "\n",
      "# 7/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.249 (error: 0.5)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.313 (error: 0.444)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.004 (error: 0.389)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.459 (error: 0.611)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Var_400\n",
      "\t   tr_acc: 0.946\n",
      "\t   val_acc: 0.611\n",
      "\t   Weighted error: 0.004, Alpha: 2.763\n",
      "\n",
      "\n",
      "# 8/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.019 (error: 0.5)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.267 (error: 0.5)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.0 (error: 0.444)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.504 (error: 0.556)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Var_400\n",
      "\t   tr_acc: 0.851\n",
      "\t   val_acc: 0.556\n",
      "\t   Weighted error: 0.0, Alpha: 4.574\n",
      "\n",
      "\n",
      "# 9/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.019 (error: 0.444)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.266 (error: 0.444)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.192 (error: 0.5)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.436 (error: 0.611)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Annotation3000_400\n",
      "\t   tr_acc: 0.821\n",
      "\t   val_acc: 0.556\n",
      "\t   Weighted error: 0.019, Alpha: 1.983\n",
      "\n",
      "\n",
      "# 10/10th step\n",
      "\n",
      "Weighted error of OV_six_fold_Annotation3000_400:\n",
      "\t0.126 (error: 0.389)\n",
      "Weighted error of OV_six_fold_CV_400:\n",
      "\t0.253 (error: 0.444)\n",
      "Weighted error of OV_six_fold_Var_400:\n",
      "\t0.426 (error: 0.667)\n",
      "Weighted error of OV_six_fold_new_Diff_400:\n",
      "\t0.495 (error: 0.667)\n",
      "\n",
      "\t-> Selected: OV_six_fold_Annotation3000_400\n",
      "\t   tr_acc: 0.911\n",
      "\t   val_acc: 0.611\n",
      "\t   Weighted error: 0.126, Alpha: 0.967\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.array([1/val_x_1.shape[0]]*val_x_1.shape[0])\n",
    "model_list = []\n",
    "alpha_list = []\n",
    "type_num_list = []\n",
    "sample_weight_list = []\n",
    "error_list = []\n",
    "\n",
    "for step in range(max_model_num):\n",
    "    print(\"# \"+str(step+1)+\"/\"+str(max_model_num)+\"th step\\n\")\n",
    "    best_model = 0\n",
    "    best_weighted_errors = 0\n",
    "    best_weighted_error_sum = -1\n",
    "    best_alpha = 0\n",
    "    best_type_num = 0\n",
    "    best_tr_acc = 0\n",
    "    best_val_acc = 0\n",
    "    best_error = 0\n",
    "    \n",
    "    for t in range(4):\n",
    "        \n",
    "        tr_x = tr_x_list[t]\n",
    "        tr_y = tr_y_list[t]\n",
    "        val_x = val_x_list[t]\n",
    "        val_y = val_y_list[t]\n",
    "        #print(tr_x.iloc[:2][:2])\n",
    "\n",
    "        model_t = train_NN_model(tr_x_val=tr_x, tr_y_val=tr_y, val_x_val=val_x, val_y_val=val_y, n_epoch=50, train_method)\n",
    "        pred_Y = model_performance(using_model = model_t, tr_x_val=val_x, tr_y_val=val_y, output_list=[\"labeled_tr_predictions\"])\n",
    "        #if pred_Y is 0 or 1, all weighted predict Y of 0 samples will be just 0\n",
    "        pred_Y_proc = pred_Y*2-1\n",
    "        Y_proc = np.array(val_y)*2-1\n",
    "        error = abs(Y_proc - pred_Y_proc)/2\n",
    "        error_sum = np.sum(error)\n",
    "        weighted_errors = sample_weight*error.T\n",
    "        weighted_error_sum = np.sum(weighted_errors)\n",
    "        print(\"Weighted error of \"+types[t]+\":\\n\\t\"+str(weighted_error_sum.round(3))+\" (error: \"+str((error_sum/val_x_1.shape[0]).round(3))+\")\")\n",
    "        \n",
    "        if best_weighted_error_sum == -1 or best_weighted_error_sum > weighted_error_sum:\n",
    "            #print(error)\n",
    "            #print(weighted_errors)\n",
    "            \n",
    "            best_model = model_t\n",
    "            best_error = error\n",
    "            best_weighted_errors = weighted_errors\n",
    "            best_weighted_error_sum = np.sum(weighted_errors)\n",
    "            best_alpha = math.log((1-min(best_weighted_error_sum, (1-math.exp(-16))))/max(best_weighted_error_sum, math.exp(-16)))/2\n",
    "            best_type_num = t\n",
    "            best_tr_acc = model_t.evaluate(tr_x, tr_y, verbose = 0)[1]\n",
    "            best_val_acc = model_t.evaluate(val_x, val_y, verbose = 0)[1]\n",
    "\n",
    "\n",
    "    print(\"\\n\\t-> Selected: \"+types[best_type_num])\n",
    "    print(\"\\t   tr_acc: \"+str(best_tr_acc.round(3)))\n",
    "    print(\"\\t   val_acc: \"+str(best_val_acc.round(3)))\n",
    "    print(\"\\t   Weighted error: \"+str(best_weighted_error_sum.round(3))+\", Alpha: \"+str(np.float64(best_alpha).round(3))+\"\\n\\n\")\n",
    "    error_term = (best_error*2)-1\n",
    "    updated_weight = sample_weight*np.exp((-1)*best_alpha*error_term).T\n",
    "    sample_weight = updated_weight / np.sum(updated_weight)\n",
    "    \n",
    "    model_list.append(best_model)\n",
    "    alpha_list.append(best_alpha)\n",
    "    type_num_list.append(best_type_num)\n",
    "    sample_weight_list.append(sample_weight)\n",
    "    error_list.append(best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OV_six_fold_Annotation3000_400: 4\n",
      "OV_six_fold_CV_400: 0\n",
      "OV_six_fold_Var_400: 1\n",
      "OV_six_fold_new_Diff_400: 0\n"
     ]
    }
   ],
   "source": [
    "for k in range(4):\n",
    "    print(types[k]+\": \"+str(type_num_list.count(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th best model is OV_six_fold_Annotation3000_400\n",
      "# 1 th model: OV_six_fold_Annotation3000_400\n",
      "tr_acc: 0.9523809523809523\n",
      "ts_acc: 0.6774193644523621\n",
      "tr_acc_tot: 0.9523809523809523\n",
      "ts_acc_tot: 0.6774193548387097\n",
      "\n",
      "2th best model is OV_six_fold_Annotation3000_400\n",
      "# 2 th model: OV_six_fold_Annotation3000_400\n",
      "tr_acc: 0.9821428571428571\n",
      "ts_acc: 0.6774193644523621\n",
      "tr_acc_tot: 0.9821428571428571\n",
      "ts_acc_tot: 0.6774193548387097\n",
      "\n",
      "3th best model is OV_six_fold_Annotation3000_400\n",
      "# 3 th model: OV_six_fold_Annotation3000_400\n",
      "tr_acc: 0.8690476190476191\n",
      "ts_acc: 0.6774193644523621\n",
      "tr_acc_tot: 0.9583333333333334\n",
      "ts_acc_tot: 0.6774193548387097\n",
      "\n",
      "4th best model is OV_six_fold_Annotation3000_400\n",
      "# 4 th model: OV_six_fold_Annotation3000_400\n",
      "tr_acc: 0.9107142857142857\n",
      "ts_acc: 0.6774193644523621\n",
      "tr_acc_tot: 0.9464285714285714\n",
      "ts_acc_tot: 0.6451612903225806\n",
      "\n",
      "5th best model is OV_six_fold_Var_400\n",
      "# 5 th model: OV_six_fold_Var_400\n",
      "tr_acc: 0.9880952380952381\n",
      "ts_acc: 0.4838709533214569\n",
      "tr_acc_tot: 0.9523809523809523\n",
      "ts_acc_tot: 0.6774193548387097\n",
      "\n",
      "####################### Final acc: 0.9523809523809523, 0.6774193548387097\n"
     ]
    }
   ],
   "source": [
    "tr_sum = 0\n",
    "ts_sum = 0\n",
    "alpha_sum = 0\n",
    "\n",
    "for m in range(len(model_list)):\n",
    "    b = type_num_list[m]\n",
    "    best_type = types[b]\n",
    "    print(str(m+1)+\"th best model is \"+best_type)\n",
    "    [tr_x, tr_y, ts_x, ts_y] = [tr_x_list[b], tr_y_list[b], ts_x_list[b], ts_y_list[b]]\n",
    "    print(\"# \"+str(m+1)+\" th model: \"+best_type)\n",
    "    tr_pred_Y = np.array(model_performance(using_model = model_list[m], tr_x_val=tr_x, tr_y_val=tr_y, ts_x_val=ts_x, ts_y_val=ts_y, output_list=[\"labeled_tr_predictions\"]))\n",
    "    ts_pred_Y = np.array(model_performance(using_model = model_list[m], tr_x_val=tr_x, tr_y_val=tr_y, ts_x_val=ts_x, ts_y_val=ts_y, output_list=[\"labeled_ts_predictions\"]))\n",
    "    tr_acc, ts_acc = np.array(model_performance(using_model = model_list[m], tr_x_val=tr_x, tr_y_val=tr_y, ts_x_val=ts_x, ts_y_val=ts_y, output_list=[\"tr_accuracy\", \"ts_accuracy\"]))\n",
    "    best_val_acc = model_t.evaluate(val_x, val_y, verbose = 0)[1]\n",
    "    \n",
    "    print(\"tr_acc: \"+str(tr_acc))\n",
    "    print(\"ts_acc: \"+str(ts_acc))\n",
    "\n",
    "    tr_pred_Y_proc = tr_pred_Y*2 -1 \n",
    "    ts_pred_Y_proc = ts_pred_Y*2 -1\n",
    "    \n",
    "    #print(tr_pred_Y)\n",
    "    #print(tr_pred_Y_proc)\n",
    "    \n",
    "    tr_sum = tr_sum + alpha_list[m]*tr_pred_Y_proc\n",
    "    ts_sum = ts_sum + alpha_list[m]*ts_pred_Y_proc\n",
    "    alpha_sum = alpha_sum + alpha_list[m]\n",
    "    \n",
    "    tr_sum_tot = np.where(tr_sum / alpha_sum > 0, 1, 0).flatten()\n",
    "    ts_sum_tot = np.where(ts_sum / alpha_sum > 0, 1, 0).flatten()\n",
    "    \n",
    "    tr_acc_tot = 1 - np.sum(np.abs(tr_sum_tot - np.asarray(tr_y))) / tr_sum_tot.shape[0]\n",
    "    ts_acc_tot = 1 - np.sum(np.abs(ts_sum_tot - np.asarray(ts_y))) / ts_sum_tot.shape[0]\n",
    "\n",
    "    print(\"tr_acc_tot: \"+str(tr_acc_tot))\n",
    "    print(\"ts_acc_tot: \"+str(ts_acc_tot)+\"\\n\")\n",
    "    \n",
    "    \n",
    "print(\"####################### Final acc: \"+str(tr_acc_tot)+\", \"+str(ts_acc_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
